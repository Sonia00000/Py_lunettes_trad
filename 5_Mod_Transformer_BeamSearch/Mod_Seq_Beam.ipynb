{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec24d92",
   "metadata": {},
   "source": [
    "# Modélisation 4 : Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece91cb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8d5e8",
   "metadata": {},
   "source": [
    "Lors de cette itération, nous allons nous intéresser à l'application d'un Beam Search Decoder accouplé à un modèle Seq2Seq. Nous allons dans un premier temps importer un nouveau jeu de données. Comme nous l'avons vu lors de l'itération précédente, le jeu de données dont nous disposions était assez limité en terme de vocabulaire. En effet, le modèle se généralisait très mal et n'était performant que pour traduire les phrases qui ressemblait aux phrases du jeu de données. Afin d'avoir un point de comparaison avec un modèle Seq2Seq simple, nous allons d'abord entraîner le modèle de l'itération précédente avec le nouveau jeu de données, puis mettre au point un autre modèle Seq2Seq combiné cette fois-ci avec un Beam Search Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34a8db",
   "metadata": {},
   "source": [
    "## Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e45297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RNN, Lambda, Embedding\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7056a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0e0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des données \n",
    "fr = pd.read_csv('french.txt', sep = \"\\n\\n\", names = [\"Français\"], encoding = 'utf-8', engine = 'python')\n",
    "en = pd.read_csv('english.txt', sep = \"\\n\\n\", names = [\"English\"], encoding = 'utf-8', engine = 'python')\n",
    "data = pd.concat([en, fr], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764cdbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Français</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; go . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; va ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; run ! &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; cours ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; run ! &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; courez ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; fire ! &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; au feu ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; help ! &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; a l aide ! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                English                  Français\n",
       "0    <start> go . <end>        <start> va ! <end>\n",
       "1   <start> run ! <end>     <start> cours ! <end>\n",
       "2   <start> run ! <end>    <start> courez ! <end>\n",
       "3  <start> fire ! <end>    <start> au feu ! <end>\n",
       "4  <start> help ! <end>  <start> a l aide ! <end>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nettoyage des données \n",
    "\n",
    "# Fonction de conversion d'un document d'unicode vers ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Prétraitement des phrases\n",
    "def clean_sentence(w):\n",
    "   \n",
    "    # conversion unicode vers ascii\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # séparation entre un mot et sa ponctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # remplacement de caractères spéciaux par un espace\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # ajout des balises <start> et <end> placées respectivement au début et à la fin des phrases\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne english\n",
    "data.English = data.English.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne french\n",
    "data.Français = data.Français.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "# Visulisation des 5 premières lignes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a1c68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire :\n",
      " - input : 13473\n",
      " - target : 22089\n"
     ]
    }
   ],
   "source": [
    "# Structuration des données \n",
    "\n",
    "def tokenize(sentences):\n",
    "    # tokenization des phrases\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    # transformation des phrases en séquences\n",
    "    seq = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # complète les séquences de sorte à ce qu'ils aient la même longueur\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq,padding='post')\n",
    "\n",
    "    # post : le padding est réalisé à la fin des séquences\n",
    "    return seq, tokenizer\n",
    "\n",
    "# Tokenization(transforme une phrase en une liste de nombres qui correspondent chacun à un mot unique du vocabulaire)\n",
    "input_seq, input_tokenizer = tokenize(data.English) # phrases tokenisées en fonction du vocabulaire anglais  \n",
    "target_seq, target_tokenizer = tokenize(data.Français) # phrases tokenisées en fonction du vocabulaire français \n",
    "# input_tokenizer: est le tokenizer entraîné sur le jeu de données des phrases anglaises\n",
    "\n",
    "# Calcul de la taille des vocabulaires via l'attribut .word_index qui transforme l'index d'un mot en sa chaîne de caractères \n",
    "vocab_size_inp = len(input_tokenizer.word_index)+1 # nombre de mots différents en français \n",
    "vocab_size_targ = len(target_tokenizer.word_index)+1 # nombre de mots différents en anglais \n",
    "\n",
    "# Calcul de longeur maximale des séquences \n",
    "max_length_inp, max_length_targ = input_seq.shape[1], target_seq.shape[1] \n",
    "\n",
    "print(\"Taille du vocabulaire :\")\n",
    "print(\" - input :\", vocab_size_inp)\n",
    "print(\" - target :\", vocab_size_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00687e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du batch input :  (64, 53)\n",
      "Dimensions du batch target :  (64, 65)\n"
     ]
    }
   ],
   "source": [
    "# Séparation des données en un ensemble d'apprentissage et de test\n",
    "\n",
    "# Création des ensembles d'apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_seq, target_seq,test_size=0.2)\n",
    "\n",
    "# Paramètres de l'entrainement\n",
    "# buffer_size: permet de déterminer combien d'éléments vont être mélangés au fur et à mesure, \n",
    "\n",
    "# batch_size: sépare dans l'ordre le dataset en jeux de données de dimension fixe\n",
    "batch_size = 64\n",
    "buffer_size = int(len(X_train)) \n",
    "steps_per_epoch = buffer_size//batch_size\n",
    "\n",
    "# Création du dataset d'entrainement\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Affichage des deux composantes d'un jeu de données\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(\"Dimensions du batch input : \", example_input_batch.shape)\n",
    "print(\"Dimensions du batch target : \", example_target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb816eb2",
   "metadata": {},
   "source": [
    "## Construction du modèle Seq2Seq avec le nouveau jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a1b66",
   "metadata": {},
   "source": [
    "### Encodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb33a0",
   "metadata": {},
   "source": [
    "Nous avons sélectionné un encodeur avec une couche embedding avec une matrice de dimension 300 et une couche GRU (gated recurrent unit) avec 512 neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1d4852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la sortie de l'Encodeur : \n",
      " (batch size, max_length_inp, latent_dim) -> (64, 53, 512)\n",
      "\n",
      "Dimensions de l'état caché :\n",
      " (batch size, latent_dim) -> (64, 512)\n"
     ]
    }
   ],
   "source": [
    "# Définition de l'encodeur \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim):\n",
    "        # vocab_size: taille du vocabulaire dans la langue de départ\n",
    "        # embedding_dim: dimension de la matrice d'embedding \n",
    "        # latent_dim : dimension de l'état caché (units)\n",
    "        \n",
    "        super(Encoder, self).__init__() # pour faciliter des questions d'héritage \n",
    "        \n",
    "        self.units = latent_dim\n",
    "        \n",
    "        # sélection de l'embedding, fonction qui permet de vectoriser les mots représentés précédemment par leur index \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # sélection de la cellule de calcul GRU \n",
    "        self.gru = tf.keras.layers.GRU(self.units, # nombre d'états cachés - neurones  \n",
    "                                       return_sequences=True, # retourne une séquence/ bidirectionelle\n",
    "                                       return_state=True, # permet de conserver la mémoire de l'encodeur et le rend disponible pour le décodeur \n",
    "                                       recurrent_initializer='glorot_uniform') # initialisation des poids de la matrice utilisée pour la transformation linéaire des états récurrents par distribution uniforme \n",
    "      \n",
    "    def call(self, x, hidden): # calcul des sorties du modèle \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units)) # définition d'une matrice de dim jeu de données fixé * nombre d'états cachés \n",
    "\n",
    "\n",
    "# Paramètres du modèle\n",
    "\n",
    "latent_dim = 512 # nombre d'états cachés (2 puissance 9)\n",
    "embedding_dim = 300 # dimension de la matrice d'embedding\n",
    "\n",
    "encoder = Encoder(vocab_size_inp, embedding_dim, latent_dim)\n",
    "\n",
    "hidden = encoder.initialize_hidden_state(batch_size) # état caché\n",
    "enc_output, hidden = encoder(example_input_batch, hidden) # sortie\n",
    "\n",
    "print('Dimensions de la sortie de l\\'Encodeur : ')\n",
    "print(' (batch size, max_length_inp, latent_dim) -> {}'.format(enc_output.shape))\n",
    "print()\n",
    "\n",
    "print('Dimensions de l\\'état caché :')\n",
    "print(' (batch size, latent_dim) -> {}'.format(hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a617b7e",
   "metadata": {},
   "source": [
    "### Mécanisme d'attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08b71665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la sortie de la couche d'attention :\n",
      " (batch size, latent_dim) (64, 512)\n",
      "Dimensions des poids d'attention :\n",
      " (batch_size, max_length_inp, 1) (64, 53, 1)\n"
     ]
    }
   ],
   "source": [
    "# définition de la classe \n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # définition de 3 couches de neuronnes denses \n",
    "        self.W1 = tf.keras.layers.Dense(units)# couche de l'état caché \n",
    "        self.W2 = tf.keras.layers.Dense(units) # couche de sortie de l'encodeur\n",
    "        self.V = tf.keras.layers.Dense(1) # couche de sortie du mécanisme d'attention  \n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        # dimensions de 'hidden' : (batch_size, units)\n",
    "        # dimensions de 'enc_output' : (batch_size, max_length_input, units)\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # dimensions de 'hidden_with_time_axis' : (batch_size, 1, units)\n",
    "        # ce changement de dimension est nécessaire pour le calcul du score\n",
    "        \n",
    "        # Application de la formule score = FC(tanh(FC(EO) + FC(H))\n",
    "        # avec FC: couche Fully-connected (dense layer) c'est-à-dire W1 et W2\n",
    "        # EO: sortie de l’Encodeur, encoder_output \n",
    "        # H : état caché, hidden_state. \n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(hidden_with_time_axis) + self.W2(enc_output)))\n",
    "        # dimensions de 'score' : (batch_size, max_length_inp, 1)\n",
    "        # dimensions avant d'appliquer 'self.V' : (batch_size, max_length_inp, units)\n",
    "\n",
    "        # calcul des poids d'attention via la fonction softmax \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # dimensions de 'attention_weights' : (batch_size, max_length_inp, 1)\n",
    "        \n",
    "        # calcul du vecteur contexte par produit scalaire\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        # dimensions de context_vector après somme (batch_size, units)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# paramétrage\n",
    "\n",
    "attention_layer = BahdanauAttention(latent_dim)\n",
    "attention_result, attention_weights = attention_layer(hidden, enc_output)\n",
    "\n",
    "print(\"Dimensions de la sortie de la couche d'attention :\")\n",
    "print(\" (batch size, latent_dim) {}\".format(attention_result.shape))\n",
    "print(\"Dimensions des poids d'attention :\")\n",
    "print(\" (batch_size, max_length_inp, 1) {}\".format(attention_weights.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b2a4f",
   "metadata": {},
   "source": [
    "### Décodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4441a7",
   "metadata": {},
   "source": [
    "Le décodeur est constitué des mêmes couches que l'Encodeur avec la notion d'attention ajoutée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff20b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de la sortie du Décodeur :\n",
      " (batch_size, vocab size) -> (64, 22089)\n"
     ]
    }
   ],
   "source": [
    "# Définition du décodeur \n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim, attention_layer):\n",
    "        # même architecture que l'encodeur\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.units = latent_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        # couche dense comprenant le vocabulaire de la langue cible \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # prise en compte de la notion d'attention en input du décodeur \n",
    "        self.attention = attention_layer(latent_dim)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # dimensions de 'enc_output' : (batch_size, max_length_inp, latent_dim)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        # dimensions de 'context_vector' : (batch size, latent_dim)\n",
    "        # dimensions de 'attention_weights' : (batch_size, max_length_inp, 1)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        # dimensions de 'x' : (batch_size, 1, embedding_dim)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # dimensions de 'x' : (batch_size, 1, embedding_dim + latent_dim)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # dimensions de 'output' : (batch_size * 1, latent_dim )\n",
    "\n",
    "        output = self.fc(output)\n",
    "        # dimensions de 'output' : (batch_size, vocab)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "# paramétrage \n",
    "\n",
    "decoder = Decoder(vocab_size_targ, embedding_dim, latent_dim, BahdanauAttention)\n",
    "\n",
    "dec_input = tf.random.uniform((batch_size, 1))\n",
    "dec_output, _, _ = decoder(dec_input, hidden, enc_output)\n",
    "\n",
    "print ('Dimension de la sortie du Décodeur :')\n",
    "print(' (batch_size, vocab size) -> {}'.format(dec_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce0839",
   "metadata": {},
   "source": [
    "### Fonction de coût et optimiseur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1f122c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de coût et optimiseur\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # calcul de l'entropie croisée à partir de la fonction logits entre la valeur réelle et la valeur prédite  \n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
    "    # masque de même taille que la séquence, qui contient 0 si la valeur de l'élement de la séquence est nulle, 1 sinon\n",
    "    # ce masque permet de contrer l'effet du padding sur la fonction loss\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask # application de ce masque comme poids lors du calcul de l'erreur \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # application de la méthode d'optimisation réalisée à partir de la fonction Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4c2c4",
   "metadata": {},
   "source": [
    "## Entrainement du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f19c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Entrainement du modèle \n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_initial_hidden):\n",
    "    \n",
    "    loss = 0 # initisalisation de la perte\n",
    "    \n",
    "    # utilisation d'une fonction de perte de Gradient \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = encoder(inp, enc_initial_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        # parcours de chaque élement de 'targ'\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # appel au Décodeur\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            # calcul de la perte\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # mise à jour de la variable 'dec_input'\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        # calcul des variables du modèle\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    #calcul du gradient du modèle\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # optimisation des variables du modèle\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fb8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.3994\n",
      "Epoch 1 Batch 100 Loss 0.8208\n",
      "Epoch 1 Batch 200 Loss 0.6995\n",
      "Epoch 1 Batch 300 Loss 0.6811\n",
      "Epoch 1 Batch 400 Loss 0.6407\n",
      "Epoch 1 Batch 500 Loss 0.5771\n",
      "Epoch 1 Batch 600 Loss 0.6083\n",
      "Epoch 1 Batch 700 Loss 0.5530\n",
      "Epoch 1 Batch 800 Loss 0.5415\n",
      "Epoch 1 Batch 900 Loss 0.5397\n",
      "Epoch 1 Batch 1000 Loss 0.4662\n",
      "Epoch 1 Batch 1100 Loss 0.5053\n",
      "Epoch 1 Batch 1200 Loss 0.4646\n",
      "Epoch 1 Batch 1300 Loss 0.4169\n",
      "Epoch 1 Batch 1400 Loss 0.4597\n",
      "Epoch 1 Batch 1500 Loss 0.4696\n",
      "Epoch 1 Batch 1600 Loss 0.4230\n",
      "Epoch 1 Batch 1700 Loss 0.4386\n",
      "Epoch 1 Batch 1800 Loss 0.4011\n",
      "Epoch 1 Batch 1900 Loss 0.3625\n",
      "Epoch 1 Loss 0.5445\n",
      "Time taken for 1 epoch 828.79 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3014\n",
      "Epoch 2 Batch 100 Loss 0.3884\n",
      "Epoch 2 Batch 200 Loss 0.3649\n",
      "Epoch 2 Batch 300 Loss 0.3478\n",
      "Epoch 2 Batch 400 Loss 0.2817\n",
      "Epoch 2 Batch 500 Loss 0.2977\n",
      "Epoch 2 Batch 600 Loss 0.3292\n",
      "Epoch 2 Batch 700 Loss 0.3270\n",
      "Epoch 2 Batch 800 Loss 0.3053\n",
      "Epoch 2 Batch 900 Loss 0.3180\n",
      "Epoch 2 Batch 1000 Loss 0.2708\n",
      "Epoch 2 Batch 1100 Loss 0.3216\n",
      "Epoch 2 Batch 1200 Loss 0.3185\n",
      "Epoch 2 Batch 1300 Loss 0.2362\n",
      "Epoch 2 Batch 1400 Loss 0.2797\n",
      "Epoch 2 Batch 1500 Loss 0.2752\n",
      "Epoch 2 Batch 1600 Loss 0.2753\n",
      "Epoch 2 Batch 1700 Loss 0.2581\n",
      "Epoch 2 Batch 1800 Loss 0.2946\n",
      "Epoch 2 Batch 1900 Loss 0.2839\n",
      "Epoch 2 Loss 0.2939\n",
      "Time taken for 1 epoch 747.71 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2047\n",
      "Epoch 3 Batch 100 Loss 0.2389\n",
      "Epoch 3 Batch 200 Loss 0.1874\n",
      "Epoch 3 Batch 300 Loss 0.1814\n",
      "Epoch 3 Batch 400 Loss 0.1687\n",
      "Epoch 3 Batch 500 Loss 0.1742\n",
      "Epoch 3 Batch 600 Loss 0.2147\n",
      "Epoch 3 Batch 700 Loss 0.2244\n",
      "Epoch 3 Batch 800 Loss 0.2257\n",
      "Epoch 3 Batch 900 Loss 0.1777\n",
      "Epoch 3 Batch 1000 Loss 0.1576\n",
      "Epoch 3 Batch 1100 Loss 0.1799\n",
      "Epoch 3 Batch 1200 Loss 0.1890\n",
      "Epoch 3 Batch 1300 Loss 0.1786\n",
      "Epoch 3 Batch 1400 Loss 0.1990\n",
      "Epoch 3 Batch 1500 Loss 0.2090\n",
      "Epoch 3 Batch 1600 Loss 0.2046\n",
      "Epoch 3 Batch 1700 Loss 0.1821\n",
      "Epoch 3 Batch 1800 Loss 0.1702\n",
      "Epoch 3 Batch 1900 Loss 0.1918\n",
      "Epoch 3 Loss 0.2023\n",
      "Time taken for 1 epoch 758.64 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1680\n",
      "Epoch 4 Batch 100 Loss 0.1564\n",
      "Epoch 4 Batch 200 Loss 0.1239\n",
      "Epoch 4 Batch 300 Loss 0.1343\n",
      "Epoch 4 Batch 400 Loss 0.1261\n",
      "Epoch 4 Batch 500 Loss 0.1543\n",
      "Epoch 4 Batch 600 Loss 0.1362\n",
      "Epoch 4 Batch 700 Loss 0.1411\n",
      "Epoch 4 Batch 800 Loss 0.1513\n",
      "Epoch 4 Batch 900 Loss 0.1758\n",
      "Epoch 4 Batch 1000 Loss 0.1515\n",
      "Epoch 4 Batch 1100 Loss 0.1766\n",
      "Epoch 4 Batch 1200 Loss 0.1500\n",
      "Epoch 4 Batch 1300 Loss 0.1772\n",
      "Epoch 4 Batch 1400 Loss 0.1548\n",
      "Epoch 4 Batch 1500 Loss 0.1140\n",
      "Epoch 4 Batch 1600 Loss 0.1286\n",
      "Epoch 4 Batch 1700 Loss 0.1606\n",
      "Epoch 4 Batch 1800 Loss 0.1553\n",
      "Epoch 4 Batch 1900 Loss 0.1571\n",
      "Epoch 4 Loss 0.1566\n",
      "Time taken for 1 epoch 738.19 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1140\n",
      "Epoch 5 Batch 100 Loss 0.1093\n",
      "Epoch 5 Batch 200 Loss 0.1156\n",
      "Epoch 5 Batch 300 Loss 0.1154\n",
      "Epoch 5 Batch 400 Loss 0.1178\n",
      "Epoch 5 Batch 500 Loss 0.1057\n",
      "Epoch 5 Batch 600 Loss 0.1293\n",
      "Epoch 5 Batch 700 Loss 0.1089\n",
      "Epoch 5 Batch 800 Loss 0.1497\n",
      "Epoch 5 Batch 900 Loss 0.1388\n",
      "Epoch 5 Batch 1000 Loss 0.1682\n",
      "Epoch 5 Batch 1100 Loss 0.1528\n",
      "Epoch 5 Batch 1200 Loss 0.1232\n",
      "Epoch 5 Batch 1300 Loss 0.1578\n",
      "Epoch 5 Batch 1400 Loss 0.1375\n",
      "Epoch 5 Batch 1500 Loss 0.1594\n",
      "Epoch 5 Batch 1600 Loss 0.1175\n",
      "Epoch 5 Batch 1700 Loss 0.0975\n",
      "Epoch 5 Batch 1800 Loss 0.1283\n",
      "Epoch 5 Batch 1900 Loss 0.1390\n",
      "Epoch 5 Loss 0.1262\n",
      "Time taken for 1 epoch 758.67 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1010\n",
      "Epoch 6 Batch 100 Loss 0.0961\n",
      "Epoch 6 Batch 200 Loss 0.1132\n",
      "Epoch 6 Batch 300 Loss 0.1060\n",
      "Epoch 6 Batch 400 Loss 0.0943\n",
      "Epoch 6 Batch 500 Loss 0.1155\n",
      "Epoch 6 Batch 600 Loss 0.0936\n",
      "Epoch 6 Batch 700 Loss 0.1116\n",
      "Epoch 6 Batch 800 Loss 0.0919\n",
      "Epoch 6 Batch 900 Loss 0.1032\n",
      "Epoch 6 Batch 1000 Loss 0.0954\n",
      "Epoch 6 Batch 1100 Loss 0.1013\n",
      "Epoch 6 Batch 1200 Loss 0.0999\n",
      "Epoch 6 Batch 1300 Loss 0.1267\n",
      "Epoch 6 Batch 1400 Loss 0.1269\n",
      "Epoch 6 Batch 1500 Loss 0.1324\n",
      "Epoch 6 Batch 1600 Loss 0.0758\n",
      "Epoch 6 Batch 1700 Loss 0.0991\n",
      "Epoch 6 Batch 1800 Loss 0.1199\n",
      "Epoch 6 Batch 1900 Loss 0.1012\n",
      "Epoch 6 Loss 0.1074\n",
      "Time taken for 1 epoch 758.85 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1025\n",
      "Epoch 7 Batch 100 Loss 0.0920\n",
      "Epoch 7 Batch 200 Loss 0.1077\n",
      "Epoch 7 Batch 300 Loss 0.0809\n",
      "Epoch 7 Batch 400 Loss 0.0776\n",
      "Epoch 7 Batch 500 Loss 0.0611\n",
      "Epoch 7 Batch 600 Loss 0.0965\n",
      "Epoch 7 Batch 700 Loss 0.0926\n",
      "Epoch 7 Batch 800 Loss 0.1000\n",
      "Epoch 7 Batch 900 Loss 0.1192\n",
      "Epoch 7 Batch 1000 Loss 0.0934\n",
      "Epoch 7 Batch 1100 Loss 0.0823\n",
      "Epoch 7 Batch 1200 Loss 0.0815\n",
      "Epoch 7 Batch 1300 Loss 0.0859\n",
      "Epoch 7 Batch 1400 Loss 0.1075\n",
      "Epoch 7 Batch 1500 Loss 0.1124\n",
      "Epoch 7 Batch 1600 Loss 0.0965\n",
      "Epoch 7 Batch 1700 Loss 0.0889\n",
      "Epoch 7 Batch 1800 Loss 0.1078\n",
      "Epoch 7 Batch 1900 Loss 0.0761\n",
      "Epoch 7 Loss 0.0940\n",
      "Time taken for 1 epoch 763.26 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0669\n",
      "Epoch 8 Batch 100 Loss 0.0750\n",
      "Epoch 8 Batch 200 Loss 0.0556\n",
      "Epoch 8 Batch 300 Loss 0.0792\n",
      "Epoch 8 Batch 400 Loss 0.0658\n",
      "Epoch 8 Batch 500 Loss 0.0732\n",
      "Epoch 8 Batch 600 Loss 0.0856\n",
      "Epoch 8 Batch 700 Loss 0.0852\n",
      "Epoch 8 Batch 800 Loss 0.0794\n",
      "Epoch 8 Batch 900 Loss 0.0922\n",
      "Epoch 8 Batch 1000 Loss 0.1037\n",
      "Epoch 8 Batch 1100 Loss 0.0804\n",
      "Epoch 8 Batch 1200 Loss 0.0955\n",
      "Epoch 8 Batch 1300 Loss 0.0814\n",
      "Epoch 8 Batch 1400 Loss 0.0950\n",
      "Epoch 8 Batch 1500 Loss 0.0795\n",
      "Epoch 8 Batch 1600 Loss 0.0905\n",
      "Epoch 8 Batch 1700 Loss 0.1057\n",
      "Epoch 8 Batch 1800 Loss 0.1006\n",
      "Epoch 8 Batch 1900 Loss 0.0748\n",
      "Epoch 8 Loss 0.0841\n",
      "Time taken for 1 epoch 761.10 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0621\n",
      "Epoch 9 Batch 100 Loss 0.0693\n",
      "Epoch 9 Batch 200 Loss 0.0731\n",
      "Epoch 9 Batch 300 Loss 0.0648\n",
      "Epoch 9 Batch 400 Loss 0.0911\n",
      "Epoch 9 Batch 500 Loss 0.0806\n",
      "Epoch 9 Batch 600 Loss 0.0753\n",
      "Epoch 9 Batch 700 Loss 0.0856\n",
      "Epoch 9 Batch 800 Loss 0.0780\n",
      "Epoch 9 Batch 900 Loss 0.0683\n",
      "Epoch 9 Batch 1000 Loss 0.0653\n",
      "Epoch 9 Batch 1100 Loss 0.0940\n",
      "Epoch 9 Batch 1200 Loss 0.0779\n",
      "Epoch 9 Batch 1300 Loss 0.0669\n",
      "Epoch 9 Batch 1400 Loss 0.0753\n",
      "Epoch 9 Batch 1500 Loss 0.0938\n",
      "Epoch 9 Batch 1600 Loss 0.0765\n",
      "Epoch 9 Batch 1700 Loss 0.0812\n",
      "Epoch 9 Batch 1800 Loss 0.0860\n",
      "Epoch 9 Batch 1900 Loss 0.0744\n",
      "Epoch 9 Loss 0.0743\n",
      "Time taken for 1 epoch 770.95 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0584\n",
      "Epoch 10 Batch 100 Loss 0.0699\n",
      "Epoch 10 Batch 200 Loss 0.0547\n",
      "Epoch 10 Batch 300 Loss 0.0597\n",
      "Epoch 10 Batch 400 Loss 0.0629\n",
      "Epoch 10 Batch 500 Loss 0.0728\n",
      "Epoch 10 Batch 600 Loss 0.0634\n",
      "Epoch 10 Batch 700 Loss 0.0813\n",
      "Epoch 10 Batch 800 Loss 0.0735\n",
      "Epoch 10 Batch 900 Loss 0.0744\n",
      "Epoch 10 Batch 1000 Loss 0.0760\n",
      "Epoch 10 Batch 1100 Loss 0.0570\n",
      "Epoch 10 Batch 1200 Loss 0.0707\n",
      "Epoch 10 Batch 1300 Loss 0.0485\n",
      "Epoch 10 Batch 1400 Loss 0.0961\n",
      "Epoch 10 Batch 1500 Loss 0.0735\n",
      "Epoch 10 Batch 1600 Loss 0.0947\n",
      "Epoch 10 Batch 1700 Loss 0.0740\n",
      "Epoch 10 Batch 1800 Loss 0.0755\n",
      "Epoch 10 Batch 1900 Loss 0.0864\n",
      "Epoch 10 Loss 0.0686\n",
      "Time taken for 1 epoch 748.17 sec\n",
      "\n",
      "Time elapsed :7634.33s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Evaluation des performances pendant l'entrainement du modèle \n",
    "# avec suivi de la fonction de perte tous les 100 batchs dans chaque époque\n",
    "\n",
    "# Checkpoint - permet d'enregistrer le modèle\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cp.ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "def train(n_epoch = 10):\n",
    "\n",
    "    t=time.time()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state(batch_size)\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "\n",
    "        # conservation du modèle tout les 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                          total_loss / steps_per_epoch))\n",
    "\n",
    "        print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))\n",
    "\n",
    "\n",
    "    print(\"Time elapsed :{}s\".format(round(time.time() - t,2)))\n",
    "\n",
    "train(n_epoch = 10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10911599",
   "metadata": {},
   "source": [
    "## Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1826b9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x220165367b8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cp.ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed14b8",
   "metadata": {},
   "source": [
    "## Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c88127de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction de traduction \n",
    "\n",
    "def translate(sentence, is_seq = False):\n",
    "    \n",
    "    # Initialisation de la matrice d'attention\n",
    "    attention_matrix = np.zeros((max_length_targ, max_length_inp))    \n",
    "    \n",
    "    # permet de transformer l'entrée sous forme de phrase en séquence (pour de nouvelles phrases non séquencées)\n",
    "    if not(is_seq):\n",
    "        # prétraitement de la phrase d'entrée\n",
    "        sentence = clean_sentence(sentence)\n",
    "        # transformation de la phrase en séquence\n",
    "        sentence = input_tokenizer.texts_to_sequences([sentence])\n",
    "        sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence,\n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "        \n",
    "    # initialisation des variables\n",
    "    hidden = [tf.zeros((1, latent_dim))]\n",
    "    enc_out, enc_hidden = encoder(sentence, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    # remplissage du premier élement par l'indice associé à la balise <start>\n",
    "    dec_input = tf.expand_dims([input_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    stop_condition = False # initialisation du status de la traduction -> True correspond à la balise <\"end\">\n",
    "    words = [] # initialisation de la phrase de sortie\n",
    "    t = 0\n",
    "\n",
    "    while not stop_condition:\n",
    "        \n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # conservation des poids d'attention\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # utilisation du mot ayant la meilleure probabilité pour la prédiction \n",
    "        predicted_index = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if predicted_index != 0:\n",
    "            word = target_tokenizer.index_word[predicted_index]\n",
    "        else :\n",
    "            word = ''\n",
    "\n",
    "        # retour de la prédiction id dans le modèle \n",
    "        dec_input = tf.expand_dims([predicted_index], 0)\n",
    "        \n",
    "        # verification des conditions de sortie: \n",
    "        if (word == '<end>' or # la balise <end> \n",
    "           t >= max_length_targ-1): # longeur maximale atteinte\n",
    "            \n",
    "            stop_condition = True\n",
    "            break\n",
    "            \n",
    "         # ajout du mot à la phrase de sortie    \n",
    "        words.append(word)\n",
    "        \n",
    "        t+=1\n",
    "\n",
    "    return \" \".join(words), attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b39c30f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('les etats unis est pluvieux .',\n",
       " array([[6.92999065e-02, 4.39389586e-01, 3.09402227e-01, ...,\n",
       "         2.10158364e-03, 2.09905999e-03, 2.09666323e-03],\n",
       "        [3.49451415e-02, 5.24549961e-01, 3.13057631e-01, ...,\n",
       "         9.25402201e-05, 9.25343993e-05, 9.25287459e-05],\n",
       "        [4.70855981e-02, 3.27038378e-01, 3.46358865e-01, ...,\n",
       "         3.57802201e-05, 3.58014513e-05, 3.58226935e-05],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"United States is rainy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df538eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30977/30977 [38:14<00:00, 13.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test[i], input_tokenizer)\n",
    "        y_pred, attention_matrix = translate(X_test[i].reshape((-1,max_length_inp)), is_seq = True)\n",
    "        y_true = find_sentence(y_test[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_greedy2.csv', index=False)\n",
    "        \n",
    "test(len(X_test))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156ec22",
   "metadata": {},
   "source": [
    "## Evaluation des résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb1c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhedh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; they can add something if they wish ....</td>\n",
       "      <td>ils peuvent ajouter autre chose s ils le sou...</td>\n",
       "      <td>ils peuvent savoir quelque chose si elles le s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; my father stopped smoking . &lt;end&gt;</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; she will report directly to me . &lt;end&gt;</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; he not only speaks french , but he sp...</td>\n",
       "      <td>il ne parle pas seulement en francais , mais...</td>\n",
       "      <td>il ne parle pas seulement le francais , mais a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i can follow the rules . &lt;end&gt;</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> they can add something if they wish ....   \n",
       "1          <start> my father stopped smoking . <end>   \n",
       "2     <start> she will report directly to me . <end>   \n",
       "3   <start> he not only speaks french , but he sp...   \n",
       "4             <start> i can follow the rules . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    ils peuvent ajouter autre chose s ils le sou...   \n",
       "1                      mon pere a arrete de fumer .    \n",
       "2              elle me sera directement rattachee .    \n",
       "3    il ne parle pas seulement en francais , mais...   \n",
       "4                       je peux suivre les regles .    \n",
       "\n",
       "                                       Sortie_modele  \n",
       "0  ils peuvent savoir quelque chose si elles le s...  \n",
       "1                       mon pere a arrete de fumer .  \n",
       "2               elle me sera directement rattachee .  \n",
       "3  il ne parle pas seulement le francais , mais a...  \n",
       "4                        je peux suivre les regles .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports nécessaires \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# récupération des résultats sous forme d'un dataframe \n",
    "df_results=pd.read_csv('results_mod4_greedy2.csv')\n",
    "\n",
    "# retrait des balises <start> et <end> de la colonne \"Phrase_traduite\"\n",
    "phrase_purgee=[]\n",
    "for phrase in df_results['Phrase_traduite']:\n",
    "    phrase = phrase.replace('<start>','')\n",
    "    phrase = phrase.replace('<end>', '')\n",
    "    phrase_purgee.append(phrase)\n",
    "df_results['Phrase_traduite']=phrase_purgee    \n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ac21d",
   "metadata": {},
   "source": [
    "Evaluer les résultats entre la \"Phrase_traduite\" et la phrase \"Sortie_modele\" n'est pas immédiat car celles-ci peuvent être différentes mais correspondre toutes deux à une traduction possible. Il est également difficile de juger de la syntaxe et de l'orthographe des phrases. \n",
    "\n",
    "Nous avons utilisé deux indicateurs pour tenter de mieux comprendre les résultats: \n",
    "- vérification du nombre de mots dans chaque des phrases\n",
    "- comparaison des racinisation des mots de chaque des phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15945ef1",
   "metadata": {},
   "source": [
    "### Vérification du nombre de mots dans chaque des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8d7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'Nb_words_cible' fournissant le nombre de mots dans la phrase cible \n",
    "nb_words_cible=[]\n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "    nb_words_cible.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_cible']=nb_words_cible\n",
    "\n",
    "# Création d'une colonne 'Nb_words_mod' fournissant le nombre de mots dans la phrase prédite par le modèle \n",
    "nb_words_mod=[]\n",
    "for sentence in df_results['Sortie_modele']:\n",
    "    nb_words_mod.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_mod']=nb_words_mod\n",
    "\n",
    "# Création d'une colonne diff\n",
    "df_results['Différence']=df_results['nb_words_cible']-df_results['nb_words_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412e817d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df_results.Différence)\n",
    "plt.title(\"Histogramme représentant la différence de nombre de mots entre la phrase traduite et la phrase sortie du modèle\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283d96e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La majorité des phrases sorties du modèle, soit 83.57 %, ne présente au plus qu'un mot d'écart avec la phrase traduite.\n"
     ]
    }
   ],
   "source": [
    "print(\"La majorité des phrases sorties du modèle, soit\", \n",
    "      round(len(df_results.loc[(df_results.Différence <= 1)&(df_results.Différence >= -1)])/df_results.shape[0]*100, 2),  \n",
    "      \"%, ne présente au plus qu'un mot d'écart avec la phrase traduite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc63222",
   "metadata": {},
   "source": [
    "Cet histogramme montre qu'une très grande partie des phrases contiennent le même nombre de mots dans les deux cas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a39bd7",
   "metadata": {},
   "source": [
    "### Comparaison des racinisation des mots de chaque des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0850ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhedh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer stopwords de la classe nltk.corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Initialisation de la liste des mots vides en français puis ajout de certains mots \n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words.update([\",\", \".\", \":\", \";\", \"!\", \"?\"])\n",
    "\n",
    "# Intialisation de la racinisation en français \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e42cfeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>nb_words_cible</th>\n",
       "      <th>nb_words_mod</th>\n",
       "      <th>Différence</th>\n",
       "      <th>racine_cible</th>\n",
       "      <th>racine_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; they can add something if they wish ....</td>\n",
       "      <td>ils peuvent ajouter autre chose s ils le sou...</td>\n",
       "      <td>ils peuvent savoir quelque chose si elles le s...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"il\", \"peuvent\", \"ajout\", \"autr\", \"chos\", \"s\"...</td>\n",
       "      <td>[\"il\", \"peuvent\", \"savoir\", \"quelqu\", \"chos\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; my father stopped smoking . &lt;end&gt;</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"mon\", \"per\", \"a\", \"arret\", \"de\", \"fum\", \".\"]</td>\n",
       "      <td>[\"mon\", \"per\", \"a\", \"arret\", \"de\", \"fum\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; she will report directly to me . &lt;end&gt;</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"elle\", \"me\", \"ser\", \"direct\", \"rattache\", \".\"]</td>\n",
       "      <td>[\"elle\", \"me\", \"ser\", \"direct\", \"rattache\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; he not only speaks french , but he sp...</td>\n",
       "      <td>il ne parle pas seulement en francais , mais...</td>\n",
       "      <td>il ne parle pas seulement le francais , mais a...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"il\", \"ne\", \"parl\", \"pas\", \"seul\", \"en\", \"fra...</td>\n",
       "      <td>[\"il\", \"ne\", \"parl\", \"pas\", \"seul\", \"le\", \"fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i can follow the rules . &lt;end&gt;</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"je\", \"peux\", \"suivr\", \"le\", \"regl\", \".\"]</td>\n",
       "      <td>[\"je\", \"peux\", \"suivr\", \"le\", \"regl\", \".\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> they can add something if they wish ....   \n",
       "1          <start> my father stopped smoking . <end>   \n",
       "2     <start> she will report directly to me . <end>   \n",
       "3   <start> he not only speaks french , but he sp...   \n",
       "4             <start> i can follow the rules . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    ils peuvent ajouter autre chose s ils le sou...   \n",
       "1                      mon pere a arrete de fumer .    \n",
       "2              elle me sera directement rattachee .    \n",
       "3    il ne parle pas seulement en francais , mais...   \n",
       "4                       je peux suivre les regles .    \n",
       "\n",
       "                                       Sortie_modele  nb_words_cible  \\\n",
       "0  ils peuvent savoir quelque chose si elles le s...              10   \n",
       "1                       mon pere a arrete de fumer .               7   \n",
       "2               elle me sera directement rattachee .               6   \n",
       "3  il ne parle pas seulement le francais , mais a...              13   \n",
       "4                        je peux suivre les regles .               6   \n",
       "\n",
       "   nb_words_mod  Différence  \\\n",
       "0            10           0   \n",
       "1             7           0   \n",
       "2             6           0   \n",
       "3            12           1   \n",
       "4             6           0   \n",
       "\n",
       "                                        racine_cible  \\\n",
       "0  [\"il\", \"peuvent\", \"ajout\", \"autr\", \"chos\", \"s\"...   \n",
       "1     [\"mon\", \"per\", \"a\", \"arret\", \"de\", \"fum\", \".\"]   \n",
       "2   [\"elle\", \"me\", \"ser\", \"direct\", \"rattache\", \".\"]   \n",
       "3  [\"il\", \"ne\", \"parl\", \"pas\", \"seul\", \"en\", \"fra...   \n",
       "4         [\"je\", \"peux\", \"suivr\", \"le\", \"regl\", \".\"]   \n",
       "\n",
       "                                          racine_mod  \n",
       "0  [\"il\", \"peuvent\", \"savoir\", \"quelqu\", \"chos\", ...  \n",
       "1     [\"mon\", \"per\", \"a\", \"arret\", \"de\", \"fum\", \".\"]  \n",
       "2   [\"elle\", \"me\", \"ser\", \"direct\", \"rattache\", \".\"]  \n",
       "3  [\"il\", \"ne\", \"parl\", \"pas\", \"seul\", \"le\", \"fra...  \n",
       "4         [\"je\", \"peux\", \"suivr\", \"le\", \"regl\", \".\"]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application à tout le dataframe \n",
    "\n",
    "# Instanciation des listes nécessaires \n",
    "racine_cible_tot=[]\n",
    "racine_cible=[]\n",
    "racine_mod_tot=[]\n",
    "racine_mod=[]\n",
    "\n",
    "# Création d'une colonne 'racine_cible' fournissant la liste des racines des mots des phrases de la colonne cible \n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "        racine_cible.append(stemmer.stem(mot))\n",
    "    if len(racine_cible) != 0:\n",
    "        racine_cible_tot.append(racine_cible)\n",
    "        racine_cible=[]\n",
    "    else :\n",
    "        racine_cible_tot.append('abc')\n",
    "    \n",
    "df_results['racine_cible']=racine_cible_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_cible'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_cible']]\n",
    "\n",
    "\n",
    "# Création d'une colonne 'racine_mod' fournissant la liste des racines des mots des phrases de la colonne prédite par le modèle  \n",
    "for sentence in df_results['Sortie_modele']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "        racine_mod.append(stemmer.stem(mot))\n",
    "    racine_mod_tot.append(racine_mod)\n",
    "    racine_mod=[]\n",
    "\n",
    "df_results['racine_mod']=racine_mod_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_mod'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_mod']]\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc9dbd",
   "metadata": {},
   "source": [
    "### Etablissement des scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a29232",
   "metadata": {},
   "source": [
    "Nous avons crée deux scores permettant de calculer un score total par la moyenne des deux premiers. \n",
    "\n",
    "Le premier score correspond au ratio du nombre de racines en commun sur le nombre de racines des phrases cibles. \n",
    "\n",
    "Le second score correspond à un calcul lié au nombre de mots d'écart entre les phrases traduites et les phrases sorties du modèle. Pour une phrase sans mot d'écart, le score sera de 1 et ce score diminue avec l'augmentation du nombre de mots d'écart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c03f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne \"comparaison racine\" fournissant le ratio \n",
    "#nombre de racines en commun / nombre de racines des phrases cibles\n",
    "\n",
    "# Création d'une colonne \"comparaison différence\" fournissant le ratio \n",
    "#différence du nombre de mots / nombre de mots cibles\n",
    "\n",
    "score_racine=[]\n",
    "score_difference = []\n",
    "for i in range(df_results.shape[0]):\n",
    "    score_racine.append(len(set(df_results.iloc[i,6]) & set(df_results.iloc[i,7]))/len(df_results.iloc[i,6]))\n",
    "    if (abs(df_results.iloc[i, 5])/df_results.iloc[i,3]) > 1:\n",
    "        score_diff = 0\n",
    "    else :\n",
    "        score_diff = 1 - abs(df_results.iloc[i, 5])/df_results.iloc[i,3]\n",
    "    score_difference.append(score_diff)\n",
    "    \n",
    "df_results['score_racine']=score_racine\n",
    "\n",
    "df_results['score_diff']= score_difference\n",
    "\n",
    "df_results['score_tot']=(df_results['score_diff']+ df_results['score_racine'])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d59752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; they can add something if they wish ....</td>\n",
       "      <td>ils peuvent ajouter autre chose s ils le sou...</td>\n",
       "      <td>ils peuvent savoir quelque chose si elles le s...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; my father stopped smoking . &lt;end&gt;</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; she will report directly to me . &lt;end&gt;</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; he not only speaks french , but he sp...</td>\n",
       "      <td>il ne parle pas seulement en francais , mais...</td>\n",
       "      <td>il ne parle pas seulement le francais , mais a...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i can follow the rules . &lt;end&gt;</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> they can add something if they wish ....   \n",
       "1          <start> my father stopped smoking . <end>   \n",
       "2     <start> she will report directly to me . <end>   \n",
       "3   <start> he not only speaks french , but he sp...   \n",
       "4             <start> i can follow the rules . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    ils peuvent ajouter autre chose s ils le sou...   \n",
       "1                      mon pere a arrete de fumer .    \n",
       "2              elle me sera directement rattachee .    \n",
       "3    il ne parle pas seulement en francais , mais...   \n",
       "4                       je peux suivre les regles .    \n",
       "\n",
       "                                       Sortie_modele  score_racine  \\\n",
       "0  ils peuvent savoir quelque chose si elles le s...      0.500000   \n",
       "1                       mon pere a arrete de fumer .      1.000000   \n",
       "2               elle me sera directement rattachee .      1.000000   \n",
       "3  il ne parle pas seulement le francais , mais a...      0.846154   \n",
       "4                        je peux suivre les regles .      1.000000   \n",
       "\n",
       "   score_diff  score_tot  \n",
       "0    1.000000   0.750000  \n",
       "1    1.000000   1.000000  \n",
       "2    1.000000   1.000000  \n",
       "3    0.923077   0.884615  \n",
       "4    1.000000   1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.drop([\"nb_words_cible\", \"nb_words_mod\", \"Différence\", \"racine_cible\", \"racine_mod\"], axis = 1)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70d401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne du score_racine 0.7895969934571637\n",
      "moyenne du score_diff 0.9157793900759424\n",
      "Score du modèle au test de performance 0.8526881917665909\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne du score_tot\n",
    "\n",
    "print(\"moyenne du score_racine\", df_results['score_racine'].mean())\n",
    "print(\"moyenne du score_diff\", df_results['score_diff'].mean())\n",
    "print(\"Score du modèle au test de performance\", df_results['score_tot'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35c46d",
   "metadata": {},
   "source": [
    "### Score BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff78c6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; they can add something if they wish ....</td>\n",
       "      <td>ils peuvent ajouter autre chose s ils le sou...</td>\n",
       "      <td>ils peuvent savoir quelque chose si elles le s...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; my father stopped smoking . &lt;end&gt;</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; she will report directly to me . &lt;end&gt;</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; he not only speaks french , but he sp...</td>\n",
       "      <td>il ne parle pas seulement en francais , mais...</td>\n",
       "      <td>il ne parle pas seulement le francais , mais a...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.843374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i can follow the rules . &lt;end&gt;</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> they can add something if they wish ....   \n",
       "1          <start> my father stopped smoking . <end>   \n",
       "2     <start> she will report directly to me . <end>   \n",
       "3   <start> he not only speaks french , but he sp...   \n",
       "4             <start> i can follow the rules . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    ils peuvent ajouter autre chose s ils le sou...   \n",
       "1                      mon pere a arrete de fumer .    \n",
       "2              elle me sera directement rattachee .    \n",
       "3    il ne parle pas seulement en francais , mais...   \n",
       "4                       je peux suivre les regles .    \n",
       "\n",
       "                                       Sortie_modele  score_racine  \\\n",
       "0  ils peuvent savoir quelque chose si elles le s...      0.500000   \n",
       "1                       mon pere a arrete de fumer .      1.000000   \n",
       "2               elle me sera directement rattachee .      1.000000   \n",
       "3  il ne parle pas seulement le francais , mais a...      0.846154   \n",
       "4                        je peux suivre les regles .      1.000000   \n",
       "\n",
       "   score_diff  score_tot  Score_bleu  \n",
       "0    1.000000   0.750000    0.500000  \n",
       "1    1.000000   1.000000    1.000000  \n",
       "2    1.000000   1.000000    1.000000  \n",
       "3    0.923077   0.884615    0.843374  \n",
       "4    1.000000   1.000000    1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score_bleu = []\n",
    "for i in range(df_results.shape[0]):\n",
    "    reference = [''.join(df_results.iloc[i, 1]).split()]\n",
    "    candidate = ''.join(df_results.iloc[i, 2]).split()\n",
    "    score_bleu.append(sentence_bleu(reference, candidate, weights = (1,0,0,0)))\n",
    "df_results[\"Score_bleu\"] = score_bleu\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acb811d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score BLEU du modèle 0.7554051591860872\n"
     ]
    }
   ],
   "source": [
    "# Score BLEU du modèle : \n",
    "print(\"Score BLEU du modèle\", df_results['Score_bleu'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717e64e",
   "metadata": {},
   "source": [
    "### Score ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ce494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "r = Rouge()\n",
    "precision_col = []\n",
    "recall_col = []\n",
    "fscore_col = []\n",
    "\n",
    "for i in range(df_results.shape[0]):\n",
    "    reference = df_results.iloc[i, 1]\n",
    "    candidate = df_results.iloc[i, 2]\n",
    "    score_rouge = r.get_scores(candidate, reference)\n",
    "    precision_col.append(score_rouge[0]['rouge-1']['p'])\n",
    "    recall_col.append(score_rouge[0]['rouge-1']['r'])\n",
    "    fscore_col.append(score_rouge[0]['rouge-1']['f'])\n",
    "\n",
    "df_results['Rouge_recall'] = recall_col\n",
    "df_results['Rouge_precision'] = precision_col\n",
    "df_results['Rouge_f1_score'] = fscore_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b444a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE recall du modèle 0.6772868511564343\n",
      "Score ROUGE precision du modèle 0.770816946125865\n",
      "Score ROUGE f1_score du modèle 0.71710511002731\n"
     ]
    }
   ],
   "source": [
    "# Score ROUGE du modèle : \n",
    "print(\"Score ROUGE recall du modèle\", df_results['Rouge_recall'].mean())\n",
    "print(\"Score ROUGE precision du modèle\", df_results['Rouge_precision'].mean())\n",
    "print(\"Score ROUGE f1_score du modèle\", df_results['Rouge_f1_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99eec5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.drop([\"score_racine\", \"score_diff\", \"Rouge_recall\", \"Rouge_precision\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d38833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "      <th>Rouge_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; they can add something if they wish ....</td>\n",
       "      <td>ils peuvent ajouter autre chose s ils le sou...</td>\n",
       "      <td>ils peuvent savoir quelque chose si elles le s...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; my father stopped smoking . &lt;end&gt;</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>mon pere a arrete de fumer .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; she will report directly to me . &lt;end&gt;</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>elle me sera directement rattachee .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; he not only speaks french , but he sp...</td>\n",
       "      <td>il ne parle pas seulement en francais , mais...</td>\n",
       "      <td>il ne parle pas seulement le francais , mais a...</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.843374</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i can follow the rules . &lt;end&gt;</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>je peux suivre les regles .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> they can add something if they wish ....   \n",
       "1          <start> my father stopped smoking . <end>   \n",
       "2     <start> she will report directly to me . <end>   \n",
       "3   <start> he not only speaks french , but he sp...   \n",
       "4             <start> i can follow the rules . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    ils peuvent ajouter autre chose s ils le sou...   \n",
       "1                      mon pere a arrete de fumer .    \n",
       "2              elle me sera directement rattachee .    \n",
       "3    il ne parle pas seulement en francais , mais...   \n",
       "4                       je peux suivre les regles .    \n",
       "\n",
       "                                       Sortie_modele  score_tot  Score_bleu  \\\n",
       "0  ils peuvent savoir quelque chose si elles le s...   0.750000    0.500000   \n",
       "1                       mon pere a arrete de fumer .   1.000000    1.000000   \n",
       "2               elle me sera directement rattachee .   1.000000    1.000000   \n",
       "3  il ne parle pas seulement le francais , mais a...   0.884615    0.843374   \n",
       "4                        je peux suivre les regles .   1.000000    1.000000   \n",
       "\n",
       "   Rouge_f1_score  \n",
       "0        0.444444  \n",
       "1        0.923077  \n",
       "2        0.909091  \n",
       "3        0.869565  \n",
       "4        0.909091  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6e90d",
   "metadata": {},
   "source": [
    "## Création d'un modèle pour le Beam Search Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142928f",
   "metadata": {},
   "source": [
    "### Paramètres d'entrainements et du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0047a72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 53]), TensorShape([64, 65]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size = vocab_size_inp\n",
    "vocab_tar_size = vocab_size_targ\n",
    "max_length_input = max_length_inp\n",
    "max_length_output = max_length_targ\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "units = 1024 # nombre d'états cachés (2 puissance 10)\n",
    "embedding_dim_beam = 256 # dimension de la matrice d'embedding\n",
    "steps_per_epoch = buffer_size//BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7231b",
   "metadata": {},
   "source": [
    "### Encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4946ac",
   "metadata": {},
   "source": [
    "Nous avons choisi ici un encodeur avec une couche d'embedding de dimension 256 et une couche LSTM (long short-term memory) avec 1024 neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160e9be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la sortie de l'Encodeur : (batch size, sequence length, units) (64, 53, 1024)\n",
      "Dimensions du veceur h de l'Encodeur: (batch size, units) (64, 1024)\n",
      "Dimensions du vecteur c de l'Encodeur: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder_beam(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim_beam, enc_units, batch_sz):\n",
    "        # vocab_size: taille du vocabulaire dans la langue de départ\n",
    "        # embedding_dim_beam: dimension de la matrice d'embedding \n",
    "        # enc_units : dimension de l'état caché (units)\n",
    "        #batch_sz = batch size\n",
    "        super(Encoder_beam, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        # sélection de l'embedding, fonction qui permet de vectoriser les mots représentés précédemment par leur index\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim_beam)\n",
    "       \n",
    "\n",
    "        # sélection de la cellule de calcul LSTM\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units, # nombre d'états cachés - neurones\n",
    "                                       return_sequences=True,  # retourne une séquence/ bidirectionelle\n",
    "                                       return_state=True,  # permet de conserver la mémoire de l'encodeur et le rend disponible pour le décodeur\n",
    "                                       recurrent_initializer='glorot_uniform')# initialisation des poids de la matrice utilisée pour la transformation linéaire des états récurrents par distribution uniforme\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
    "\n",
    "encoder_beam = Encoder_beam(vocab_inp_size, embedding_dim_beam, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "sample_hidden = encoder_beam.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder_beam(example_input_batch, sample_hidden)\n",
    "print (\"Dimensions de la sortie de l'Encodeur : (batch size, sequence length, units) {}\".format(sample_output.shape))\n",
    "print (\"Dimensions du veceur h de l'Encodeur: (batch size, units) {}\".format(sample_h.shape))\n",
    "print (\"Dimensions du vecteur c de l'Encodeur: (batch size, units) {}\".format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000ee08",
   "metadata": {},
   "source": [
    "### Décodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79755ab",
   "metadata": {},
   "source": [
    "Nous construisons ensuite le décodeur. Ici, nous n'aurons pas à créer une classe pour le mécanisme d'attention au préalable car nous utiliserons le module tensorflow-addons qui permet d'appliquer directement le mécanisme d'attention désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0d10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (64, 64, 22089)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Decoder_beam(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim_beam, dec_units, batch_sz, attention_type='luong'):\n",
    "        super(Decoder_beam, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim_beam)\n",
    "\n",
    "        #Final Dense layer on which softmax will be applied\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # Define the fundamental cell for decoder recurrent structure\n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "\n",
    "\n",
    "\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                                  None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "        # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "        # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "\n",
    "    def build_rnn_cell(self, batch_sz):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                      self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "\n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='bahdanau'):\n",
    "        # ------------- #\n",
    "        # typ: Which sort of attention (Bahdanau, Luong)\n",
    "        # dec_units: final dimension of attention outputs \n",
    "        # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "        # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "decoder_beam = Decoder_beam(vocab_tar_size, embedding_dim_beam, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder_beam.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder_beam.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder_beam(sample_x, initial_state)\n",
    "\n",
    "print(\"Dimension de sortie du Decodeur: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d3b67",
   "metadata": {},
   "source": [
    "### Fonction de coût et optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b016ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "\n",
    "checkpoint_dir_beam = './training_checkpoints_beam'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir_beam, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder_beam,\n",
    "                                 decoder=decoder_beam)\n",
    "\n",
    "@tf.function\n",
    "def train_step_beam(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder_beam(inp, enc_hidden)\n",
    "\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder_beam.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder_beam.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder_beam(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder_beam.trainable_variables + decoder_beam.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6055981",
   "metadata": {},
   "source": [
    "### Entrainement du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e164c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.3995\n",
      "Epoch 1 Batch 100 Loss 0.7648\n",
      "Epoch 1 Batch 200 Loss 0.7159\n",
      "Epoch 1 Batch 300 Loss 0.6033\n",
      "Epoch 1 Batch 400 Loss 0.5686\n",
      "Epoch 1 Batch 500 Loss 0.5188\n",
      "Epoch 1 Batch 600 Loss 0.5164\n",
      "Epoch 1 Batch 700 Loss 0.5136\n",
      "Epoch 1 Batch 800 Loss 0.4599\n",
      "Epoch 1 Batch 900 Loss 0.4005\n",
      "Epoch 1 Batch 1000 Loss 0.4163\n",
      "Epoch 1 Batch 1100 Loss 0.4242\n",
      "Epoch 1 Batch 1200 Loss 0.3767\n",
      "Epoch 1 Batch 1300 Loss 0.3736\n",
      "Epoch 1 Batch 1400 Loss 0.3222\n",
      "Epoch 1 Batch 1500 Loss 0.3577\n",
      "Epoch 1 Batch 1600 Loss 0.3192\n",
      "Epoch 1 Batch 1700 Loss 0.2799\n",
      "Epoch 1 Batch 1800 Loss 0.2819\n",
      "Epoch 1 Batch 1900 Loss 0.2813\n",
      "Epoch 1 Loss 0.4681\n",
      "Time taken for 1 epoch 1215.6163401603699 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2539\n",
      "Epoch 2 Batch 100 Loss 0.2688\n",
      "Epoch 2 Batch 200 Loss 0.2887\n",
      "Epoch 2 Batch 300 Loss 0.2520\n",
      "Epoch 2 Batch 400 Loss 0.2220\n",
      "Epoch 2 Batch 500 Loss 0.2577\n",
      "Epoch 2 Batch 600 Loss 0.2174\n",
      "Epoch 2 Batch 700 Loss 0.2259\n",
      "Epoch 2 Batch 800 Loss 0.2420\n",
      "Epoch 2 Batch 900 Loss 0.3061\n",
      "Epoch 2 Batch 1000 Loss 0.2374\n",
      "Epoch 2 Batch 1100 Loss 0.2620\n",
      "Epoch 2 Batch 1200 Loss 0.2121\n",
      "Epoch 2 Batch 1300 Loss 0.2348\n",
      "Epoch 2 Batch 1400 Loss 0.1834\n",
      "Epoch 2 Batch 1500 Loss 0.2502\n",
      "Epoch 2 Batch 1600 Loss 0.2090\n",
      "Epoch 2 Batch 1700 Loss 0.2267\n",
      "Epoch 2 Batch 1800 Loss 0.2843\n",
      "Epoch 2 Batch 1900 Loss 0.1614\n",
      "Epoch 2 Loss 0.2345\n",
      "Time taken for 1 epoch 1210.3985259532928 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1702\n",
      "Epoch 3 Batch 100 Loss 0.1875\n",
      "Epoch 3 Batch 200 Loss 0.1394\n",
      "Epoch 3 Batch 300 Loss 0.1642\n",
      "Epoch 3 Batch 400 Loss 0.1605\n",
      "Epoch 3 Batch 500 Loss 0.1665\n",
      "Epoch 3 Batch 600 Loss 0.1537\n",
      "Epoch 3 Batch 700 Loss 0.1612\n",
      "Epoch 3 Batch 800 Loss 0.1787\n",
      "Epoch 3 Batch 900 Loss 0.1465\n",
      "Epoch 3 Batch 1000 Loss 0.1455\n",
      "Epoch 3 Batch 1100 Loss 0.1855\n",
      "Epoch 3 Batch 1200 Loss 0.1789\n",
      "Epoch 3 Batch 1300 Loss 0.1280\n",
      "Epoch 3 Batch 1400 Loss 0.1742\n",
      "Epoch 3 Batch 1500 Loss 0.1769\n",
      "Epoch 3 Batch 1600 Loss 0.1698\n",
      "Epoch 3 Batch 1700 Loss 0.1336\n",
      "Epoch 3 Batch 1800 Loss 0.1731\n",
      "Epoch 3 Batch 1900 Loss 0.1789\n",
      "Epoch 3 Loss 0.1663\n",
      "Time taken for 1 epoch 1209.2292032241821 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1129\n",
      "Epoch 4 Batch 100 Loss 0.1145\n",
      "Epoch 4 Batch 200 Loss 0.1214\n",
      "Epoch 4 Batch 300 Loss 0.0869\n",
      "Epoch 4 Batch 400 Loss 0.0963\n",
      "Epoch 4 Batch 500 Loss 0.1299\n",
      "Epoch 4 Batch 600 Loss 0.1278\n",
      "Epoch 4 Batch 700 Loss 0.1200\n",
      "Epoch 4 Batch 800 Loss 0.1283\n",
      "Epoch 4 Batch 900 Loss 0.1522\n",
      "Epoch 4 Batch 1000 Loss 0.1455\n",
      "Epoch 4 Batch 1100 Loss 0.1290\n",
      "Epoch 4 Batch 1200 Loss 0.1345\n",
      "Epoch 4 Batch 1300 Loss 0.1076\n",
      "Epoch 4 Batch 1400 Loss 0.1316\n",
      "Epoch 4 Batch 1500 Loss 0.1374\n",
      "Epoch 4 Batch 1600 Loss 0.1509\n",
      "Epoch 4 Batch 1700 Loss 0.1244\n",
      "Epoch 4 Batch 1800 Loss 0.1358\n",
      "Epoch 4 Batch 1900 Loss 0.1674\n",
      "Epoch 4 Loss 0.1321\n",
      "Time taken for 1 epoch 1203.7867691516876 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1138\n",
      "Epoch 5 Batch 100 Loss 0.0895\n",
      "Epoch 5 Batch 200 Loss 0.1013\n",
      "Epoch 5 Batch 300 Loss 0.1105\n",
      "Epoch 5 Batch 400 Loss 0.1086\n",
      "Epoch 5 Batch 500 Loss 0.0968\n",
      "Epoch 5 Batch 600 Loss 0.0989\n",
      "Epoch 5 Batch 700 Loss 0.0995\n",
      "Epoch 5 Batch 800 Loss 0.1164\n",
      "Epoch 5 Batch 900 Loss 0.1430\n",
      "Epoch 5 Batch 1000 Loss 0.1133\n",
      "Epoch 5 Batch 1100 Loss 0.0960\n",
      "Epoch 5 Batch 1200 Loss 0.1139\n",
      "Epoch 5 Batch 1300 Loss 0.1347\n",
      "Epoch 5 Batch 1400 Loss 0.1049\n",
      "Epoch 5 Batch 1500 Loss 0.1187\n",
      "Epoch 5 Batch 1600 Loss 0.0974\n",
      "Epoch 5 Batch 1700 Loss 0.1191\n",
      "Epoch 5 Batch 1800 Loss 0.1123\n",
      "Epoch 5 Batch 1900 Loss 0.1123\n",
      "Epoch 5 Loss 0.1105\n",
      "Time taken for 1 epoch 1203.1540937423706 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0656\n",
      "Epoch 6 Batch 100 Loss 0.0864\n",
      "Epoch 6 Batch 200 Loss 0.0827\n",
      "Epoch 6 Batch 300 Loss 0.0625\n",
      "Epoch 6 Batch 400 Loss 0.0892\n",
      "Epoch 6 Batch 500 Loss 0.0879\n",
      "Epoch 6 Batch 600 Loss 0.1198\n",
      "Epoch 6 Batch 700 Loss 0.0980\n",
      "Epoch 6 Batch 800 Loss 0.0927\n",
      "Epoch 6 Batch 900 Loss 0.0913\n",
      "Epoch 6 Batch 1000 Loss 0.0747\n",
      "Epoch 6 Batch 1100 Loss 0.0948\n",
      "Epoch 6 Batch 1200 Loss 0.1102\n",
      "Epoch 6 Batch 1300 Loss 0.1185\n",
      "Epoch 6 Batch 1400 Loss 0.1118\n",
      "Epoch 6 Batch 1500 Loss 0.0939\n",
      "Epoch 6 Batch 1600 Loss 0.0987\n",
      "Epoch 6 Batch 1700 Loss 0.0922\n",
      "Epoch 6 Batch 1800 Loss 0.1174\n",
      "Epoch 6 Batch 1900 Loss 0.0963\n",
      "Epoch 6 Loss 0.0959\n",
      "Time taken for 1 epoch 1202.7981412410736 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0679\n",
      "Epoch 7 Batch 100 Loss 0.0760\n",
      "Epoch 7 Batch 200 Loss 0.0703\n",
      "Epoch 7 Batch 300 Loss 0.0881\n",
      "Epoch 7 Batch 400 Loss 0.0769\n",
      "Epoch 7 Batch 500 Loss 0.0681\n",
      "Epoch 7 Batch 600 Loss 0.0817\n",
      "Epoch 7 Batch 700 Loss 0.0805\n",
      "Epoch 7 Batch 800 Loss 0.0835\n",
      "Epoch 7 Batch 900 Loss 0.0973\n",
      "Epoch 7 Batch 1000 Loss 0.1063\n",
      "Epoch 7 Batch 1100 Loss 0.0715\n",
      "Epoch 7 Batch 1200 Loss 0.1101\n",
      "Epoch 7 Batch 1300 Loss 0.0906\n",
      "Epoch 7 Batch 1400 Loss 0.0728\n",
      "Epoch 7 Batch 1500 Loss 0.0856\n",
      "Epoch 7 Batch 1600 Loss 0.0995\n",
      "Epoch 7 Batch 1700 Loss 0.0609\n",
      "Epoch 7 Batch 1800 Loss 0.0964\n",
      "Epoch 7 Batch 1900 Loss 0.0913\n",
      "Epoch 7 Loss 0.0849\n",
      "Time taken for 1 epoch 1202.3259208202362 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0581\n",
      "Epoch 8 Batch 100 Loss 0.0667\n",
      "Epoch 8 Batch 200 Loss 0.0555\n",
      "Epoch 8 Batch 300 Loss 0.0669\n",
      "Epoch 8 Batch 400 Loss 0.0672\n",
      "Epoch 8 Batch 500 Loss 0.0663\n",
      "Epoch 8 Batch 600 Loss 0.0733\n",
      "Epoch 8 Batch 700 Loss 0.0820\n",
      "Epoch 8 Batch 800 Loss 0.0860\n",
      "Epoch 8 Batch 900 Loss 0.0898\n",
      "Epoch 8 Batch 1000 Loss 0.0835\n",
      "Epoch 8 Batch 1100 Loss 0.0683\n",
      "Epoch 8 Batch 1200 Loss 0.0818\n",
      "Epoch 8 Batch 1300 Loss 0.0776\n",
      "Epoch 8 Batch 1400 Loss 0.0734\n",
      "Epoch 8 Batch 1500 Loss 0.0821\n",
      "Epoch 8 Batch 1600 Loss 0.0648\n",
      "Epoch 8 Batch 1700 Loss 0.0928\n",
      "Epoch 8 Batch 1800 Loss 0.0878\n",
      "Epoch 8 Batch 1900 Loss 0.0741\n",
      "Epoch 8 Loss 0.0766\n",
      "Time taken for 1 epoch 1211.5861077308655 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0562\n",
      "Epoch 9 Batch 100 Loss 0.0641\n",
      "Epoch 9 Batch 200 Loss 0.0569\n",
      "Epoch 9 Batch 300 Loss 0.0634\n",
      "Epoch 9 Batch 400 Loss 0.0714\n",
      "Epoch 9 Batch 500 Loss 0.0640\n",
      "Epoch 9 Batch 600 Loss 0.0743\n",
      "Epoch 9 Batch 700 Loss 0.0781\n",
      "Epoch 9 Batch 800 Loss 0.0672\n",
      "Epoch 9 Batch 900 Loss 0.0679\n",
      "Epoch 9 Batch 1000 Loss 0.0771\n",
      "Epoch 9 Batch 1100 Loss 0.0763\n",
      "Epoch 9 Batch 1200 Loss 0.0906\n",
      "Epoch 9 Batch 1300 Loss 0.0690\n",
      "Epoch 9 Batch 1400 Loss 0.0651\n",
      "Epoch 9 Batch 1500 Loss 0.0619\n",
      "Epoch 9 Batch 1600 Loss 0.0858\n",
      "Epoch 9 Batch 1700 Loss 0.0885\n",
      "Epoch 9 Batch 1800 Loss 0.0802\n",
      "Epoch 9 Batch 1900 Loss 0.0752\n",
      "Epoch 9 Loss 0.0701\n",
      "Time taken for 1 epoch 1219.4306206703186 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0482\n",
      "Epoch 10 Batch 100 Loss 0.0529\n",
      "Epoch 10 Batch 200 Loss 0.0635\n",
      "Epoch 10 Batch 300 Loss 0.0466\n",
      "Epoch 10 Batch 400 Loss 0.0504\n",
      "Epoch 10 Batch 500 Loss 0.0590\n",
      "Epoch 10 Batch 600 Loss 0.0649\n",
      "Epoch 10 Batch 700 Loss 0.0836\n",
      "Epoch 10 Batch 800 Loss 0.0706\n",
      "Epoch 10 Batch 900 Loss 0.0664\n",
      "Epoch 10 Batch 1000 Loss 0.0864\n",
      "Epoch 10 Batch 1100 Loss 0.0715\n",
      "Epoch 10 Batch 1200 Loss 0.0597\n",
      "Epoch 10 Batch 1300 Loss 0.0743\n",
      "Epoch 10 Batch 1400 Loss 0.0816\n",
      "Epoch 10 Batch 1500 Loss 0.0871\n",
      "Epoch 10 Batch 1600 Loss 0.0820\n",
      "Epoch 10 Batch 1700 Loss 0.0578\n",
      "Epoch 10 Batch 1800 Loss 0.0623\n",
      "Epoch 10 Batch 1900 Loss 0.0783\n",
      "Epoch 10 Loss 0.0645\n",
      "Time taken for 1 epoch 1218.6409437656403 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0521\n",
      "Epoch 11 Batch 100 Loss 0.0499\n",
      "Epoch 11 Batch 200 Loss 0.0662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c69679b75ed9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step_beam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder_beam.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step_beam(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30d85ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed : 12092s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print('time elapsed : 12092s')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd9798",
   "metadata": {},
   "source": [
    "### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8f3abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x223ab84ea20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir_beam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136270b",
   "metadata": {},
   "source": [
    "### Traduction avec le Beam Search Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef33e1",
   "metadata": {},
   "source": [
    "Nous créons ici deux fonction :\n",
    "\n",
    "-beam_evaluate_sentences qui prendra en entrée une phrase à traduire et un paramètre k, qui appliquera le principe du Beam Search decoder grâce à la fonction du même nom du module Tensorflow-addons et qui donnera en sortie les k vecteurs ayant le plus de probabilité d'êtres les tradcutions de la phrase en entrée ainsi que ces dites probabilités.\n",
    "\n",
    "-beam_translate qui prendra en entrée une phrase vectorisée et qui convertira ces vecteurs en phrases en utilisant la méthode sequences_to_text du tokenizer utilisé pour vectoriser les phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4403d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=5):\n",
    "    \n",
    "    sentence = sentence.replace('<start>','')\n",
    "    sentence = sentence.replace('<end>', '')\n",
    "    sentence = clean_sentence(sentence)\n",
    "    \n",
    "\n",
    "    inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder_beam(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], target_tokenizer.word_index['<start>'])\n",
    "    end_token = target_tokenizer.word_index['<end>']\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder_beam.attention_mechanism.setup_memory(enc_out)\n",
    "    #print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder_beam.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder_beam.rnn_cell,beam_width=beam_width, output_layer=decoder_beam.fc)\n",
    "    decoder_embedding_matrix = decoder_beam.embedding.variables[0]\n",
    "\n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "    # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "    # The final beam predictions are stored in outputs.predicted_id\n",
    "    # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "    # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "    # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "\n",
    "    # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bbcae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(sentence):\n",
    "    result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "    #print(result.shape, beam_scores.shape)\n",
    "    for beam, score in zip(result, beam_scores):\n",
    "        #print(beam.shape, score.shape)\n",
    "        output = target_tokenizer.sequences_to_texts(beam)\n",
    "        output = [a[:a.index('<end>')] for a in output]\n",
    "        beam_score = [a.sum() for a in score]\n",
    "        #print('Input: %s' % (sentence))\n",
    "        #for i in range(len(output)):\n",
    "            #print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28bc683b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la traduction de cette phrase sera correcte . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_translate(\"The translation of this sentence will be correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0957657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7750/7750 [22:17<00:00,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "\"\"\"from tqdm import tqdm\n",
    "X_test0 = X_test[:7750]\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test0[i], input_tokenizer)\n",
    "        y_pred = beam_translate(x_test)\n",
    "        y_true = find_sentence(y_test[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_beam0.csv', index=False)\n",
    "       \n",
    "    \n",
    "test(len(X_test0))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6adc6094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7750/7750 [22:43<00:00,  5.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "\"\"\"from tqdm import tqdm\n",
    "X_test1= X_test[7750:15500]\n",
    "y_test1=y_test[7750:15500]\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test1[i], input_tokenizer)\n",
    "        y_pred = beam_translate(x_test)\n",
    "        y_true = find_sentence(y_test1[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "        \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_beam1.csv', index=False)\n",
    "       \n",
    "    \n",
    "test(len(X_test1))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d90d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7550/7550 [22:59<00:00,  5.47it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "\"\"\"from tqdm import tqdm\n",
    "\n",
    "X_test2= X_test[15500:23050]\n",
    "y_test2=y_test[15500:23050]\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test2[i], input_tokenizer)\n",
    "        y_pred = beam_translate(x_test)\n",
    "        y_true = find_sentence(y_test2[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "       \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_beam2.csv', index=False)\n",
    "       \n",
    "    \n",
    "test(len(X_test2))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f62edb71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 53/3964 [00:09<13:02,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223B1663048>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223B1663390>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223B1663198>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3964/3964 [12:42<00:00,  5.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "\"\"\"from tqdm import tqdm\n",
    "X_test3=X_test[23050:27014]\n",
    "y_test3=y_test[23050:27014]\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test3[i], input_tokenizer)\n",
    "        y_pred = beam_translate(x_test)\n",
    "        y_true = find_sentence(y_test3[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "        \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_beam3.csv', index=False)\n",
    "       \n",
    "    \n",
    "test(len(X_test3))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e576c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 1135/3963 [03:39<09:31,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223C9E03470>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223C9E03240>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000223C9E03518>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 485, in body\n",
      "    next_sequence_lengths,  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 569, in map_structure\n",
      "    expand_composites=expand_composites)  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 477, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3963/3963 [13:10<00:00,  5.01it/s]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "from tqdm import tqdm\n",
    "X_test4=X_test[27014:]\n",
    "y_test4=y_test[27014:]\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test4[i], input_tokenizer)\n",
    "        y_pred = beam_translate(x_test)\n",
    "        y_true = find_sentence(y_test4[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "        \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_beam4.csv', index=False)\n",
    "       \n",
    "    \n",
    "test(len(X_test4))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ca805",
   "metadata": {},
   "source": [
    "##  Evaluation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5c90953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mouni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; i love fried bananas . &lt;end&gt;</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i want to be alone for a while . &lt;end&gt;</td>\n",
       "      <td>je veux etre seule un moment .</td>\n",
       "      <td>je veux etre seul un moment .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i could never do that sort of thing ....</td>\n",
       "      <td>ce genre de chose ne m etait encore jamais a...</td>\n",
       "      <td>je ne pourrais jamais faire ce type de chose .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i was offered the choice of tea or co...</td>\n",
       "      <td>on m a propose le choix entre un the et un c...</td>\n",
       "      <td>on m a propose le choix entre un cafe .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; are you at home ? &lt;end&gt;</td>\n",
       "      <td>tu es chez toi ?</td>\n",
       "      <td>etes vous chez vous ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0               <start> i love fried bananas . <end>   \n",
       "1     <start> i want to be alone for a while . <end>   \n",
       "2   <start> i could never do that sort of thing ....   \n",
       "3   <start> i was offered the choice of tea or co...   \n",
       "4                    <start> are you at home ? <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0                      j adore les bananes frites .    \n",
       "1                    je veux etre seule un moment .    \n",
       "2    ce genre de chose ne m etait encore jamais a...   \n",
       "3    on m a propose le choix entre un the et un c...   \n",
       "4                                  tu es chez toi ?    \n",
       "\n",
       "                                     Sortie_modele  \n",
       "0                    j adore les bananes frites .   \n",
       "1                   je veux etre seul un moment .   \n",
       "2  je ne pourrais jamais faire ce type de chose .   \n",
       "3         on m a propose le choix entre un cafe .   \n",
       "4                           etes vous chez vous ?   "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports nécessaires \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# récupération des résultats sous forme d'un dataframe \n",
    "beam0=pd.read_csv('results_mod4_beam0.csv')   \n",
    "beam1=pd.read_csv('results_mod4_beam1.csv')\n",
    "beam2=pd.read_csv('results_mod4_beam2.csv')\n",
    "beam3=pd.read_csv('results_mod4_beam3.csv')\n",
    "beam4=pd.read_csv('results_mod4_beam4.csv')\n",
    "df_results_beam = pd.concat([beam0, beam1, beam2, beam3, beam4], ignore_index=True)\n",
    "\n",
    "phrase_purgee=[]\n",
    "for phrase in df_results_beam['Phrase_traduite']:\n",
    "    phrase = phrase.replace('<start>','')\n",
    "    phrase = phrase.replace('<end>', '')\n",
    "    phrase_purgee.append(phrase)\n",
    "df_results_beam['Phrase_traduite']=phrase_purgee    \n",
    "\n",
    "df_results_beam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fd6c4",
   "metadata": {},
   "source": [
    "### Vérification du nombre de mots dans chaque phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3489b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'Nb_words_cible' fournissant le nombre de mots dans la phrase cible \n",
    "nb_words_cible=[]\n",
    "for sentence in df_results_beam['Phrase_traduite']:\n",
    "    nb_words_cible.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results_beam['nb_words_cible']=nb_words_cible\n",
    "\n",
    "# Création d'une colonne 'Nb_words_mod' fournissant le nombre de mots dans la phrase prédite par le modèle \n",
    "nb_words_mod=[]\n",
    "for sentence in df_results_beam['Sortie_modele']:\n",
    "    nb_words_mod.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results_beam['nb_words_mod']=nb_words_mod\n",
    "\n",
    "# Création d'une colonne diff\n",
    "df_results_beam['Différence']=df_results_beam['nb_words_cible']-df_results_beam['nb_words_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fc8ee14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df_results_beam.Différence)\n",
    "plt.title(\"Histogramme représentant la différence de nombre de mots entre la phrase traduite et \"\n",
    "          \"la phrase sortie du modèle\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6476d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La majorité des phrases sorties du modèle, soit 86.77 %, présente au plus 1 mots d'écart avec la phrase traduite.\n"
     ]
    }
   ],
   "source": [
    "print(\"La majorité des phrases sorties du modèle, soit\", \n",
    "      round(len(df_results_beam.loc[(df_results_beam.Différence <= 1)&(df_results_beam.Différence >= -1)])/df_results_beam.shape[0]*100, 2),  \n",
    "      \"%, présente au plus 1 mots d'écart avec la phrase traduite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a210b",
   "metadata": {},
   "source": [
    "### Comparaison des racinisation des mots de chaque des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03ca7bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mouni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>nb_words_cible</th>\n",
       "      <th>nb_words_mod</th>\n",
       "      <th>Différence</th>\n",
       "      <th>racine_cible</th>\n",
       "      <th>racine_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; i love fried bananas . &lt;end&gt;</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"j\", \"ador\", \"le\", \"banan\", \"frit\", \".\"]</td>\n",
       "      <td>[\"j\", \"ador\", \"le\", \"banan\", \"frit\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i want to be alone for a while . &lt;end&gt;</td>\n",
       "      <td>je veux etre seule un moment .</td>\n",
       "      <td>je veux etre seul un moment .</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"je\", \"veux\", \"etre\", \"seul\", \"un\", \"moment\",...</td>\n",
       "      <td>[\"je\", \"veux\", \"etre\", \"seul\", \"un\", \"moment\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i could never do that sort of thing ....</td>\n",
       "      <td>ce genre de chose ne m etait encore jamais a...</td>\n",
       "      <td>je ne pourrais jamais faire ce type de chose .</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"ce\", \"genr\", \"de\", \"chos\", \"ne\", \"m\", \"etait...</td>\n",
       "      <td>[\"je\", \"ne\", \"pourr\", \"jam\", \"fair\", \"ce\", \"ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i was offered the choice of tea or co...</td>\n",
       "      <td>on m a propose le choix entre un the et un c...</td>\n",
       "      <td>on m a propose le choix entre un cafe .</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>[\"on\", \"m\", \"a\", \"propos\", \"le\", \"choix\", \"ent...</td>\n",
       "      <td>[\"on\", \"m\", \"a\", \"propos\", \"le\", \"choix\", \"ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; are you at home ? &lt;end&gt;</td>\n",
       "      <td>tu es chez toi ?</td>\n",
       "      <td>etes vous chez vous ?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"tu\", \"e\", \"chez\", \"toi\", \"?\"]</td>\n",
       "      <td>[\"ete\", \"vous\", \"chez\", \"vous\", \"?\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;start&gt; my friend helped me . &lt;end&gt;</td>\n",
       "      <td>mon amie m aida .</td>\n",
       "      <td>mon ami m a aidee .</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"mon\", \"ami\", \"m\", \"aid\", \".\"]</td>\n",
       "      <td>[\"mon\", \"ami\", \"m\", \"a\", \"aide\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;start&gt; i can t find tom anywhere . &lt;end&gt;</td>\n",
       "      <td>je ne trouve tom nulle part .</td>\n",
       "      <td>je ne peux trouver tom nulle part .</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"je\", \"ne\", \"trouv\", \"tom\", \"null\", \"part\", \".\"]</td>\n",
       "      <td>[\"je\", \"ne\", \"peux\", \"trouv\", \"tom\", \"null\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;start&gt; you re bad . &lt;end&gt;</td>\n",
       "      <td>tu es vilain .</td>\n",
       "      <td>tu es vilain .</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"tu\", \"e\", \"vilain\", \".\"]</td>\n",
       "      <td>[\"tu\", \"e\", \"vilain\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;start&gt; i d like to eat something . &lt;end&gt;</td>\n",
       "      <td>je voudrais manger quelque chose .</td>\n",
       "      <td>j aimerais manger quelque chose .</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"je\", \"voudr\", \"mang\", \"quelqu\", \"chos\", \".\"]</td>\n",
       "      <td>[\"j\", \"aim\", \"mang\", \"quelqu\", \"chos\", \".\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;start&gt; i picked the lock . &lt;end&gt;</td>\n",
       "      <td>j ai crochete la serrure .</td>\n",
       "      <td>j ai crochete la serrure .</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"j\", \"ai\", \"crochet\", \"la\", \"serrur\", \".\"]</td>\n",
       "      <td>[\"j\", \"ai\", \"crochet\", \"la\", \"serrur\", \".\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0               <start> i love fried bananas . <end>   \n",
       "1     <start> i want to be alone for a while . <end>   \n",
       "2   <start> i could never do that sort of thing ....   \n",
       "3   <start> i was offered the choice of tea or co...   \n",
       "4                    <start> are you at home ? <end>   \n",
       "5                <start> my friend helped me . <end>   \n",
       "6          <start> i can t find tom anywhere . <end>   \n",
       "7                         <start> you re bad . <end>   \n",
       "8          <start> i d like to eat something . <end>   \n",
       "9                  <start> i picked the lock . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0                      j adore les bananes frites .    \n",
       "1                    je veux etre seule un moment .    \n",
       "2    ce genre de chose ne m etait encore jamais a...   \n",
       "3    on m a propose le choix entre un the et un c...   \n",
       "4                                  tu es chez toi ?    \n",
       "5                                 mon amie m aida .    \n",
       "6                     je ne trouve tom nulle part .    \n",
       "7                                    tu es vilain .    \n",
       "8                je voudrais manger quelque chose .    \n",
       "9                        j ai crochete la serrure .    \n",
       "\n",
       "                                     Sortie_modele  nb_words_cible  \\\n",
       "0                    j adore les bananes frites .                6   \n",
       "1                   je veux etre seul un moment .                7   \n",
       "2  je ne pourrais jamais faire ce type de chose .               11   \n",
       "3         on m a propose le choix entre un cafe .               13   \n",
       "4                           etes vous chez vous ?                5   \n",
       "5                             mon ami m a aidee .                5   \n",
       "6             je ne peux trouver tom nulle part .                7   \n",
       "7                                  tu es vilain .                4   \n",
       "8               j aimerais manger quelque chose .                6   \n",
       "9                      j ai crochete la serrure .                6   \n",
       "\n",
       "   nb_words_mod  Différence  \\\n",
       "0             6           0   \n",
       "1             7           0   \n",
       "2            10           1   \n",
       "3            10           3   \n",
       "4             5           0   \n",
       "5             6          -1   \n",
       "6             8          -1   \n",
       "7             4           0   \n",
       "8             6           0   \n",
       "9             6           0   \n",
       "\n",
       "                                        racine_cible  \\\n",
       "0          [\"j\", \"ador\", \"le\", \"banan\", \"frit\", \".\"]   \n",
       "1  [\"je\", \"veux\", \"etre\", \"seul\", \"un\", \"moment\",...   \n",
       "2  [\"ce\", \"genr\", \"de\", \"chos\", \"ne\", \"m\", \"etait...   \n",
       "3  [\"on\", \"m\", \"a\", \"propos\", \"le\", \"choix\", \"ent...   \n",
       "4                    [\"tu\", \"e\", \"chez\", \"toi\", \"?\"]   \n",
       "5                    [\"mon\", \"ami\", \"m\", \"aid\", \".\"]   \n",
       "6  [\"je\", \"ne\", \"trouv\", \"tom\", \"null\", \"part\", \".\"]   \n",
       "7                         [\"tu\", \"e\", \"vilain\", \".\"]   \n",
       "8     [\"je\", \"voudr\", \"mang\", \"quelqu\", \"chos\", \".\"]   \n",
       "9        [\"j\", \"ai\", \"crochet\", \"la\", \"serrur\", \".\"]   \n",
       "\n",
       "                                          racine_mod  \n",
       "0          [\"j\", \"ador\", \"le\", \"banan\", \"frit\", \".\"]  \n",
       "1  [\"je\", \"veux\", \"etre\", \"seul\", \"un\", \"moment\",...  \n",
       "2  [\"je\", \"ne\", \"pourr\", \"jam\", \"fair\", \"ce\", \"ty...  \n",
       "3  [\"on\", \"m\", \"a\", \"propos\", \"le\", \"choix\", \"ent...  \n",
       "4               [\"ete\", \"vous\", \"chez\", \"vous\", \"?\"]  \n",
       "5              [\"mon\", \"ami\", \"m\", \"a\", \"aide\", \".\"]  \n",
       "6  [\"je\", \"ne\", \"peux\", \"trouv\", \"tom\", \"null\", \"...  \n",
       "7                         [\"tu\", \"e\", \"vilain\", \".\"]  \n",
       "8        [\"j\", \"aim\", \"mang\", \"quelqu\", \"chos\", \".\"]  \n",
       "9        [\"j\", \"ai\", \"crochet\", \"la\", \"serrur\", \".\"]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports nécessaires \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# récupération des résultats sous forme d'un dataframe\n",
    "\n",
    "# retrait des balises <start> et <end> de la colonne \"Phrase_traduite\"\n",
    "phrase_purgee=[]\n",
    "for phrase in df_results_beam['Phrase_traduite']:\n",
    "    phrase = phrase.replace('<start>','')\n",
    "    phrase = phrase.replace('<end>', '')\n",
    "    phrase_purgee.append(phrase)\n",
    "df_results_beam['Phrase_traduite']=phrase_purgee    \n",
    "\n",
    "df_results_beam.head()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "stop_words = set([\",\", \".\", \"?\", \"!\", \":\", \";\"])\n",
    "\n",
    "# Intialisation de la racinisation en français \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "# Application à tout le dataframe \n",
    "\n",
    "# Instanciation des listes nécessaires \n",
    "racine_cible_tot=[]\n",
    "racine_cible=[]\n",
    "racine_mod_tot=[]\n",
    "racine_mod=[]\n",
    "\n",
    "# Création d'une colonne 'racine_cible' fournissant la liste des racines des mots des phrases de la colonne cible \n",
    "for sentence in df_results_beam['Phrase_traduite']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "            racine_cible.append(stemmer.stem(mot))\n",
    "    if len(racine_cible) != 0:\n",
    "        racine_cible_tot.append(racine_cible)\n",
    "        racine_cible=[]\n",
    "    else :\n",
    "        racine_cible_tot.append('abc')\n",
    "    \n",
    "df_results_beam['racine_cible']=racine_cible_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results_beam['racine_cible'] = [[f'\"{j}\"' for j in i] for i in df_results_beam['racine_cible']]\n",
    "\n",
    "\n",
    "# Création d'une colonne 'racine_mod' fournissant la liste des racines des mots des phrases de la colonne prédite par le modèle  \n",
    "for sentence in df_results_beam['Sortie_modele']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "            racine_mod.append(stemmer.stem(mot))\n",
    "    racine_mod_tot.append(racine_mod)\n",
    "    racine_mod=[]\n",
    "\n",
    "df_results_beam['racine_mod']=racine_mod_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results_beam['racine_mod'] = [[f'\"{j}\"' for j in i] for i in df_results_beam['racine_mod']]\n",
    "\n",
    "df_results_beam.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d661fee",
   "metadata": {},
   "source": [
    "### Etablissement des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a501288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne \"comparaison racine\" fournissant le ratio \n",
    "#nombre de racines en commun / nombre de racines des phrases cibles\n",
    "\n",
    "# Création d'une colonne \"comparaison différence\" fournissant le ratio \n",
    "#différence du nombre de mots / nombre de mots cibles\n",
    "\n",
    "score_racine=[]\n",
    "score_difference = []\n",
    "for i in range(df_results_beam.shape[0]):\n",
    "    score_racine.append(len(set(df_results_beam.iloc[i,6]) & set(df_results_beam.iloc[i,7]))/len(df_results_beam.iloc[i,6]))\n",
    "    if (abs(df_results_beam.iloc[i, 5])/df_results_beam.iloc[i,3]) > 1:\n",
    "        score_diff = 0\n",
    "    else :\n",
    "        score_diff = 1 - abs(df_results_beam.iloc[i, 5])/df_results_beam.iloc[i,3]\n",
    "    score_difference.append(score_diff)\n",
    "    \n",
    "df_results_beam['score_racine']=score_racine\n",
    "\n",
    "df_results_beam['score_diff']= score_difference\n",
    "\n",
    "df_results_beam['score_tot']=(df_results_beam['score_diff']+ df_results_beam['score_racine'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "935a1805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; i love fried bananas . &lt;end&gt;</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i want to be alone for a while . &lt;end&gt;</td>\n",
       "      <td>je veux etre seule un moment .</td>\n",
       "      <td>je veux etre seul un moment .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i could never do that sort of thing ....</td>\n",
       "      <td>ce genre de chose ne m etait encore jamais a...</td>\n",
       "      <td>je ne pourrais jamais faire ce type de chose .</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i was offered the choice of tea or co...</td>\n",
       "      <td>on m a propose le choix entre un the et un c...</td>\n",
       "      <td>on m a propose le choix entre un cafe .</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; are you at home ? &lt;end&gt;</td>\n",
       "      <td>tu es chez toi ?</td>\n",
       "      <td>etes vous chez vous ?</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0               <start> i love fried bananas . <end>   \n",
       "1     <start> i want to be alone for a while . <end>   \n",
       "2   <start> i could never do that sort of thing ....   \n",
       "3   <start> i was offered the choice of tea or co...   \n",
       "4                    <start> are you at home ? <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0                      j adore les bananes frites .    \n",
       "1                    je veux etre seule un moment .    \n",
       "2    ce genre de chose ne m etait encore jamais a...   \n",
       "3    on m a propose le choix entre un the et un c...   \n",
       "4                                  tu es chez toi ?    \n",
       "\n",
       "                                     Sortie_modele  score_racine  score_diff  \\\n",
       "0                    j adore les bananes frites .       1.000000    1.000000   \n",
       "1                   je veux etre seul un moment .       1.000000    1.000000   \n",
       "2  je ne pourrais jamais faire ce type de chose .       0.545455    0.909091   \n",
       "3         on m a propose le choix entre un cafe .       0.769231    0.769231   \n",
       "4                           etes vous chez vous ?       0.400000    1.000000   \n",
       "\n",
       "   score_tot  \n",
       "0   1.000000  \n",
       "1   1.000000  \n",
       "2   0.727273  \n",
       "3   0.769231  \n",
       "4   0.700000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_beam = df_results_beam.drop([\"nb_words_cible\", \"nb_words_mod\", \"Différence\", \"racine_cible\", \"racine_mod\"], axis = 1)\n",
    "df_results_beam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "70643593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne du score_racine 0.8052585389389256\n",
      "moyenne du score_diff 0.9348595297286239\n",
      "Score du modèle au test de performance 0.8700590343337749\n"
     ]
    }
   ],
   "source": [
    "print(\"moyenne du score_racine\", df_results_beam['score_racine'].mean())\n",
    "print(\"moyenne du score_diff\", df_results_beam['score_diff'].mean())\n",
    "print(\"Score du modèle au test de performance\", df_results_beam['score_tot'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996a973",
   "metadata": {},
   "source": [
    "### Score BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4825fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mouni\\.conda\\envs\\glasses_project\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; i love fried bananas . &lt;end&gt;</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i want to be alone for a while . &lt;end&gt;</td>\n",
       "      <td>je veux etre seule un moment .</td>\n",
       "      <td>je veux etre seul un moment .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i could never do that sort of thing ....</td>\n",
       "      <td>ce genre de chose ne m etait encore jamais a...</td>\n",
       "      <td>je ne pourrais jamais faire ce type de chose .</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.542902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i was offered the choice of tea or co...</td>\n",
       "      <td>on m a propose le choix entre un the et un c...</td>\n",
       "      <td>on m a propose le choix entre un cafe .</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; are you at home ? &lt;end&gt;</td>\n",
       "      <td>tu es chez toi ?</td>\n",
       "      <td>etes vous chez vous ?</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0               <start> i love fried bananas . <end>   \n",
       "1     <start> i want to be alone for a while . <end>   \n",
       "2   <start> i could never do that sort of thing ....   \n",
       "3   <start> i was offered the choice of tea or co...   \n",
       "4                    <start> are you at home ? <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0                      j adore les bananes frites .    \n",
       "1                    je veux etre seule un moment .    \n",
       "2    ce genre de chose ne m etait encore jamais a...   \n",
       "3    on m a propose le choix entre un the et un c...   \n",
       "4                                  tu es chez toi ?    \n",
       "\n",
       "                                     Sortie_modele  score_racine  score_diff  \\\n",
       "0                    j adore les bananes frites .       1.000000    1.000000   \n",
       "1                   je veux etre seul un moment .       1.000000    1.000000   \n",
       "2  je ne pourrais jamais faire ce type de chose .       0.545455    0.909091   \n",
       "3         on m a propose le choix entre un cafe .       0.769231    0.769231   \n",
       "4                           etes vous chez vous ?       0.400000    1.000000   \n",
       "\n",
       "   score_tot  Score_bleu  \n",
       "0   1.000000    1.000000  \n",
       "1   1.000000    0.857143  \n",
       "2   0.727273    0.542902  \n",
       "3   0.769231    0.740818  \n",
       "4   0.700000    0.400000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score_bleu = []\n",
    "for i in range(df_results_beam.shape[0]):\n",
    "    reference = [''.join(df_results_beam.iloc[i, 1]).split()]\n",
    "    candidate = ''.join(df_results_beam.iloc[i, 2]).split()\n",
    "    score_bleu.append(sentence_bleu(reference, candidate, weights = (1,0,0,0)))\n",
    "df_results_beam[\"Score_bleu\"] = score_bleu\n",
    "df_results_beam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c7eab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score BLEU du modèle 0.7799929614583093\n"
     ]
    }
   ],
   "source": [
    "# Score BLEU du modèle : \n",
    "print(\"Score BLEU du modèle\", df_results_beam['Score_bleu'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ec047",
   "metadata": {},
   "source": [
    "### Score ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6c7e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "r = Rouge()\n",
    "precision_col = []\n",
    "recall_col = []\n",
    "fscore_col = []\n",
    "\n",
    "for i in range(df_results_beam.shape[0]):\n",
    "    reference = df_results_beam.iloc[i, 1]\n",
    "    candidate = df_results_beam.iloc[i, 2]\n",
    "    score_rouge = r.get_scores(candidate, reference)\n",
    "    precision_col.append(score_rouge[0]['rouge-1']['p'])\n",
    "    recall_col.append(score_rouge[0]['rouge-1']['r'])\n",
    "    fscore_col.append(score_rouge[0]['rouge-1']['f'])\n",
    "\n",
    "df_results_beam['Rouge_recall'] = recall_col\n",
    "df_results_beam['Rouge_precision'] = precision_col\n",
    "df_results_beam['Rouge_f1_score'] = fscore_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3a65926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE recall du modèle 0.800275968206703\n",
      "Score ROUGE precision du modèle 0.8134560096110748\n",
      "Score ROUGE f1_score du modèle 0.804184990371129\n"
     ]
    }
   ],
   "source": [
    "# Score ROUGE du modèle : \n",
    "print(\"Score ROUGE recall du modèle\", df_results_beam['Rouge_recall'].mean())\n",
    "print(\"Score ROUGE precision du modèle\", df_results_beam['Rouge_precision'].mean())\n",
    "print(\"Score ROUGE f1_score du modèle\", df_results_beam['Rouge_f1_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c8d75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_beam = df_results_beam.drop([\"score_racine\", \"score_diff\", \"Rouge_recall\", \"Rouge_precision\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07114745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "      <th>Rouge_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; i love fried bananas . &lt;end&gt;</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>j adore les bananes frites .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i want to be alone for a while . &lt;end&gt;</td>\n",
       "      <td>je veux etre seule un moment .</td>\n",
       "      <td>je veux etre seul un moment .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i could never do that sort of thing ....</td>\n",
       "      <td>ce genre de chose ne m etait encore jamais a...</td>\n",
       "      <td>je ne pourrais jamais faire ce type de chose .</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.542902</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i was offered the choice of tea or co...</td>\n",
       "      <td>on m a propose le choix entre un the et un c...</td>\n",
       "      <td>on m a propose le choix entre un cafe .</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.740818</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; are you at home ? &lt;end&gt;</td>\n",
       "      <td>tu es chez toi ?</td>\n",
       "      <td>etes vous chez vous ?</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;start&gt; my friend helped me . &lt;end&gt;</td>\n",
       "      <td>mon amie m aida .</td>\n",
       "      <td>mon ami m a aidee .</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;start&gt; i can t find tom anywhere . &lt;end&gt;</td>\n",
       "      <td>je ne trouve tom nulle part .</td>\n",
       "      <td>je ne peux trouver tom nulle part .</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;start&gt; you re bad . &lt;end&gt;</td>\n",
       "      <td>tu es vilain .</td>\n",
       "      <td>tu es vilain .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;start&gt; i d like to eat something . &lt;end&gt;</td>\n",
       "      <td>je voudrais manger quelque chose .</td>\n",
       "      <td>j aimerais manger quelque chose .</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;start&gt; i picked the lock . &lt;end&gt;</td>\n",
       "      <td>j ai crochete la serrure .</td>\n",
       "      <td>j ai crochete la serrure .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0               <start> i love fried bananas . <end>   \n",
       "1     <start> i want to be alone for a while . <end>   \n",
       "2   <start> i could never do that sort of thing ....   \n",
       "3   <start> i was offered the choice of tea or co...   \n",
       "4                    <start> are you at home ? <end>   \n",
       "5                <start> my friend helped me . <end>   \n",
       "6          <start> i can t find tom anywhere . <end>   \n",
       "7                         <start> you re bad . <end>   \n",
       "8          <start> i d like to eat something . <end>   \n",
       "9                  <start> i picked the lock . <end>   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0                      j adore les bananes frites .    \n",
       "1                    je veux etre seule un moment .    \n",
       "2    ce genre de chose ne m etait encore jamais a...   \n",
       "3    on m a propose le choix entre un the et un c...   \n",
       "4                                  tu es chez toi ?    \n",
       "5                                 mon amie m aida .    \n",
       "6                     je ne trouve tom nulle part .    \n",
       "7                                    tu es vilain .    \n",
       "8                je voudrais manger quelque chose .    \n",
       "9                        j ai crochete la serrure .    \n",
       "\n",
       "                                     Sortie_modele  score_tot  Score_bleu  \\\n",
       "0                    j adore les bananes frites .    1.000000    1.000000   \n",
       "1                   je veux etre seul un moment .    1.000000    0.857143   \n",
       "2  je ne pourrais jamais faire ce type de chose .    0.727273    0.542902   \n",
       "3         on m a propose le choix entre un cafe .    0.769231    0.740818   \n",
       "4                           etes vous chez vous ?    0.700000    0.400000   \n",
       "5                             mon ami m a aidee .    0.800000    0.500000   \n",
       "6             je ne peux trouver tom nulle part .    0.928571    0.750000   \n",
       "7                                  tu es vilain .    1.000000    1.000000   \n",
       "8               j aimerais manger quelque chose .    0.833333    0.666667   \n",
       "9                      j ai crochete la serrure .    1.000000    1.000000   \n",
       "\n",
       "   Rouge_f1_score  \n",
       "0        1.000000  \n",
       "1        0.857143  \n",
       "2        0.571429  \n",
       "3        0.909091  \n",
       "4        0.444444  \n",
       "5        0.545455  \n",
       "6        0.800000  \n",
       "7        1.000000  \n",
       "8        0.666667  \n",
       "9        1.000000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_beam.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
