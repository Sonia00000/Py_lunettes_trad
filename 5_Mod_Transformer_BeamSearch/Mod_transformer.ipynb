{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c65780",
   "metadata": {},
   "source": [
    "#  Modélisation par transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b3db9",
   "metadata": {},
   "source": [
    "## Pré-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16015091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, RNN, Lambda, Embedding\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d64950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Français</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154878</th>\n",
       "      <td>\"Top-down economics never works,\" said Obama. ...</td>\n",
       "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154879</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154880</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154881</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154882</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  English  \\\n",
       "154878  \"Top-down economics never works,\" said Obama. ...   \n",
       "154879  A carbon footprint is the amount of carbon dio...   \n",
       "154880  Death is something that we're often discourage...   \n",
       "154881  Since there are usually multiple websites on a...   \n",
       "154882  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                 Français  \n",
       "154878  « L'économie en partant du haut vers le bas, ç...  \n",
       "154879  Une empreinte carbone est la somme de pollutio...  \n",
       "154880  La mort est une chose qu'on nous décourage sou...  \n",
       "154881  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "154882  Si quelqu'un qui ne connaît pas vos antécédent...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des données \n",
    "fr = pd.read_csv('french.txt', sep = \"\\n\\n\", names = [\"Français\"], encoding ='utf-8', engine = 'python')\n",
    "en = pd.read_csv('english.txt', sep = \"\\n\\n\", names = [\"English\"], encoding ='utf-8', engine = 'python')\n",
    "data = pd.concat([en, fr], axis=1)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2869105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des données \n",
    "\n",
    "def clean_sentence(w):\n",
    "\n",
    "    # séparation entre un mot et sa ponctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne english\n",
    "data.English = data.English.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne french\n",
    "data.Français = data.Français.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "corpus_en=[]\n",
    "corpus_fr=[]\n",
    "\n",
    "for sentences in data.English: \n",
    "    corpus_en.append(sentences)\n",
    "\n",
    "for sentences in data.Français: \n",
    "    corpus_fr.append(sentences)\n",
    "\n",
    "# Vérification \n",
    "#print(corpus_en[:5])\n",
    "#print(corpus_fr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d270ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots maximum dans les phrases françaises : 58\n",
      "Nombre de mots maximum dans les phrases anglaises : 48\n",
      "\n",
      "Taille du vocabulaire français : 30661\n",
      "Taille du vocabulaire anglais : 16122\n"
     ]
    }
   ],
   "source": [
    "# Nombre de mots maximum par phrases\n",
    "max_words_en = 0\n",
    "max_words_fr = 0\n",
    "for i in range(data.shape[0]):\n",
    "    max_words_en = max(len(corpus_en[i].split()), max_words_en)\n",
    "    max_words_fr = max(len(corpus_fr[i].split()), max_words_fr)\n",
    "\n",
    "print(\"Nombre de mots maximum dans les phrases françaises :\", max_words_fr)\n",
    "print(\"Nombre de mots maximum dans les phrases anglaises :\", max_words_en)\n",
    "\n",
    "# Taille des vocabulaires\n",
    "words_en = []\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(len(corpus_en[i].split())):\n",
    "        if corpus_en[i].split()[j] not in words_en:\n",
    "            words_en.append(corpus_en[i].split()[j])\n",
    "            \n",
    "words_fr = []\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(len(corpus_fr[i].split())):\n",
    "        if corpus_fr[i].split()[j] not in words_fr:\n",
    "            words_fr.append(corpus_fr[i].split()[j])\n",
    "            \n",
    "print(\"\\nTaille du vocabulaire français :\", len(words_fr))\n",
    "print(\"Taille du vocabulaire anglais :\", len(words_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9906a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "\n",
    "# Création des tokenizers\n",
    "tokenizer_en = Tokenizer(num_words = 32000) # seuil fixé bien au-dessus des 16500 mots uniques\n",
    "tokenizer_fr = Tokenizer(num_words = 32000)# seuil fixé bien au-dessus des 31000 mots uniques\n",
    "\n",
    "# Génération des vocabulaires francais et anglais: \n",
    "    #permet de corréler chaque mot unique à un nombre \n",
    "tokenizer_fr.fit_on_texts(corpus_fr) \n",
    "tokenizer_en.fit_on_texts(corpus_en)\n",
    "\n",
    "# Transformation des corpus en une liste de listes de nombres (correspondant chacun à un mot unique)\n",
    "tokenized_fr = tokenizer_fr.texts_to_sequences(corpus_fr)\n",
    "tokenized_en = tokenizer_en.texts_to_sequences(corpus_en)\n",
    "\n",
    "# Vérification \n",
    "#print(tokenized_fr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8616189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'un token de début et d'un token de fin pour chaque phrase (liste) de la liste globale\n",
    "new_fr = []\n",
    "new_en = []\n",
    "\n",
    "for sentence_fr, sentence_en in zip(tokenized_fr, tokenized_en):\n",
    "    new_sentence_fr = [32000] + sentence_fr + [32001]\n",
    "    new_sentence_en = [32000] + sentence_en + [32001]\n",
    "    new_fr.append(new_sentence_fr)\n",
    "    new_en.append(new_sentence_en)\n",
    "\n",
    "tokenized_fr = new_fr\n",
    "tokenized_en = new_en\n",
    "\n",
    "# Vérification \n",
    "#n = np.random.randint(100)\n",
    "#tokenized_fr[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debe8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur maximale FR: 56\n",
      "Longueur maximale EN: 46\n",
      "\n",
      "Taille Corpus FR: 154883\n",
      "Taille Corpus EN: 154883\n"
     ]
    }
   ],
   "source": [
    "# Vérification de la longueur des phrases\n",
    "\n",
    "lengths_fr = []\n",
    "lengths_en = []\n",
    "\n",
    "for sentence_fr, sentence_en in zip(tokenized_fr, tokenized_en):\n",
    "    lengths_fr.append(len(sentence_fr))\n",
    "    lengths_en.append(len(sentence_en))\n",
    "    \n",
    "print(\"Longueur maximale FR:\", max(lengths_fr))\n",
    "print(\"Longueur maximale EN:\", max(lengths_en))\n",
    "print()\n",
    "print(\"Taille Corpus FR:\", len(tokenized_fr))\n",
    "print(\"Taille Corpus EN:\", len(tokenized_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c785bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding: objectif les phrases tokenisées doivent avoir la même longueur (65 mots)\n",
    "\n",
    "MAX_LENGTH = 60\n",
    "\n",
    "new_fr = []\n",
    "new_en = []\n",
    "\n",
    "for sentence_fr, sentence_en in zip(tokenized_fr, tokenized_en):\n",
    "    new_fr.append(sentence_fr + [0] * (MAX_LENGTH - len(sentence_fr)))\n",
    "    new_en.append(sentence_en + [0] * (MAX_LENGTH - len(sentence_en)))\n",
    "    \n",
    "tokenized_fr = new_fr\n",
    "tokenized_en = new_en\n",
    "\n",
    "# Vérifications\n",
    "\n",
    "# longueur des phrases \n",
    "for sentence_fr, sentence_en in zip(tokenized_fr, tokenized_en):\n",
    "    if len(sentence_fr) != MAX_LENGTH or len(sentence_en) != MAX_LENGTH:\n",
    "        print(\"Les phrases n'ont pas la bonne longueur.\")\n",
    "        break\n",
    "\n",
    "# vérification du dernier nombre \n",
    "sample_sentence = tokenized_fr[0]\n",
    "if len(sample_sentence) == MAX_LENGTH and sample_sentence[-1] != 0:\n",
    "    print(\"Le token de padding utilisé n'est pas le bon.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca9da0",
   "metadata": {},
   "source": [
    "## Entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f587e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de train_dataset (tensor_slices permet de découper la liste de listes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_en, tokenized_fr,test_size=0.2)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b371df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 60), dtype=int32, numpy=\n",
       " array([[32000,     1,    92, ...,     0,     0,     0],\n",
       "        [32000,     5,   113, ...,     0,     0,     0],\n",
       "        [32000,    40,   138, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [32000,     1,    16, ...,     0,     0,     0],\n",
       "        [32000,     1,    77, ...,     0,     0,     0],\n",
       "        [32000,     1,   106, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(64, 60), dtype=int32, numpy=\n",
       " array([[32000,     1,    73, ...,     0,     0,     0],\n",
       "        [32000,    16,   148, ...,     0,     0,     0],\n",
       "        [32000, 26680,     2, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [32000,   373,    72, ...,     0,     0,     0],\n",
       "        [32000,    21,    89, ...,     0,     0,     0],\n",
       "        [32000,     1,    23, ...,     0,     0,     0]])>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fonction iter() renvoie l'objet itérateur qui parcourt chaque élément de la liste.\n",
    "# la fonction next() renvoit l'élément suivant\n",
    "fr_batch, en_batch = next(iter(train_dataset))\n",
    "fr_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092d8b7",
   "metadata": {},
   "source": [
    "## Génération de masques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9d045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du look-ahead mask qui permet de masquer les fins de la phrase traduite \n",
    "# pour obtenir des phrases partiellement traduites qui seront utilisées dans le décodeur \n",
    "# concrètement, le masque cache les tokends strictement au-dessus de la diagonale. \n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (size, size)\n",
    "\n",
    "# Vérifications \n",
    "#create_look_ahead_mask(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1667c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du masque de padding\n",
    "# permet d'empêcher que les mécanismes prêtent attention aux tokens de padding\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # tf.cast fait passer de x (tenseur) à dtype\n",
    "    # tf.math.equal(seq, 0): permet de vérifier si la sequence seq est égale à 0\n",
    "\n",
    "    # ajouter de deux dimensions avant et après pour obtenir le bon nombre de dimensions (batch_size, 1, 1, seq_len)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Vérifications\n",
    "#x = tf.constant([[7, 6, 0, 0, 1],\n",
    "#                 [1, 2, 3, 0, 0],\n",
    "#                 [0, 0, 0, 4, 5]])\n",
    "\n",
    "#mask = create_padding_mask(x)\n",
    "#mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97d8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encodage du masque de padding de l'entrée du décodeur \n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Masque de padding utilisé dans le bloc décodeur - second attention \n",
    "    # permet depour cacher une partie des sorties de l'encodeur \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Masque de padding utilisé dans le bloc de décodeur - première attention  \n",
    "    # permet de cacher la fin des phrases traduites \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f0681",
   "metadata": {},
   "source": [
    "## Entrainement d'un transformer pour la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b57943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape  (64, 43, 512)\n",
      "Output Shape (64, 43, 512)\n",
      "Encoder Input Shape:      (64, 62)\n",
      "Decoder Input Shape:      (64, 62)\n",
      "Transformer Output Shape: (64, 62, 32000)\n"
     ]
    }
   ],
   "source": [
    "%run Transformeur.ipynb\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_en.num_words + 2\n",
    "target_vocab_size = tokenizer_fr.num_words + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf161cd",
   "metadata": {},
   "source": [
    "### Fonction de coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95feb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    valid_tokens = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # if real != 0 -> valid_tokens = True This is used to mask padded outputs\n",
    "\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "  \n",
    "    valid_tokens = tf.cast(valid_tokens, dtype=loss_.dtype)#cast valid_tokens to the same type as loss_\n",
    "    loss_ = tf.reduce_sum(loss_*valid_tokens)/tf.reduce_sum(valid_tokens)\n",
    "  \n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5332d09",
   "metadata": {},
   "source": [
    "### Optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f14e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d07fef",
   "metadata": {},
   "source": [
    "### Pas d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02461521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serviront à l'affichage pendant l'entraînement\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db0b06",
   "metadata": {},
   "source": [
    "### Boucles d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec81a580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.3819 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 10.2672 Accuracy 0.0053\n",
      "Epoch 1 Batch 200 Loss 9.9498 Accuracy 0.0111\n",
      "Epoch 1 Batch 300 Loss 9.4857 Accuracy 0.0130\n",
      "Epoch 1 Batch 400 Loss 8.9407 Accuracy 0.0140\n",
      "Epoch 1 Batch 500 Loss 8.4443 Accuracy 0.0146\n",
      "Epoch 1 Batch 600 Loss 8.0710 Accuracy 0.0150\n",
      "Epoch 1 Batch 700 Loss 7.7780 Accuracy 0.0159\n",
      "Epoch 1 Batch 800 Loss 7.5407 Accuracy 0.0168\n",
      "Epoch 1 Batch 900 Loss 7.3404 Accuracy 0.0176\n",
      "Epoch 1 Batch 1000 Loss 7.1658 Accuracy 0.0184\n",
      "Epoch 1 Batch 1100 Loss 7.0075 Accuracy 0.0192\n",
      "Epoch 1 Batch 1200 Loss 6.8667 Accuracy 0.0200\n",
      "Epoch 1 Batch 1300 Loss 6.7352 Accuracy 0.0208\n",
      "Epoch 1 Batch 1400 Loss 6.6126 Accuracy 0.0216\n",
      "Epoch 1 Batch 1500 Loss 6.5000 Accuracy 0.0225\n",
      "Epoch 1 Batch 1600 Loss 6.3941 Accuracy 0.0233\n",
      "Epoch 1 Batch 1700 Loss 6.2954 Accuracy 0.0241\n",
      "Epoch 1 Batch 1800 Loss 6.2023 Accuracy 0.0249\n",
      "Epoch 1 Batch 1900 Loss 6.1168 Accuracy 0.0257\n",
      "Epoch 1 Loss 6.0865 Accuracy 0.0260\n",
      "Time taken for 1 epoch: 461.41957426071167 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.3636 Accuracy 0.0400\n",
      "Epoch 2 Batch 100 Loss 4.3947 Accuracy 0.0418\n",
      "Epoch 2 Batch 200 Loss 4.3362 Accuracy 0.0425\n",
      "Epoch 2 Batch 300 Loss 4.2988 Accuracy 0.0430\n",
      "Epoch 2 Batch 400 Loss 4.2616 Accuracy 0.0436\n",
      "Epoch 2 Batch 500 Loss 4.2213 Accuracy 0.0442\n",
      "Epoch 2 Batch 600 Loss 4.1824 Accuracy 0.0448\n",
      "Epoch 2 Batch 700 Loss 4.1395 Accuracy 0.0455\n",
      "Epoch 2 Batch 800 Loss 4.1024 Accuracy 0.0460\n",
      "Epoch 2 Batch 900 Loss 4.0652 Accuracy 0.0466\n",
      "Epoch 2 Batch 1000 Loss 4.0266 Accuracy 0.0472\n",
      "Epoch 2 Batch 1100 Loss 3.9909 Accuracy 0.0478\n",
      "Epoch 2 Batch 1200 Loss 3.9568 Accuracy 0.0483\n",
      "Epoch 2 Batch 1300 Loss 3.9227 Accuracy 0.0489\n",
      "Epoch 2 Batch 1400 Loss 3.8873 Accuracy 0.0494\n",
      "Epoch 2 Batch 1500 Loss 3.8538 Accuracy 0.0500\n",
      "Epoch 2 Batch 1600 Loss 3.8208 Accuracy 0.0505\n",
      "Epoch 2 Batch 1700 Loss 3.7891 Accuracy 0.0510\n",
      "Epoch 2 Batch 1800 Loss 3.7583 Accuracy 0.0516\n",
      "Epoch 2 Batch 1900 Loss 3.7313 Accuracy 0.0521\n",
      "Epoch 2 Loss 3.7210 Accuracy 0.0523\n",
      "Time taken for 1 epoch: 456.1055495738983 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.8964 Accuracy 0.0644\n",
      "Epoch 3 Batch 100 Loss 3.1377 Accuracy 0.0629\n",
      "Epoch 3 Batch 200 Loss 3.0962 Accuracy 0.0634\n",
      "Epoch 3 Batch 300 Loss 3.0810 Accuracy 0.0634\n",
      "Epoch 3 Batch 400 Loss 3.0601 Accuracy 0.0639\n",
      "Epoch 3 Batch 500 Loss 3.0364 Accuracy 0.0644\n",
      "Epoch 3 Batch 600 Loss 3.0138 Accuracy 0.0648\n",
      "Epoch 3 Batch 700 Loss 2.9893 Accuracy 0.0653\n",
      "Epoch 3 Batch 800 Loss 2.9713 Accuracy 0.0656\n",
      "Epoch 3 Batch 900 Loss 2.9528 Accuracy 0.0659\n",
      "Epoch 3 Batch 1000 Loss 2.9307 Accuracy 0.0663\n",
      "Epoch 3 Batch 1100 Loss 2.9121 Accuracy 0.0667\n",
      "Epoch 3 Batch 1200 Loss 2.8950 Accuracy 0.0670\n",
      "Epoch 3 Batch 1300 Loss 2.8757 Accuracy 0.0674\n",
      "Epoch 3 Batch 1400 Loss 2.8553 Accuracy 0.0677\n",
      "Epoch 3 Batch 1500 Loss 2.8383 Accuracy 0.0681\n",
      "Epoch 3 Batch 1600 Loss 2.8206 Accuracy 0.0683\n",
      "Epoch 3 Batch 1700 Loss 2.8037 Accuracy 0.0686\n",
      "Epoch 3 Batch 1800 Loss 2.7883 Accuracy 0.0690\n",
      "Epoch 3 Batch 1900 Loss 2.7758 Accuracy 0.0693\n",
      "Epoch 3 Loss 2.7705 Accuracy 0.0694\n",
      "Time taken for 1 epoch: 460.8169927597046 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.3446 Accuracy 0.0744\n",
      "Epoch 4 Batch 100 Loss 2.4773 Accuracy 0.0753\n",
      "Epoch 4 Batch 200 Loss 2.4534 Accuracy 0.0757\n",
      "Epoch 4 Batch 300 Loss 2.4481 Accuracy 0.0756\n",
      "Epoch 4 Batch 400 Loss 2.4417 Accuracy 0.0758\n",
      "Epoch 4 Batch 500 Loss 2.4348 Accuracy 0.0760\n",
      "Epoch 4 Batch 600 Loss 2.4260 Accuracy 0.0762\n",
      "Epoch 4 Batch 700 Loss 2.4161 Accuracy 0.0764\n",
      "Epoch 4 Batch 800 Loss 2.4121 Accuracy 0.0766\n",
      "Epoch 4 Batch 900 Loss 2.4070 Accuracy 0.0767\n",
      "Epoch 4 Batch 1000 Loss 2.3974 Accuracy 0.0768\n",
      "Epoch 4 Batch 1100 Loss 2.3927 Accuracy 0.0770\n",
      "Epoch 4 Batch 1200 Loss 2.3877 Accuracy 0.0771\n",
      "Epoch 4 Batch 1300 Loss 2.3824 Accuracy 0.0772\n",
      "Epoch 4 Batch 1400 Loss 2.3741 Accuracy 0.0773\n",
      "Epoch 4 Batch 1500 Loss 2.3688 Accuracy 0.0775\n",
      "Epoch 4 Batch 1600 Loss 2.3626 Accuracy 0.0776\n",
      "Epoch 4 Batch 1700 Loss 2.3561 Accuracy 0.0777\n",
      "Epoch 4 Batch 1800 Loss 2.3502 Accuracy 0.0779\n",
      "Epoch 4 Batch 1900 Loss 2.3476 Accuracy 0.0780\n",
      "Epoch 4 Loss 2.3455 Accuracy 0.0780\n",
      "Time taken for 1 epoch: 459.5462477207184 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.0350 Accuracy 0.0800\n",
      "Epoch 5 Batch 100 Loss 2.2270 Accuracy 0.0809\n",
      "Epoch 5 Batch 200 Loss 2.2129 Accuracy 0.0813\n",
      "Epoch 5 Batch 300 Loss 2.2208 Accuracy 0.0809\n",
      "Epoch 5 Batch 400 Loss 2.2244 Accuracy 0.0809\n",
      "Epoch 5 Batch 500 Loss 2.2241 Accuracy 0.0810\n",
      "Epoch 5 Batch 600 Loss 2.2214 Accuracy 0.0811\n",
      "Epoch 5 Batch 700 Loss 2.2161 Accuracy 0.0813\n",
      "Epoch 5 Batch 800 Loss 2.2145 Accuracy 0.0813\n",
      "Epoch 5 Batch 900 Loss 2.2100 Accuracy 0.0813\n",
      "Epoch 5 Batch 1000 Loss 2.1995 Accuracy 0.0814\n",
      "Epoch 5 Batch 1100 Loss 2.1946 Accuracy 0.0815\n",
      "Epoch 5 Batch 1200 Loss 2.1919 Accuracy 0.0816\n",
      "Epoch 5 Batch 1300 Loss 2.1896 Accuracy 0.0816\n",
      "Epoch 5 Batch 1400 Loss 2.1841 Accuracy 0.0817\n",
      "Epoch 5 Batch 1500 Loss 2.1816 Accuracy 0.0818\n",
      "Epoch 5 Batch 1600 Loss 2.1781 Accuracy 0.0819\n",
      "Epoch 5 Batch 1700 Loss 2.1738 Accuracy 0.0819\n",
      "Epoch 5 Batch 1800 Loss 2.1702 Accuracy 0.0821\n",
      "Epoch 5 Batch 1900 Loss 2.1684 Accuracy 0.0821\n",
      "Epoch 5 Loss 2.1667 Accuracy 0.0822\n",
      "Time taken for 1 epoch: 462.1647825241089 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.8693 Accuracy 0.0840\n",
      "Epoch 6 Batch 100 Loss 2.0698 Accuracy 0.0839\n",
      "Epoch 6 Batch 200 Loss 2.0627 Accuracy 0.0840\n",
      "Epoch 6 Batch 300 Loss 2.0722 Accuracy 0.0838\n",
      "Epoch 6 Batch 400 Loss 2.0781 Accuracy 0.0839\n",
      "Epoch 6 Batch 500 Loss 2.0793 Accuracy 0.0840\n",
      "Epoch 6 Batch 600 Loss 2.0779 Accuracy 0.0842\n",
      "Epoch 6 Batch 700 Loss 2.0750 Accuracy 0.0843\n",
      "Epoch 6 Batch 800 Loss 2.0751 Accuracy 0.0843\n",
      "Epoch 6 Batch 900 Loss 2.0717 Accuracy 0.0843\n",
      "Epoch 6 Batch 1000 Loss 2.0623 Accuracy 0.0844\n",
      "Epoch 6 Batch 1100 Loss 2.0585 Accuracy 0.0844\n",
      "Epoch 6 Batch 1200 Loss 2.0566 Accuracy 0.0844\n",
      "Epoch 6 Batch 1300 Loss 2.0557 Accuracy 0.0844\n",
      "Epoch 6 Batch 1400 Loss 2.0518 Accuracy 0.0845\n",
      "Epoch 6 Batch 1500 Loss 2.0501 Accuracy 0.0846\n",
      "Epoch 6 Batch 1600 Loss 2.0477 Accuracy 0.0846\n",
      "Epoch 6 Batch 1700 Loss 2.0452 Accuracy 0.0847\n",
      "Epoch 6 Batch 1800 Loss 2.0432 Accuracy 0.0848\n",
      "Epoch 6 Batch 1900 Loss 2.0428 Accuracy 0.0848\n",
      "Epoch 6 Loss 2.0417 Accuracy 0.0848\n",
      "Time taken for 1 epoch: 460.04573583602905 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.7456 Accuracy 0.0874\n",
      "Epoch 7 Batch 100 Loss 1.9579 Accuracy 0.0863\n",
      "Epoch 7 Batch 200 Loss 1.9556 Accuracy 0.0864\n",
      "Epoch 7 Batch 300 Loss 1.9660 Accuracy 0.0861\n",
      "Epoch 7 Batch 400 Loss 1.9718 Accuracy 0.0862\n",
      "Epoch 7 Batch 500 Loss 1.9759 Accuracy 0.0862\n",
      "Epoch 7 Batch 600 Loss 1.9771 Accuracy 0.0863\n",
      "Epoch 7 Batch 700 Loss 1.9752 Accuracy 0.0864\n",
      "Epoch 7 Batch 800 Loss 1.9772 Accuracy 0.0864\n",
      "Epoch 7 Batch 900 Loss 1.9761 Accuracy 0.0864\n",
      "Epoch 7 Batch 1000 Loss 1.9685 Accuracy 0.0864\n",
      "Epoch 7 Batch 1100 Loss 1.9663 Accuracy 0.0864\n",
      "Epoch 7 Batch 1200 Loss 1.9653 Accuracy 0.0865\n",
      "Epoch 7 Batch 1300 Loss 1.9648 Accuracy 0.0865\n",
      "Epoch 7 Batch 1400 Loss 1.9612 Accuracy 0.0865\n",
      "Epoch 7 Batch 1500 Loss 1.9600 Accuracy 0.0866\n",
      "Epoch 7 Batch 1600 Loss 1.9579 Accuracy 0.0866\n",
      "Epoch 7 Batch 1700 Loss 1.9562 Accuracy 0.0866\n",
      "Epoch 7 Batch 1800 Loss 1.9554 Accuracy 0.0867\n",
      "Epoch 7 Batch 1900 Loss 1.9559 Accuracy 0.0867\n",
      "Epoch 7 Loss 1.9549 Accuracy 0.0868\n",
      "Time taken for 1 epoch: 455.12806010246277 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.6791 Accuracy 0.0877\n",
      "Epoch 8 Batch 100 Loss 1.8856 Accuracy 0.0880\n",
      "Epoch 8 Batch 200 Loss 1.8826 Accuracy 0.0880\n",
      "Epoch 8 Batch 300 Loss 1.8944 Accuracy 0.0876\n",
      "Epoch 8 Batch 400 Loss 1.9001 Accuracy 0.0877\n",
      "Epoch 8 Batch 500 Loss 1.9058 Accuracy 0.0877\n",
      "Epoch 8 Batch 600 Loss 1.9068 Accuracy 0.0877\n",
      "Epoch 8 Batch 700 Loss 1.9056 Accuracy 0.0878\n",
      "Epoch 8 Batch 800 Loss 1.9076 Accuracy 0.0878\n",
      "Epoch 8 Batch 900 Loss 1.9075 Accuracy 0.0878\n",
      "Epoch 8 Batch 1000 Loss 1.9003 Accuracy 0.0879\n",
      "Epoch 8 Batch 1100 Loss 1.8982 Accuracy 0.0879\n",
      "Epoch 8 Batch 1200 Loss 1.8976 Accuracy 0.0879\n",
      "Epoch 8 Batch 1300 Loss 1.8978 Accuracy 0.0879\n",
      "Epoch 8 Batch 1400 Loss 1.8945 Accuracy 0.0879\n",
      "Epoch 8 Batch 1500 Loss 1.8937 Accuracy 0.0880\n",
      "Epoch 8 Batch 1600 Loss 1.8921 Accuracy 0.0880\n",
      "Epoch 8 Batch 1700 Loss 1.8905 Accuracy 0.0880\n",
      "Epoch 8 Batch 1800 Loss 1.8897 Accuracy 0.0881\n",
      "Epoch 8 Batch 1900 Loss 1.8908 Accuracy 0.0881\n",
      "Epoch 8 Loss 1.8900 Accuracy 0.0882\n",
      "Time taken for 1 epoch: 454.201691865921 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.5753 Accuracy 0.0914\n",
      "Epoch 9 Batch 100 Loss 1.8369 Accuracy 0.0889\n",
      "Epoch 9 Batch 200 Loss 1.8322 Accuracy 0.0891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 300 Loss 1.8412 Accuracy 0.0887\n",
      "Epoch 9 Batch 400 Loss 1.8456 Accuracy 0.0889\n",
      "Epoch 9 Batch 500 Loss 1.8487 Accuracy 0.0890\n",
      "Epoch 9 Batch 600 Loss 1.8511 Accuracy 0.0890\n",
      "Epoch 9 Batch 700 Loss 1.8504 Accuracy 0.0891\n",
      "Epoch 9 Batch 800 Loss 1.8535 Accuracy 0.0891\n",
      "Epoch 9 Batch 900 Loss 1.8545 Accuracy 0.0890\n",
      "Epoch 9 Batch 1000 Loss 1.8479 Accuracy 0.0890\n",
      "Epoch 9 Batch 1100 Loss 1.8466 Accuracy 0.0890\n",
      "Epoch 9 Batch 1200 Loss 1.8464 Accuracy 0.0890\n",
      "Epoch 9 Batch 1300 Loss 1.8459 Accuracy 0.0890\n",
      "Epoch 9 Batch 1400 Loss 1.8425 Accuracy 0.0891\n",
      "Epoch 9 Batch 1500 Loss 1.8417 Accuracy 0.0891\n",
      "Epoch 9 Batch 1600 Loss 1.8405 Accuracy 0.0891\n",
      "Epoch 9 Batch 1700 Loss 1.8390 Accuracy 0.0891\n",
      "Epoch 9 Batch 1800 Loss 1.8386 Accuracy 0.0892\n",
      "Epoch 9 Batch 1900 Loss 1.8400 Accuracy 0.0892\n",
      "Epoch 9 Loss 1.8394 Accuracy 0.0893\n",
      "Time taken for 1 epoch: 456.1937391757965 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.6044 Accuracy 0.0879\n",
      "Epoch 10 Batch 100 Loss 1.7920 Accuracy 0.0902\n",
      "Epoch 10 Batch 200 Loss 1.7904 Accuracy 0.0902\n",
      "Epoch 10 Batch 300 Loss 1.7988 Accuracy 0.0898\n",
      "Epoch 10 Batch 400 Loss 1.8031 Accuracy 0.0899\n",
      "Epoch 10 Batch 500 Loss 1.8065 Accuracy 0.0899\n",
      "Epoch 10 Batch 600 Loss 1.8080 Accuracy 0.0900\n",
      "Epoch 10 Batch 700 Loss 1.8080 Accuracy 0.0901\n",
      "Epoch 10 Batch 800 Loss 1.8119 Accuracy 0.0900\n",
      "Epoch 10 Batch 900 Loss 1.8132 Accuracy 0.0900\n",
      "Epoch 10 Batch 1000 Loss 1.8070 Accuracy 0.0900\n",
      "Epoch 10 Batch 1100 Loss 1.8061 Accuracy 0.0900\n",
      "Epoch 10 Batch 1200 Loss 1.8053 Accuracy 0.0900\n",
      "Epoch 10 Batch 1300 Loss 1.8051 Accuracy 0.0900\n",
      "Epoch 10 Batch 1400 Loss 1.8023 Accuracy 0.0900\n",
      "Epoch 10 Batch 1500 Loss 1.8018 Accuracy 0.0900\n",
      "Epoch 10 Batch 1600 Loss 1.8008 Accuracy 0.0900\n",
      "Epoch 10 Batch 1700 Loss 1.7995 Accuracy 0.0900\n",
      "Epoch 10 Batch 1800 Loss 1.7992 Accuracy 0.0901\n",
      "Epoch 10 Batch 1900 Loss 1.8009 Accuracy 0.0901\n",
      "Epoch 10 Loss 1.8004 Accuracy 0.0901\n",
      "Time taken for 1 epoch: 456.7311019897461 secs\n",
      "\n",
      "Time taken for training : 4582.354076862335 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS =10\n",
    "\n",
    "checkpoint_dir = './training_transformer_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cp.ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model = transformer)\n",
    "\n",
    "train_start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    # conservation du modèle tout les 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "print(\"Time taken for training : {} secs\".format(time.time() - train_start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e2557",
   "metadata": {},
   "source": [
    "### Chargement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df44af2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1b2c6c0f070>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_transformer_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cp.ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model = transformer)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddece10d",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85fddc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_en.num_words]\n",
    "    end_token = [tokenizer_en.num_words + 1]\n",
    "    \n",
    "    \n",
    "    # Encodage de la phrase en anglais, en rajoutant les tokens de début \n",
    "    # et de fin du vocabulaire anglais\n",
    "    encoder_input = start_token + tokenizer_en.texts_to_sequences([inp_sentence])[0] + end_token\n",
    "    \n",
    "    # Conversion de la liste en tensor de shape (1, seq_len + 2)\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    # initialisation de la traduction partielle avec le token \n",
    "    # de début du vocabulaire français\n",
    "    decoder_input = [tokenizer_fr.num_words]\n",
    "    \n",
    "    # Conversion de la liste en tensor de shape (1, 1) \n",
    "    # pour pouvoir appliquer le transformer dessus\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions est la nouvelle traduction partielle\n",
    "        predictions = transformer(encoder_input, \n",
    "                                         output,\n",
    "                                         False,\n",
    "                                         enc_padding_mask,\n",
    "                                         combined_mask,\n",
    "                                         dec_padding_mask)\n",
    "\n",
    "        # on récupère les probabilités pour le dernier mot prédit\n",
    "        # puis l'id du mot le plus probable\n",
    "        next_word = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        next_word_id = tf.cast(tf.argmax(next_word, axis=-1), tf.int32)\n",
    "\n",
    "        # si c'est le token de fin de phrase, on arrete l'auto-complétion\n",
    "        if next_word_id == tokenizer_fr.num_words+1:\n",
    "            \n",
    "            output = tf.squeeze(output, axis=0) # passage à 1 dimension\n",
    "            return output\n",
    "\n",
    "        # sinon, on rajoute le mot à la traduction partielle\n",
    "        output = tf.concat([output, next_word_id], axis=-1)\n",
    "    \n",
    "    # si on a atteint la longueur maximale, on retourne \n",
    "    # la traduction partielle actuelle\n",
    "    output = tf.squeeze(output, axis=0) # passage à 1 dimension\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c52e87",
   "metadata": {},
   "source": [
    "## Fonction de traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e94f7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inp_sentence):\n",
    "    result = evaluate(inp_sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_fr.sequences_to_texts([[i for i in result\n",
    "                            if i < tokenizer_fr.num_words]])[0]\n",
    "\n",
    "    print('Phrase originale : {}'.format(inp_sentence))\n",
    "    print('Traduction : {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17895934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_test(inp_sentence):\n",
    "    result = evaluate(inp_sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_fr.sequences_to_texts([[i for i in result\n",
    "                            if i < tokenizer_fr.num_words]])[0]\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a148494",
   "metadata": {},
   "source": [
    "## Evaluation du modèle sur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "feb78bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30977/30977 [1:26:25<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "    carac = [\"[\", \"]\"]\n",
    "    \n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = ''.join(tokenizer_en.sequences_to_texts([X_test[i]]))\n",
    "        for c in x_test:\n",
    "            if c in carac:\n",
    "                x_test = x_test.replace(c, \" \")\n",
    "        \n",
    "        y_pred = translate_test(x_test)\n",
    "        \n",
    "        y_true = ''.join(tokenizer_fr.sequences_to_texts([y_test[i]]))\n",
    "        for c in y_true:\n",
    "            if c in carac:\n",
    "                y_true = y_true.replace(c, \" \")\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod4_transformer_greedy.csv', index=False)\n",
    "        \n",
    "test(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83dc35de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhedh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you are old enough to know better than to act ...</td>\n",
       "      <td>tu es assez âgée pour savoir qu'il ne faut pas...</td>\n",
       "      <td>tu es assez âgée pour savoir que de prendre co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all the money was spent on clothes</td>\n",
       "      <td>tout l'argent a été dépensé dans des vêtements</td>\n",
       "      <td>tout l'argent a été passé à des vêtements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got off the train</td>\n",
       "      <td>il descendit du train</td>\n",
       "      <td>il a éteint le train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have you ever been run over</td>\n",
       "      <td>avez vous jamais été renversée</td>\n",
       "      <td>as tu jamais été au courant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you sure you can handle this</td>\n",
       "      <td>êtes vous sûrs de pouvoir gérer ceci</td>\n",
       "      <td>êtes vous sûres de pouvoir gérer ceci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0  you are old enough to know better than to act ...   \n",
       "1                 all the money was spent on clothes   \n",
       "2                               he got off the train   \n",
       "3                        have you ever been run over   \n",
       "4                   are you sure you can handle this   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0  tu es assez âgée pour savoir qu'il ne faut pas...   \n",
       "1     tout l'argent a été dépensé dans des vêtements   \n",
       "2                              il descendit du train   \n",
       "3                     avez vous jamais été renversée   \n",
       "4              êtes vous sûrs de pouvoir gérer ceci    \n",
       "\n",
       "                                       Sortie_modele  \n",
       "0  tu es assez âgée pour savoir que de prendre co...  \n",
       "1          tout l'argent a été passé à des vêtements  \n",
       "2                               il a éteint le train  \n",
       "3                        as tu jamais été au courant  \n",
       "4              êtes vous sûres de pouvoir gérer ceci  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports nécessaires \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# récupération des résultats sous forme d'un dataframe \n",
    "df_results=pd.read_csv('results_mod4_transformer_greedy.csv')   \n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9746e1",
   "metadata": {},
   "source": [
    "### Test de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b09ca8",
   "metadata": {},
   "source": [
    "Ce test de performance va servir de première approche au calcul d'un score du modèle. Il va se baser la recherche de similarité entre les phrases sorties du modèle et les gtraductions déjà proposées (phrases traduites) en calculant les deux scores suivants : \n",
    "- un score_diff qui représente la similarité dans la construction des phrases via le calcul du ratio du nombre de mots de différence entre la sortie du modèle et la phrase traduite sur le nombre de mots que contient cette dernière\n",
    "- un score_racine qui représente la similarité de sens des phrases via le calcul du ratio des racines communes aux deux phrases sur le nombres de racines dans la phrase traduite.\n",
    "\n",
    "La moyenne de ces deux scores servira à établir un premier score pour ce modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da0d4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'Nb_words_cible' fournissant le nombre de mots dans la phrase cible \n",
    "nb_words_cible=[]\n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "    nb_words_cible.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_cible']=nb_words_cible\n",
    "\n",
    "# Création d'une colonne 'Nb_words_mod' fournissant le nombre de mots dans la phrase prédite par le modèle \n",
    "nb_words_mod=[]\n",
    "for sentence in df_results['Sortie_modele']:\n",
    "    nb_words_mod.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_mod']=nb_words_mod\n",
    "\n",
    "# Création d'une colonne diff\n",
    "df_results['Différence']=df_results['nb_words_cible']-df_results['nb_words_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "535f47cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df_results.Différence)\n",
    "plt.title(\"Histogramme représentant la différence de nombre de mots entre la phrase traduite et \"\n",
    "          \"la phrase sortie du modèle\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "253fe38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La majorité des phrases sorties du modèle, soit 76.45 %, ne présente au plus qu'un mot d'écart avec la phrase traduite.\n"
     ]
    }
   ],
   "source": [
    "print(\"La majorité des phrases sorties du modèle, soit\", \n",
    "      round(len(df_results.loc[(df_results.Différence <= 1)&(df_results.Différence >= -1)])/df_results.shape[0]*100, 2),  \n",
    "      \"%, ne présente au plus qu'un mot d'écart avec la phrase traduite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7536f2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>nb_words_cible</th>\n",
       "      <th>nb_words_mod</th>\n",
       "      <th>Différence</th>\n",
       "      <th>racine_cible</th>\n",
       "      <th>racine_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you are old enough to know better than to act ...</td>\n",
       "      <td>tu es assez âgée pour savoir qu'il ne faut pas...</td>\n",
       "      <td>tu es assez âgée pour savoir que de prendre co...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"tu\", \"e\", \"assez\", \"âgé\", \"pour\", \"savoir\", ...</td>\n",
       "      <td>[\"tu\", \"e\", \"assez\", \"âgé\", \"pour\", \"savoir\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all the money was spent on clothes</td>\n",
       "      <td>tout l'argent a été dépensé dans des vêtements</td>\n",
       "      <td>tout l'argent a été passé à des vêtements</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"tout\", \"l'argent\", \"a\", \"été\", \"dépens\", \"da...</td>\n",
       "      <td>[\"tout\", \"l'argent\", \"a\", \"été\", \"pass\", \"à\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got off the train</td>\n",
       "      <td>il descendit du train</td>\n",
       "      <td>il a éteint le train</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"il\", \"descend\", \"du\", \"train\"]</td>\n",
       "      <td>[\"il\", \"a\", \"éteint\", \"le\", \"train\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have you ever been run over</td>\n",
       "      <td>avez vous jamais été renversée</td>\n",
       "      <td>as tu jamais été au courant</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"avez\", \"vous\", \"jam\", \"été\", \"renvers\"]</td>\n",
       "      <td>[\"as\", \"tu\", \"jam\", \"été\", \"au\", \"cour\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you sure you can handle this</td>\n",
       "      <td>êtes vous sûrs de pouvoir gérer ceci</td>\n",
       "      <td>êtes vous sûres de pouvoir gérer ceci</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"ête\", \"vous\", \"sûr\", \"de\", \"pouvoir\", \"ger\",...</td>\n",
       "      <td>[\"ête\", \"vous\", \"sûr\", \"de\", \"pouvoir\", \"ger\",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0  you are old enough to know better than to act ...   \n",
       "1                 all the money was spent on clothes   \n",
       "2                               he got off the train   \n",
       "3                        have you ever been run over   \n",
       "4                   are you sure you can handle this   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0  tu es assez âgée pour savoir qu'il ne faut pas...   \n",
       "1     tout l'argent a été dépensé dans des vêtements   \n",
       "2                              il descendit du train   \n",
       "3                     avez vous jamais été renversée   \n",
       "4              êtes vous sûrs de pouvoir gérer ceci    \n",
       "\n",
       "                                       Sortie_modele  nb_words_cible  \\\n",
       "0  tu es assez âgée pour savoir que de prendre co...              12   \n",
       "1          tout l'argent a été passé à des vêtements               8   \n",
       "2                               il a éteint le train               4   \n",
       "3                        as tu jamais été au courant               5   \n",
       "4              êtes vous sûres de pouvoir gérer ceci               7   \n",
       "\n",
       "   nb_words_mod  Différence  \\\n",
       "0            11           1   \n",
       "1             8           0   \n",
       "2             5          -1   \n",
       "3             6          -1   \n",
       "4             7           0   \n",
       "\n",
       "                                        racine_cible  \\\n",
       "0  [\"tu\", \"e\", \"assez\", \"âgé\", \"pour\", \"savoir\", ...   \n",
       "1  [\"tout\", \"l'argent\", \"a\", \"été\", \"dépens\", \"da...   \n",
       "2                   [\"il\", \"descend\", \"du\", \"train\"]   \n",
       "3          [\"avez\", \"vous\", \"jam\", \"été\", \"renvers\"]   \n",
       "4  [\"ête\", \"vous\", \"sûr\", \"de\", \"pouvoir\", \"ger\",...   \n",
       "\n",
       "                                          racine_mod  \n",
       "0  [\"tu\", \"e\", \"assez\", \"âgé\", \"pour\", \"savoir\", ...  \n",
       "1  [\"tout\", \"l'argent\", \"a\", \"été\", \"pass\", \"à\", ...  \n",
       "2               [\"il\", \"a\", \"éteint\", \"le\", \"train\"]  \n",
       "3           [\"as\", \"tu\", \"jam\", \"été\", \"au\", \"cour\"]  \n",
       "4  [\"ête\", \"vous\", \"sûr\", \"de\", \"pouvoir\", \"ger\",...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "stop_words = set([\",\", \".\", \"?\", \"!\", \":\", \";\"])\n",
    "\n",
    "# Intialisation de la racinisation en français \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "# Application à tout le dataframe \n",
    "\n",
    "# Instanciation des listes nécessaires \n",
    "racine_cible_tot=[]\n",
    "racine_cible=[]\n",
    "racine_mod_tot=[]\n",
    "racine_mod=[]\n",
    "\n",
    "# Création d'une colonne 'racine_cible' fournissant la liste des racines des mots des phrases de la colonne cible \n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "        racine_cible.append(stemmer.stem(mot))\n",
    "    racine_cible_tot.append(racine_cible)\n",
    "    racine_cible=[]\n",
    "    \n",
    "df_results['racine_cible']=racine_cible_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_cible'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_cible']]\n",
    "\n",
    "\n",
    "# Création d'une colonne 'racine_mod' fournissant la liste des racines des mots des phrases de la colonne prédite \n",
    "#par le modèle  \n",
    "for sentence in df_results['Sortie_modele']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        #if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "        racine_mod.append(stemmer.stem(mot))\n",
    "    racine_mod_tot.append(racine_mod)\n",
    "    racine_mod=[]\n",
    "\n",
    "df_results['racine_mod']=racine_mod_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_mod'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_mod']]\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d95f295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne \"comparaison racine\" fournissant le ratio \n",
    "#nombre de racines en commun / nombre de racines des phrases cibles\n",
    "\n",
    "# Création d'une colonne \"comparaison différence\" fournissant le ratio \n",
    "#différence du nombre de mots / nombre de mots cibles\n",
    "\n",
    "score_racine=[]\n",
    "score_difference = []\n",
    "for i in range(df_results.shape[0]):\n",
    "    score_racine.append(len(set(df_results.iloc[i,6]) & set(df_results.iloc[i,7]))/len(df_results.iloc[i,6]))\n",
    "    if (abs(df_results.iloc[i, 5])/df_results.iloc[i,3]) > 1:\n",
    "        score_diff = 0\n",
    "    else :\n",
    "        score_diff = 1 - abs(df_results.iloc[i, 5])/df_results.iloc[i,3]\n",
    "    score_difference.append(score_diff)\n",
    "    \n",
    "df_results['score_racine']=score_racine\n",
    "\n",
    "df_results['score_diff']= score_difference\n",
    "\n",
    "df_results['score_tot']=(df_results['score_diff']+ df_results['score_racine'])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "256237dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you are old enough to know better than to act ...</td>\n",
       "      <td>tu es assez âgée pour savoir qu'il ne faut pas...</td>\n",
       "      <td>tu es assez âgée pour savoir que de prendre co...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all the money was spent on clothes</td>\n",
       "      <td>tout l'argent a été dépensé dans des vêtements</td>\n",
       "      <td>tout l'argent a été passé à des vêtements</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got off the train</td>\n",
       "      <td>il descendit du train</td>\n",
       "      <td>il a éteint le train</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have you ever been run over</td>\n",
       "      <td>avez vous jamais été renversée</td>\n",
       "      <td>as tu jamais été au courant</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you sure you can handle this</td>\n",
       "      <td>êtes vous sûrs de pouvoir gérer ceci</td>\n",
       "      <td>êtes vous sûres de pouvoir gérer ceci</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0  you are old enough to know better than to act ...   \n",
       "1                 all the money was spent on clothes   \n",
       "2                               he got off the train   \n",
       "3                        have you ever been run over   \n",
       "4                   are you sure you can handle this   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0  tu es assez âgée pour savoir qu'il ne faut pas...   \n",
       "1     tout l'argent a été dépensé dans des vêtements   \n",
       "2                              il descendit du train   \n",
       "3                     avez vous jamais été renversée   \n",
       "4              êtes vous sûrs de pouvoir gérer ceci    \n",
       "\n",
       "                                       Sortie_modele  score_racine  \\\n",
       "0  tu es assez âgée pour savoir que de prendre co...          0.50   \n",
       "1          tout l'argent a été passé à des vêtements          0.75   \n",
       "2                               il a éteint le train          0.50   \n",
       "3                        as tu jamais été au courant          0.40   \n",
       "4              êtes vous sûres de pouvoir gérer ceci          1.00   \n",
       "\n",
       "   score_diff  score_tot  \n",
       "0    0.916667   0.708333  \n",
       "1    1.000000   0.875000  \n",
       "2    0.750000   0.625000  \n",
       "3    0.800000   0.600000  \n",
       "4    1.000000   1.000000  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.drop([\"nb_words_cible\", \"nb_words_mod\", \"Différence\", \"racine_cible\", \"racine_mod\"], axis = 1)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d644a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne du score_racine 0.653370213905087\n",
      "moyenne du score_diff 0.8589311233082378\n",
      "Score du modèle au test de performance 0.7561506686066941\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne du score_tot\n",
    "\n",
    "print(\"moyenne du score_racine\", df_results['score_racine'].mean())\n",
    "print(\"moyenne du score_diff\", df_results['score_diff'].mean())\n",
    "print(\"Score du modèle au test de performance\", df_results['score_tot'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f8eb4",
   "metadata": {},
   "source": [
    "Ce modèle présente un score de 75% au test de performance, ce qui n'est pas négligeable.\n",
    "Cependant, d'autres méthodes de scoring existent, largement diffusées et appliquées aux travaux de traduction notamment.\n",
    "Il s'agit des scores BLEU et ROUGE.\n",
    "\n",
    "Il sera intéressant de comparer ces deux méthodes à ce test de performance afin de valider le score obtenu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4a704",
   "metadata": {},
   "source": [
    "### Score BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab3489",
   "metadata": {},
   "source": [
    "Le score BLEU (Bilingual Evaluation Understudy Score) est le score de référence pour évaluer les traductions émises par les systèmes de traductions automatiques (phreses candidates) par rapport à des phrases traduites dites de référence.\n",
    "Ce score a été mis en place par les travaux de Kishore Papineni, et al. en 2002 dans “BLEU: a Method for Automatic Evaluation of Machine Translation“.\n",
    "\n",
    "Il permet de calculer un ratio des n-grams communs aux phrases candidates et de référence sur les n-grams présents dans les références (n-grams représentant des groupes de tokens de n tokens) sans distinction d'ordre, et ressort un score compris entre 0 et 1, un score de 1 constituant une correspondance parfaite.\n",
    "\n",
    "En affectant des poids sur les n premiers niveaux de n-grams, on peut obtenir des scores individuels n-grams lorsque l'intégralité des poids est affectée au niveau d'un unique niveau n de n-grams, et des scores cumulatifs n-grams lorsque les poids sont affectés sur plusieurs niveaux n de n-grams (le score représente alors la moyenne géométrique des scores de chaque niveau). Par défaut, les poids de la fonction utilisée (sentence_bleu) sont de 0.25 sur chacun des niveau de 1-gram à 4-gram.\n",
    "\n",
    "Dans le cas de notre jeu de données, sachant que certaines phrases du corpus ne sont constituées que d'un seul mots, nous allons appliquer un score BLEU individual 1-gram (weights = (1, 0, 0, 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a3fd4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\mhedh\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you are old enough to know better than to act ...</td>\n",
       "      <td>tu es assez âgée pour savoir qu'il ne faut pas...</td>\n",
       "      <td>tu es assez âgée pour savoir que de prendre co...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.498055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all the money was spent on clothes</td>\n",
       "      <td>tout l'argent a été dépensé dans des vêtements</td>\n",
       "      <td>tout l'argent a été passé à des vêtements</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got off the train</td>\n",
       "      <td>il descendit du train</td>\n",
       "      <td>il a éteint le train</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have you ever been run over</td>\n",
       "      <td>avez vous jamais été renversée</td>\n",
       "      <td>as tu jamais été au courant</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you sure you can handle this</td>\n",
       "      <td>êtes vous sûrs de pouvoir gérer ceci</td>\n",
       "      <td>êtes vous sûres de pouvoir gérer ceci</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0  you are old enough to know better than to act ...   \n",
       "1                 all the money was spent on clothes   \n",
       "2                               he got off the train   \n",
       "3                        have you ever been run over   \n",
       "4                   are you sure you can handle this   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0  tu es assez âgée pour savoir qu'il ne faut pas...   \n",
       "1     tout l'argent a été dépensé dans des vêtements   \n",
       "2                              il descendit du train   \n",
       "3                     avez vous jamais été renversée   \n",
       "4              êtes vous sûrs de pouvoir gérer ceci    \n",
       "\n",
       "                                       Sortie_modele  score_racine  \\\n",
       "0  tu es assez âgée pour savoir que de prendre co...          0.50   \n",
       "1          tout l'argent a été passé à des vêtements          0.75   \n",
       "2                               il a éteint le train          0.50   \n",
       "3                        as tu jamais été au courant          0.40   \n",
       "4              êtes vous sûres de pouvoir gérer ceci          1.00   \n",
       "\n",
       "   score_diff  score_tot  Score_bleu  \n",
       "0    0.916667   0.708333    0.498055  \n",
       "1    1.000000   0.875000    0.750000  \n",
       "2    0.750000   0.625000    0.400000  \n",
       "3    0.800000   0.600000    0.333333  \n",
       "4    1.000000   1.000000    0.857143  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score_bleu = []\n",
    "for i in range(df_results.shape[0]):\n",
    "    reference = [''.join(df_results.iloc[i, 1]).split()]\n",
    "    candidate = ''.join(df_results.iloc[i, 2]).split()\n",
    "    score_bleu.append(sentence_bleu(reference, candidate, weights = (1,0,0,0)))\n",
    "df_results[\"Score_bleu\"] = score_bleu\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ba4f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score BLEU du modèle 0.5992656185429632\n"
     ]
    }
   ],
   "source": [
    "# Score BLEU du modèle : \n",
    "print(\"Score BLEU du modèle\", df_results['Score_bleu'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c34f2",
   "metadata": {},
   "source": [
    "### Score ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7cf26",
   "metadata": {},
   "source": [
    "Le score ROUGE (Recall-Oriented Understudy for Gisting Evaluation) s'utilise principalement dans les travaux de resumés automatique de texte mais également pour les traductions automatiques. \n",
    "Comme pour le score BLEU, le but est de comparer la traduction du modèle à une traduction de référence, et la méthode peut s'utiliser sur différents niveaux de n-grams.\n",
    "\n",
    "Ce score permet alors d'obtenir trois métriques : \n",
    "- le rappel (recall) qui se calcule par le ratio du nombre de n-grams commums aux deux phrases sur le nombre de n-grams dans la phrase de référence. Cela permet de juger si la traduction du modèle contient bien l'ensemble des mots de la phrase de référence. \n",
    "Cependant, cette métrique seule ne permettra pas de juger de la qualité de la traduction : en effet, le modèle peut ressortir des phrases très longues qui contiennet l'ensemble des mots de la référence sans que la traduction ne soit pertinente : la précision permet alors d'affiner la qualité de la traduction.\n",
    "- la précision qui se calcule par le ratio du nombre de n-grams commums aux deux phrases sur le nombre de n-grams dans la traduction du modèle. Cela permet de juger de la pertinence des mots utilisés dans la traduction du modèle.\n",
    "- le f1_score qui correspond à la moyenne harmonique des deux métriques précédentes.\n",
    "\n",
    "Comme pour le score BLEU, on applique une score ROUGE-1 (sur 1-gram). On considère le f1_score comme le score ROUGE du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0af12e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "r = Rouge()\n",
    "precision_col = []\n",
    "recall_col = []\n",
    "fscore_col = []\n",
    "\n",
    "for i in range(df_results.shape[0]):\n",
    "    reference = df_results.iloc[i, 1]\n",
    "    candidate = df_results.iloc[i, 2]\n",
    "    score_rouge = r.get_scores(candidate, reference)\n",
    "    precision_col.append(score_rouge[0]['rouge-1']['p'])\n",
    "    recall_col.append(score_rouge[0]['rouge-1']['r'])\n",
    "    fscore_col.append(score_rouge[0]['rouge-1']['f'])\n",
    "\n",
    "df_results['Rouge_recall'] = recall_col\n",
    "df_results['Rouge_precision'] = precision_col\n",
    "df_results['Rouge_f1_score'] = fscore_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b9a2b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE recall du modèle 0.6351053523108378\n",
      "Score ROUGE precision du modèle 0.6462375986502044\n",
      "Score ROUGE f1_score du modèle 0.6356379752715831\n"
     ]
    }
   ],
   "source": [
    "# Score ROUGE du modèle : \n",
    "print(\"Score ROUGE recall du modèle\", df_results['Rouge_recall'].mean())\n",
    "print(\"Score ROUGE precision du modèle\", df_results['Rouge_precision'].mean())\n",
    "print(\"Score ROUGE f1_score du modèle\", df_results['Rouge_f1_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b358f",
   "metadata": {},
   "source": [
    "La méthode BLEU attribue un score de 60% au modèle alors que la méthode ROUGE lui attribue un score de 63% (la composante f1_score sert de score ROUGE global regroupant l'information du recall et de la précision).\n",
    "Ces scores similaires contribuent à affirmer que le score du modèle serait de 60% environ.\n",
    "\n",
    "Le premier aperçu du score grâce au test de performance, dont il ressort un score bien plus important que les scores ci-dessus, n'est pas assez précis pour servir de score unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e6fc68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.drop([\"score_racine\", \"score_diff\", \"Rouge_recall\", \"Rouge_precision\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b89a4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>score_tot</th>\n",
       "      <th>Score_bleu</th>\n",
       "      <th>Rouge_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you are old enough to know better than to act ...</td>\n",
       "      <td>tu es assez âgée pour savoir qu'il ne faut pas...</td>\n",
       "      <td>tu es assez âgée pour savoir que de prendre co...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.498055</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all the money was spent on clothes</td>\n",
       "      <td>tout l'argent a été dépensé dans des vêtements</td>\n",
       "      <td>tout l'argent a été passé à des vêtements</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got off the train</td>\n",
       "      <td>il descendit du train</td>\n",
       "      <td>il a éteint le train</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have you ever been run over</td>\n",
       "      <td>avez vous jamais été renversée</td>\n",
       "      <td>as tu jamais été au courant</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you sure you can handle this</td>\n",
       "      <td>êtes vous sûrs de pouvoir gérer ceci</td>\n",
       "      <td>êtes vous sûres de pouvoir gérer ceci</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0  you are old enough to know better than to act ...   \n",
       "1                 all the money was spent on clothes   \n",
       "2                               he got off the train   \n",
       "3                        have you ever been run over   \n",
       "4                   are you sure you can handle this   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0  tu es assez âgée pour savoir qu'il ne faut pas...   \n",
       "1     tout l'argent a été dépensé dans des vêtements   \n",
       "2                              il descendit du train   \n",
       "3                     avez vous jamais été renversée   \n",
       "4              êtes vous sûrs de pouvoir gérer ceci    \n",
       "\n",
       "                                       Sortie_modele  score_tot  Score_bleu  \\\n",
       "0  tu es assez âgée pour savoir que de prendre co...   0.708333    0.498055   \n",
       "1          tout l'argent a été passé à des vêtements   0.875000    0.750000   \n",
       "2                               il a éteint le train   0.625000    0.400000   \n",
       "3                        as tu jamais été au courant   0.600000    0.333333   \n",
       "4              êtes vous sûres de pouvoir gérer ceci   1.000000    0.857143   \n",
       "\n",
       "   Rouge_f1_score  \n",
       "0        0.521739  \n",
       "1        0.750000  \n",
       "2        0.444444  \n",
       "3        0.363636  \n",
       "4        0.857143  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
