{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a6ce74",
   "metadata": {},
   "source": [
    "# Couches d'attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed0257",
   "metadata": {},
   "source": [
    "### Définition de la fonction d'attention par produit scalaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ecd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Permet de calculer les poids d'attention via une fonction d'attention\n",
    "    \n",
    "    Arguments:\n",
    "    q: requête de format == (..., seq_len_q, depth)\n",
    "    v: valeurs possibles de format == (..., seq_len_v, depth_v)\n",
    "    k: clés de ces valeurs de format == (..., seq_len_k, depth)\n",
    "    mask: Float tensor\n",
    "\n",
    "    Le produit scalaire (normalisé) peut être utilisé comme mesure de similarité entre deux vecteurs de la manière suivante:\n",
    "    - Si le produit scalaire entre la requête et une clé est positif: \n",
    "        Les deux vecteurs sont similaires.\n",
    "    - Si le produit scalaire entre la requête et une clé est négatif: \n",
    "        Les deux vecteurs sont très différents.\n",
    "        \n",
    "    Retourne:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcul de la dimension des clés \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    \n",
    "    # Calcul matriciel de q et de la transposée de k\n",
    "    dot_products = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Division par la racine carrée de dk -> normalisation pour contrer le problème de vanishing gradient\n",
    "    scaled_dot_products = dot_products / tf.math.sqrt(dk)\n",
    "\n",
    "    # Application du masque afin d'obtenir des séquences de base de données de même longueur\n",
    "    if mask is not None:\n",
    "        # Multiplicaiton du masque par une très petite valeur (considérée comme moins l'infini)\n",
    "        mask = mask * (-1e10)\n",
    "        \n",
    "        # Somme du produit scalaire \"scaled_dot_products\" et du masque \n",
    "        scaled_dot_products = scaled_dot_products + mask\n",
    "    \n",
    "    # Application de la fonction \"softmax\" permettant d'obtenir les poids d'attention\n",
    "    attention_weights = tf.nn.softmax(scaled_dot_products)\n",
    "    \n",
    "    # Calcul du vecteur d'attention par calcul matriciel entre\n",
    "    # les poids d'attention et les valeurs possibles\n",
    "    attention_vector = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return attention_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31845fe",
   "metadata": {},
   "source": [
    "### Définition d'une couche d'attention multi-têtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60c2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    # création d'une classe multi-tête qui hérite de la classe tf.keras.layers.Layer\n",
    "    # classe devra être entrainable \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # nombre de têtes \n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Scinde la dernière dimension de x en (num_heads, depth).\n",
    "        Transforme le résultat de telle sorte à ce qu'il soit de la forme\n",
    "        # (batch_size, num_heads, seq_len, depth)        \n",
    "        \"\"\"\n",
    "        # Restructuration de manière à pouvoir scinder \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        output = tf.transpose(x, perm=[0, 2, 1, 3])   \n",
    "        \n",
    "        return output\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Application des matrices denses à Q, K et V\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        # Découpage des vecteurs selon la fonction définie ci-avant \n",
    "        q = self.split_heads(q, batch_size) \n",
    "        k = self.split_heads(k, batch_size) \n",
    "        v = self.split_heads(v, batch_size) \n",
    "\n",
    "        # Application du masque précédent \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        # Concaténation des vecteurs d'attention en remodelant le tensor \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # Application de la matrice dense à la sortie\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b2d7b",
   "metadata": {},
   "source": [
    "## Encodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3560b1",
   "metadata": {},
   "source": [
    "### Couche d'encodage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ba34bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape  (64, 43, 512)\n",
      "Output Shape (64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "# Définition de la couche d'encodage comprenant: \n",
    "\n",
    "    # - Des couches de Dropout et Layer Normalisation pour régulariser le modèle.\n",
    "    # - Des Skip Connections pour éviter le problème de vanishing gradient à cause des petits gradients de la fonction softmaxsoftmax.\n",
    "    # - Un réseau dense (c'est-à-dire un MLP ou Feed Forward Neural Network comme mentionné dans le papier) est ajouté après l'attention multi-têtes.\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = tf.keras.models.Sequential()\n",
    "        self.ffn.add(tf.keras.layers.Dense(dff, activation='relu'))\n",
    "        self.ffn.add(tf.keras.layers.Dense(d_model))\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # Step 1 Multihead attentions\n",
    "        A, _ = self.mha(x, x, x, mask)\n",
    "        \n",
    "        # Step 2 Dropout\n",
    "        A = self.dropout1(A, training=training)\n",
    "        \n",
    "        # Step 3 Skip connection \n",
    "        x = A + x\n",
    "        \n",
    "        # Step 4 Layer normalization \n",
    "        x = self.layernorm1(x)\n",
    "        \n",
    "        # Step 5 Feed Forward Network \n",
    "        O = self.ffn(x)\n",
    "        \n",
    "        # Step 6 Dropout\n",
    "        O = self.dropout2(O)\n",
    "        \n",
    "        # Step 7 Skip connection \n",
    "        x = O + x\n",
    "        \n",
    "        # Step 8 Layer normalization \n",
    "        output = self.layernorm2(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "# Vérifications\n",
    "\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "input_shape = (64, 43, 512)\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform(input_shape), False, None)\n",
    "\n",
    "print(\"Input Shape \", input_shape)\n",
    "print(\"Output Shape\", sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7d7e2",
   "metadata": {},
   "source": [
    "### Encodage de la position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf75d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position_max, d_model):\n",
    "    i = np.linspace(0, d_model-1, d_model)\n",
    "    \n",
    "    pos_encoding = np.zeros((position_max, d_model))\n",
    "    \n",
    "    for pos, row in enumerate(pos_encoding):\n",
    "        pos_encoding[pos, ::2] = np.sin(pos/(10000 ** (i[::2]/d_model)))\n",
    "        pos_encoding[pos, 1::2] = np.cos(pos/(10000 ** ((i[1::2]-1)/d_model))) \n",
    "    \n",
    "    return tf.cast(pos_encoding.reshape(1, position_max, d_model), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d865e46",
   "metadata": {},
   "source": [
    "### Encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c4f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        \"\"\"\n",
    "        num_layers : Number of Encoding layers in the Encoder.\n",
    "        \n",
    "        d_model : Dimensionality of the embedding layer's output.\n",
    "        \n",
    "        num_heads : Number of attention heads in the Multi-Head Attention layers.\n",
    "        \n",
    "        dff : Number of neurons in the first layer of the Feed Forward Network in the Encoding layers.\n",
    "        \n",
    "        input_vocab_size : Size of the vocabulary the embedding layer should train on.\n",
    "        \n",
    "        maximum_position_encoding : Maximum number of tokens in a sequence.\n",
    "        \n",
    "        rate : Dropout rate in the Encoding layers.      \n",
    "        \n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Generate Embedding\n",
    "        x = self.embedding(x) \n",
    "\n",
    "        # Add Positional Encoding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Apply Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Apply Encoding Layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be057965",
   "metadata": {},
   "source": [
    "## Décodeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b245a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        \"\"\"        \n",
    "        d_model : Dimensionality of the embedding layer's output.\n",
    "        \n",
    "        num_heads : Number of attention heads in the Multi-Head Attention layers.\n",
    "        \n",
    "        dff : Number of neurons in the first layer of the Feed Forward Network in the Decoding layers.\n",
    "        \n",
    "        rate : Dropout rate in the Decoding layers.      \n",
    "        \"\"\"\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.models.Sequential()\n",
    "        self.ffn.add(tf.keras.layers.Dense(dff, activation='relu'))\n",
    "        self.ffn.add(tf.keras.layers.Dense(d_model))\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        x : Input Tensor of the embeddings of the partially translated sentence.\n",
    "        \n",
    "        enc_output : Output Tensor of the Decoder.\n",
    "        \n",
    "        training : Whether the model is training or not so that Dropout is only applied during training.\n",
    "        \n",
    "        look_ahead_mask : Tensor for masking tokens that have not been translated yet.\n",
    "        \n",
    "        padding_mask : Tensor masking padding tokens of the Decoder's input.      \n",
    "        \"\"\"\n",
    "\n",
    "        A1, _  = self.mha1(x, x, x, look_ahead_mask)\n",
    "        A1     = self.dropout1(A1, training=training)\n",
    "        x      = A1 + x\n",
    "        x      = self.layernorm1(x)\n",
    "\n",
    "        A2, _  = self.mha2(enc_output, enc_output, x, padding_mask)\n",
    "        A2     = self.dropout2(A2, training=training)\n",
    "        x      = A2 + x\n",
    "        x      = self.layernorm2(x) \n",
    "\n",
    "        O      = self.ffn(x) \n",
    "        O      = self.dropout3(O, training=training)\n",
    "        x      = O + x\n",
    "        Output = self.layernorm3(x) \n",
    "\n",
    "        return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e35825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        \"\"\"\n",
    "        num_layers : Number of Decoding layers in the Decoder.\n",
    "        \n",
    "        d_model : Dimensionality of the embedding layer's output.\n",
    "        \n",
    "        num_heads : Number of attention heads in the Multi-Head Attention layers.\n",
    "        \n",
    "        dff : Number of neurons in the first layer of the Feed Forward Network in the Decoding layers.\n",
    "        \n",
    "        target_vocab_size : Size of the vocabulary the embedding layer should train on.\n",
    "        \n",
    "        maximum_position_encoding : Maximum number of tokens in a sequence.\n",
    "        \n",
    "        rate : Dropout rate in the Decoding layers.      \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        x : Input Tensor of the partially translated sentence.\n",
    "        \n",
    "        enc_output : Output Tensor of the Encoder.\n",
    "        \n",
    "        training : Whether the model is training or not so that Dropout is only applied during training.\n",
    "        \n",
    "        look_ahead_mask : Tensor for masking tokens that have not been translated yet.\n",
    "        \n",
    "        padding_mask : Tensor masking padding tokens of the Decoder's input.      \n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Generate Embedding\n",
    "        x = self.embedding(x) \n",
    "\n",
    "        # Add Positional Encoding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Apply Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Apply Decoding Layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, training,\n",
    "                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c3ecc",
   "metadata": {},
   "source": [
    "## Transformeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b419745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        \"\"\"\n",
    "        num_layers : Number of Encoding/Decoding layers in the Encoder/Decoder.\n",
    "        d_model    : Dimensionality of the embedding layers' output.\n",
    "        num_heads  : Number of attention heads in the Multi-Head Attention layers.\n",
    "        dff        : Number of neurons in the first layer of the Feed Forward Network in the Encoding/Decoding layers.\n",
    "        rate       : Dropout rate in the Encoding/Decoding layers.    \n",
    "        \n",
    "        input_vocab_size  : Size of the vocabulary the Encoder's embedding layer should train on.\n",
    "        target_vocab_size : Size of the vocabulary the Encoder's embedding layer should train on.\n",
    "        \n",
    "        \n",
    "        pe_input  : Maximum number of tokens in the input sequence of the Encoder.\n",
    "        pe_target : Maximum number of tokens in the input sequence of the Decoder.\n",
    "        \"\"\"       \n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation = 'softmax')\n",
    "    \n",
    "    def call(self, encoder_input, decoder_input ,training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        encoder_input : Input Tensor of the original sentence to be given to the Encoder.\n",
    "        decoder_input : Input Tensor of the partially translated sentence to be given to the Decoder.\n",
    "        \n",
    "        training : Whether the model is training or not so that Dropout is only applied during training.\n",
    "        \n",
    "        enc_padding_mask : Tensor masking padding tokens of the Encoder's input.\n",
    "        look_ahead_mask  : Tensor for masking tokens that have not been translated yet.\n",
    "        dec_padding_mask : Tensor masking padding tokens of the Decoder's input.      \n",
    "        \"\"\"\n",
    "\n",
    "        # Get Encoder Output\n",
    "        enc_output = self.encoder(encoder_input, training, enc_padding_mask)\n",
    "\n",
    "        # Get Decoder Output\n",
    "        dec_output = self.decoder(decoder_input, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # Perform Classification\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6b3498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape:      (64, 62)\n",
      "Decoder Input Shape:      (64, 62)\n",
      "Transformer Output Shape: (64, 62, 32000)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "d_model    = 128\n",
    "dff        = 512\n",
    "num_heads  = 8\n",
    "\n",
    "input_vocab_size  = 32000\n",
    "target_vocab_size = 32000\n",
    "dropout_rate      = 0.1\n",
    "\n",
    "sample_transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, target_vocab_size, \n",
    "                                 pe_input = 62, \n",
    "                                 pe_target= 62,\n",
    "                                 rate=dropout_rate)\n",
    "\n",
    "sample_encoder_input = tf.random.uniform((64, 62))\n",
    "sample_decoder_input = tf.random.uniform((64, 62))\n",
    "\n",
    "sample_transformer_output = sample_transformer(sample_encoder_input,\n",
    "                                               sample_decoder_input,\n",
    "                                               False,\n",
    "                                               None, \n",
    "                                               None, \n",
    "                                               None)\n",
    "\n",
    "print(\"Encoder Input Shape:     \", sample_encoder_input.shape)\n",
    "print(\"Decoder Input Shape:     \", sample_decoder_input.shape)\n",
    "print(\"Transformer Output Shape:\", sample_transformer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a9239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
