{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec24d92",
   "metadata": {},
   "source": [
    "# Modélisation 3 : modèle Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece91cb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39989f",
   "metadata": {},
   "source": [
    "Dans l'itération précédente, nous avions étudié un modèle d'Embedding fournissant une traduction mot à mot. Nous avions constaté que ce système avait ses limites car il ne prenait pas en compte le contexte des mots dans une phrase. \n",
    "\n",
    "Dans cette itération, nous avons utilisé un modèle Seq2Seq en incorporant un mécanisme d'attention.  Ce type de modèle permet de travailler sur des phrases entières et donc de prendre en compte le contexte. \n",
    "\n",
    "Nous avons travaillé sur le \"premier\" jeu de données fourni, à savoir un ensemble de phrases identiques traduites en français et en anglais. Pour mémoire, ce jeu de données comprend environ 130 000 phrases pour un nombre de mots uniques assez faible (environ 200-300 mots par langue). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34a8db",
   "metadata": {},
   "source": [
    "## Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e45297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, RNN, Lambda, Embedding\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd0e0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des données \n",
    "data = pd.read_csv(\"small_vocab_fr-eng.csv\", sep = ';')\n",
    "\n",
    "# Suppression des doublons \n",
    "data.drop_duplicates(inplace = True)\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "#data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "764cdbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Français</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; new jersey est parfois calme pendant l...</td>\n",
       "      <td>&lt;start&gt; new jersey is sometimes quiet during a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; les etats unis est generalement froid ...</td>\n",
       "      <td>&lt;start&gt; the united states is usually chilly du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; california est generalement calme en m...</td>\n",
       "      <td>&lt;start&gt; california is usually quiet during mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; les etats unis est parfois legere en j...</td>\n",
       "      <td>&lt;start&gt; the united states is sometimes mild du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; votre moins aime fruit est le raisin ,...</td>\n",
       "      <td>&lt;start&gt; your least liked fruit is the grape , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Français  \\\n",
       "0  <start> new jersey est parfois calme pendant l...   \n",
       "1  <start> les etats unis est generalement froid ...   \n",
       "2  <start> california est generalement calme en m...   \n",
       "3  <start> les etats unis est parfois legere en j...   \n",
       "4  <start> votre moins aime fruit est le raisin ,...   \n",
       "\n",
       "                                             English  \n",
       "0  <start> new jersey is sometimes quiet during a...  \n",
       "1  <start> the united states is usually chilly du...  \n",
       "2  <start> california is usually quiet during mar...  \n",
       "3  <start> the united states is sometimes mild du...  \n",
       "4  <start> your least liked fruit is the grape , ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nettoyage des données \n",
    "\n",
    "# Fonction de conversion d'un document d'unicode vers ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Prétraitement des phrases\n",
    "def clean_sentence(w):\n",
    "   \n",
    "    # conversion unicode vers ascii\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # séparation entre un mot et sa ponctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # remplacement de caractères spéciaux par un espace\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # ajout des balises <start> et <end> placées respectivement au début et à la fin des phrases\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne english\n",
    "data.English = data.English.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "# Appliquer la fonction clean_sentence sur la colonne french\n",
    "data.Français = data.Français.apply(lambda x: clean_sentence(x))\n",
    "\n",
    "# Visulisation des 5 premières lignes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3a1c68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire :\n",
      " - input : 206\n",
      " - target : 331\n"
     ]
    }
   ],
   "source": [
    "# Structuration des données \n",
    "\n",
    "def tokenize(sentences):\n",
    "    # tokenization des phrases\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    # transformation des phrases en séquences\n",
    "    seq = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # complète les séquences de sorte à ce qu'ils aient la même longueur\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq,padding='post')\n",
    "\n",
    "    # post : le padding est réalisé à la fin des séquences\n",
    "    return seq, tokenizer\n",
    "\n",
    "# Tokenization(transforme une phrase en une liste de nombres qui correspondent chacun à un mot unique du vocabulaire)\n",
    "input_seq, input_tokenizer = tokenize(data.English) # phrases tokenisées en fonction du vocabulaire anglais  \n",
    "target_seq, target_tokenizer = tokenize(data.Français) # phrases tokenisées en fonction du vocabulaire français \n",
    "# input_tokenizer: est le tokenizer entraîné sur le jeu de données des phrases anglaises\n",
    "\n",
    "# Calcul de la taille des vocabulaires via l'attribut .word_index qui transforme l'index d'un mot en sa chaîne de caractères \n",
    "vocab_size_inp = len(input_tokenizer.word_index)+1 # nombre de mots différents en français \n",
    "vocab_size_targ = len(target_tokenizer.word_index)+1 # nombre de mots différents en anglais \n",
    "\n",
    "# Calcul de longeur maximale des séquences \n",
    "max_length_inp, max_length_targ = input_seq.shape[1], target_seq.shape[1] \n",
    "\n",
    "print(\"Taille du vocabulaire :\")\n",
    "print(\" - input :\", vocab_size_inp)\n",
    "print(\" - target :\", vocab_size_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410be19",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00687e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du batch input :  (80, 19)\n",
      "Dimensions du batch target :  (80, 25)\n"
     ]
    }
   ],
   "source": [
    "# Séparation des données en un ensemble d'apprentissage et de test\n",
    "\n",
    "# Création des ensembles d'apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_seq, target_seq,test_size=0.2)\n",
    "\n",
    "# Paramètres de l'entrainement\n",
    "# buffer_size: permet de déterminer combien d'éléments vont être mélangés au fur et à mesure, \n",
    "# commme la BDD est importante 103 339 phrases -> divisé par 5\n",
    "# batch_size: sépare dans l'ordre le dataset en jeux de données de dimension fixe\n",
    "batch_size = 80 \n",
    "buffer_size = int(len(X_train)/5) \n",
    "steps_per_epoch = buffer_size//batch_size\n",
    "\n",
    "# Création du dataset d'entrainement\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Affichage des deux composantes d'un jeu de données\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(\"Dimensions du batch input : \", example_input_batch.shape)\n",
    "print(\"Dimensions du batch target : \", example_target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3740030",
   "metadata": {},
   "source": [
    "## Modèle sélectionné"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c53eff36",
   "metadata": {},
   "source": [
    "Le modèle Seq2Seq se base sur une architecture de RNN (réseaux de neurones récurrents) avec un encodeur et un décodeur. \n",
    "\n",
    "Ce système permet de \"lire\" les phrases avec l'Encodeur pour dégager une signification globale, transcrite mathématiquement sous la forme d'un vecteur \"sens\". Le modèle traite ensuite ce vecteur pour procéder à la traduction via le Décodeur. \n",
    "\n",
    "A ce modèle de base Seq2Seq, nous rajouté un mécanisme d'attention qui permet de conserver un maximum d'information sur la séquence source et son contexte. Concrètement, cela se traduit par la construction successive de 2 vecteurs principaux (vecteur d'alignement et vecteur contexte). \n",
    "\n",
    "Le vecteur d'alignement aura la même longueur que la séquence source, chacune de ses valeurs est le score (ou la probabilité) que le mot corresponde à celui de la séquence source. \n",
    "\n",
    "Le vecteur de contexte est utilisé comme inputs par le Décodeur. Il s’agit de la moyenne pondérée de la sortie de l’Encodeur. Il est obtenu en réalisant le produit scalaire du vecteur d'alignement et de la sortie de l’Encodeur.\n",
    "\n",
    "Nous avons sélectionné un modèle Keras avec les spécificités suivantes: \n",
    "- système bidirectionnel \n",
    "- 2 couches (une couche Embedding et une couche RNN) \n",
    "- un type de cellule GRU (gated recurrent unit) avec 512 neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a1b66",
   "metadata": {},
   "source": [
    "### Encodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb33a0",
   "metadata": {},
   "source": [
    "Nous avons sélectionné un encodeur avec une couche embedding avec une matrice de dimension 300 et une couche GRU (gated recurrent unit) avec 512 neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d4852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la sortie de l'Encodeur : \n",
      " (batch size, max_length_inp, latent_dim) -> (80, 19, 512)\n",
      "\n",
      "Dimensions de l'état caché :\n",
      " (batch size, latent_dim) -> (80, 512)\n"
     ]
    }
   ],
   "source": [
    "# Définition de l'encodeur \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim):\n",
    "        # vocab_size: taille du vocabulaire dans la langue de départ\n",
    "        # embedding_dim: dimension de la matrice d'embedding \n",
    "        # latent_dim : dimension de l'état caché (units)\n",
    "        \n",
    "        super(Encoder, self).__init__() # pour faciliter des questions d'héritage \n",
    "        \n",
    "        self.units = latent_dim\n",
    "        \n",
    "        # sélection de l'embedding, fonction qui permet de vectoriser les mots représentés précédemment par leur index \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # sélection de la cellule de calcul GRU \n",
    "        self.gru = tf.keras.layers.GRU(self.units, # nombre d'états cachés - neurones  \n",
    "                                       return_sequences=True, # retourne une séquence/ bidirectionelle\n",
    "                                       return_state=True, # permet de conserver la mémoire de l'encodeur et le rend disponible pour le décodeur \n",
    "                                       recurrent_initializer='glorot_uniform') # initialisation des poids de la matrice utilisée pour la transformation linéaire des états récurrents par distribution uniforme \n",
    "      \n",
    "    def call(self, x, hidden): # calcul des sorties du modèle \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units)) # définition d'une matrice de dim jeu de données fixé * nombre d'états cachés \n",
    "\n",
    "\n",
    "# Paramètres du modèle\n",
    "\n",
    "latent_dim = 512 # nombre d'états cachés (2 puissance 9)\n",
    "embedding_dim = 300 # dimension de la matrice d'embedding\n",
    "\n",
    "encoder = Encoder(vocab_size_inp, embedding_dim, latent_dim)\n",
    "\n",
    "hidden = encoder.initialize_hidden_state(batch_size) # état caché\n",
    "enc_output, hidden = encoder(example_input_batch, hidden) # sortie\n",
    "\n",
    "print('Dimensions de la sortie de l\\'Encodeur : ')\n",
    "print(' (batch size, max_length_inp, latent_dim) -> {}'.format(enc_output.shape))\n",
    "print()\n",
    "\n",
    "print('Dimensions de l\\'état caché :')\n",
    "print(' (batch size, latent_dim) -> {}'.format(hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a617b7e",
   "metadata": {},
   "source": [
    "### Mécanisme d'attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b71665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la sortie de la couche d'attention :\n",
      " (batch size, latent_dim) (80, 512)\n",
      "Dimensions des poids d'attention :\n",
      " (batch_size, max_length_inp, 1) (80, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "# définition de la classe \n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # définition de 3 couches de neuronnes denses \n",
    "        self.W1 = tf.keras.layers.Dense(units)# couche de l'état caché \n",
    "        self.W2 = tf.keras.layers.Dense(units) # couche de sortie de l'encodeur\n",
    "        self.V = tf.keras.layers.Dense(1) # couche de sortie du mécanisme d'attention  \n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        # dimensions de 'hidden' : (batch_size, units)\n",
    "        # dimensions de 'enc_output' : (batch_size, max_length_input, units)\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # dimensions de 'hidden_with_time_axis' : (batch_size, 1, units)\n",
    "        # ce changement de dimension est nécessaire pour le calcul du score\n",
    "        \n",
    "        # Application de la formule score = FC(tanh(FC(EO) + FC(H))\n",
    "        # avec FC: couche Fully-connected (dense layer) c'est-à-dire W1 et W2\n",
    "        # EO: sortie de l’Encodeur, encoder_output \n",
    "        # H : état caché, hidden_state. \n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(hidden_with_time_axis) + self.W2(enc_output)))\n",
    "        # dimensions de 'score' : (batch_size, max_length_inp, 1)\n",
    "        # dimensions avant d'appliquer 'self.V' : (batch_size, max_length_inp, units)\n",
    "\n",
    "        # calcul des poids d'attention via la fonction softmax \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # dimensions de 'attention_weights' : (batch_size, max_length_inp, 1)\n",
    "        \n",
    "        # calcul du vecteur contexte par produit scalaire\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        # dimensions de context_vector après somme (batch_size, units)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# paramétrage\n",
    "\n",
    "attention_layer = BahdanauAttention(latent_dim)\n",
    "attention_result, attention_weights = attention_layer(hidden, enc_output)\n",
    "\n",
    "print(\"Dimensions de la sortie de la couche d'attention :\")\n",
    "print(\" (batch size, latent_dim) {}\".format(attention_result.shape))\n",
    "print(\"Dimensions des poids d'attention :\")\n",
    "print(\" (batch_size, max_length_inp, 1) {}\".format(attention_weights.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b2a4f",
   "metadata": {},
   "source": [
    "### Décodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4441a7",
   "metadata": {},
   "source": [
    "Le décodeur est constitué des mêmes couches que l'Encodeur avec la notion d'attention ajoutée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff20b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de la sortie du Décodeur :\n",
      " (batch_size, vocab size) -> (80, 331)\n"
     ]
    }
   ],
   "source": [
    "# Définition du décodeur \n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim, attention_layer):\n",
    "        # même architecture que l'encodeur\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.units = latent_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        # couche dense comprenant le vocabulaire de la langue cible \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # prise en compte de la notion d'attention en input du décodeur \n",
    "        self.attention = attention_layer(latent_dim)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # dimensions de 'enc_output' : (batch_size, max_length_inp, latent_dim)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        # dimensions de 'context_vector' : (batch size, latent_dim)\n",
    "        # dimensions de 'attention_weights' : (batch_size, max_length_inp, 1)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        # dimensions de 'x' : (batch_size, 1, embedding_dim)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # dimensions de 'x' : (batch_size, 1, embedding_dim + latent_dim)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # dimensions de 'output' : (batch_size * 1, latent_dim )\n",
    "\n",
    "        output = self.fc(output)\n",
    "        # dimensions de 'output' : (batch_size, vocab)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "# paramétrage \n",
    "\n",
    "decoder = Decoder(vocab_size_targ, embedding_dim, latent_dim, BahdanauAttention)\n",
    "\n",
    "dec_input = tf.random.uniform((batch_size, 1))\n",
    "dec_output, _, _ = decoder(dec_input, hidden, enc_output)\n",
    "\n",
    "print ('Dimension de la sortie du Décodeur :')\n",
    "print(' (batch_size, vocab size) -> {}'.format(dec_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce0839",
   "metadata": {},
   "source": [
    "### Fonction de coût et optimiseur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f122c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de coût et optimiseur\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # calcul de l'entropie croisée à partir de la fonction logits entre la valeur réelle et la valeur prédite  \n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
    "    # masque de même taille que la séquence, qui contient 0 si la valeur de l'élement de la séquence est nulle, 1 sinon\n",
    "    # ce masque permet de contrer l'effet du padding sur la fonction loss\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask # application de ce masque comme poids lors du calcul de l'erreur \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # application de la méthode d'optimisation réalisée à partir de la fonction Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4c2c4",
   "metadata": {},
   "source": [
    "## Entrainement du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f19c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle \n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_initial_hidden):\n",
    "    \n",
    "    loss = 0 # initisalisation de la perte\n",
    "    \n",
    "    # utilisation d'une fonction de perte de Gradient \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = encoder(inp, enc_initial_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        # parcours de chaque élement de 'targ'\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # appel au Décodeur\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            # calcul de la perte\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # mise à jour de la variable 'dec_input'\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        # calcul des variables du modèle\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    #calcul du gradient du modèle\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # optimisation des variables du modèle\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fb8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.6892\n",
      "Epoch 1 Batch 100 Loss 0.9740\n",
      "Epoch 1 Batch 200 Loss 0.7589\n",
      "Epoch 1 Loss 1.2163\n",
      "Time taken for 1 epoch 439.59 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6874\n",
      "Epoch 2 Batch 100 Loss 0.6217\n",
      "Epoch 2 Batch 200 Loss 0.5128\n",
      "Epoch 2 Loss 0.5859\n",
      "Time taken for 1 epoch 391.17 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4697\n",
      "Epoch 3 Batch 100 Loss 0.4215\n",
      "Epoch 3 Batch 200 Loss 0.3426\n",
      "Epoch 3 Loss 0.3601\n",
      "Time taken for 1 epoch 385.54 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2316\n",
      "Epoch 4 Batch 100 Loss 0.2030\n",
      "Epoch 4 Batch 200 Loss 0.1707\n",
      "Epoch 4 Loss 0.1845\n",
      "Time taken for 1 epoch 376.04 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1643\n",
      "Epoch 5 Batch 100 Loss 0.1188\n",
      "Epoch 5 Batch 200 Loss 0.1239\n",
      "Epoch 5 Loss 0.1286\n",
      "Time taken for 1 epoch 376.57 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1029\n",
      "Epoch 6 Batch 100 Loss 0.1122\n",
      "Epoch 6 Batch 200 Loss 0.0805\n",
      "Epoch 6 Loss 0.0869\n",
      "Time taken for 1 epoch 375.67 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0717\n",
      "Epoch 7 Batch 100 Loss 0.0451\n",
      "Epoch 7 Batch 200 Loss 0.0598\n",
      "Epoch 7 Loss 0.0690\n",
      "Time taken for 1 epoch 378.29 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0802\n",
      "Epoch 8 Batch 100 Loss 0.0524\n",
      "Epoch 8 Batch 200 Loss 0.0560\n",
      "Epoch 8 Loss 0.0591\n",
      "Time taken for 1 epoch 377.70 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0473\n",
      "Epoch 9 Batch 100 Loss 0.0779\n",
      "Epoch 9 Batch 200 Loss 0.0382\n",
      "Epoch 9 Loss 0.0488\n",
      "Time taken for 1 epoch 376.72 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0551\n",
      "Epoch 10 Batch 100 Loss 0.0284\n",
      "Epoch 10 Batch 200 Loss 0.0460\n",
      "Epoch 10 Loss 0.0410\n",
      "Time taken for 1 epoch 376.15 sec\n",
      "\n",
      "Time elapsed :3853.46s\n"
     ]
    }
   ],
   "source": [
    "# Evaluation des performances pendant l'entrainement du modèle \n",
    "# avec suivi de la fonction de perte tous les 100 batchs dans chaque époque\n",
    "\n",
    "# Checkpoint - permet d'enregistrer le modèle\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "def train(n_epoch = 10):\n",
    "\n",
    "    t=time.time()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state(batch_size)\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "\n",
    "        # conservation du modèle tout les 2 epochs\n",
    "        # if (epoch + 1) % 2 == 0:\n",
    "            # checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                          total_loss / steps_per_epoch))\n",
    "\n",
    "        print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))\n",
    "\n",
    "\n",
    "    print(\"Time elapsed :{}s\".format(round(time.time() - t,2)))\n",
    "\n",
    "train(n_epoch = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed14b8",
   "metadata": {},
   "source": [
    "## Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c88127de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction de traduction \n",
    "\n",
    "def translate(sentence, is_seq = False):\n",
    "    \n",
    "    # Initialisation de la matrice d'attention\n",
    "    attention_matrix = np.zeros((max_length_targ, max_length_inp))    \n",
    "    \n",
    "    # permet de transformer l'entrée sous forme de phrase en séquence (pour de nouvelles phrases non séquencées)\n",
    "    if not(is_seq):\n",
    "        # prétraitement de la phrase d'entrée\n",
    "        sentence = clean_sentence(sentence)\n",
    "        # transformation de la phrase en séquence\n",
    "        sentence = input_tokenizer.texts_to_sequences([sentence])\n",
    "        sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence,\n",
    "                                                             maxlen=max_length_input,\n",
    "                                                             padding='post')\n",
    "        \n",
    "    # initialisation des variables\n",
    "    hidden = [tf.zeros((1, latent_dim))]\n",
    "    enc_out, enc_hidden = encoder(sentence, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    # remplissage du premier élement par l'indice associé à la balise <start>\n",
    "    dec_input = tf.expand_dims([input_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    stop_condition = False # initialisation du status de la traduction -> True correspond à la balise <\"end\">\n",
    "    words = [] # initialisation de la phrase de sortie\n",
    "    t = 0\n",
    "\n",
    "    while not stop_condition:\n",
    "        \n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # conservation des poids d'attention\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # utilisation du mot ayant la meilleure probabilité pour la prédiction \n",
    "        predicted_index = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if predicted_index != 0:\n",
    "            word = target_tokenizer.index_word[predicted_index]\n",
    "        else :\n",
    "            word = ''\n",
    "\n",
    "        # retour de la prédiction id dans le modèle \n",
    "        dec_input = tf.expand_dims([predicted_index], 0)\n",
    "        \n",
    "        # verification des conditions de sortie: \n",
    "        if (word == '<end>' or # la balise <end> \n",
    "           t >= max_length_targ-1): # longeur maximale atteinte\n",
    "            \n",
    "            stop_condition = True\n",
    "            break\n",
    "            \n",
    "         # ajout du mot à la phrase de sortie    \n",
    "        words.append(word)\n",
    "        \n",
    "        t+=1\n",
    "\n",
    "    return \" \".join(words), attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df538eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 25835/25835 [1:35:44<00:00,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de traduction sur données de tests \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_sentence(seq, tokenizer):\n",
    "    sentence = \"\"\n",
    "    for s in seq:\n",
    "        if s!=0: \n",
    "            sentence += \" \" + tokenizer.index_word[s]\n",
    "    return sentence\n",
    "\n",
    "def test(n_test):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['Phrase_a_traduire','Phrase_traduite', 'Sortie_modele'])\n",
    "\n",
    "    for i in tqdm(range(n_test)):\n",
    "\n",
    "        x_test = find_sentence(X_test[i], input_tokenizer)\n",
    "        y_pred, attention_matrix = translate(X_test[i].reshape((-1,max_length_inp)), is_seq = True)\n",
    "        y_true = find_sentence(y_test[i], target_tokenizer)\n",
    "        \n",
    "        new_row = {'Phrase_a_traduire':x_test,'Phrase_traduite':y_true,'Sortie_modele':y_pred}\n",
    "\n",
    "        df_results = df_results.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # conservation des données sous forme d'un csv\n",
    "    return df_results.to_csv('results_mod3_vBA.csv', index=False)\n",
    "        \n",
    "test(len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156ec22",
   "metadata": {},
   "source": [
    "## Evaluation des résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cfb1c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; the grapefruit is my favorite fruit ,...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; china is usually hot during summer , ...</td>\n",
       "      <td>chine est generalement chaud pendant l ete ,...</td>\n",
       "      <td>chine est generalement chaud pendant l ete , e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; your favorite fruit is the lemon , bu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; her favorite fruit is the strawberry ...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; the united states is usually mild dur...</td>\n",
       "      <td>les etats unis est generalement doux pendant...</td>\n",
       "      <td>les etats unis est generalement doux pendant l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> the grapefruit is my favorite fruit ,...   \n",
       "1   <start> china is usually hot during summer , ...   \n",
       "2   <start> your favorite fruit is the lemon , bu...   \n",
       "3   <start> her favorite fruit is the strawberry ...   \n",
       "4   <start> the united states is usually mild dur...   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    le pamplemousse est mon fruit prefere , mais...   \n",
       "1    chine est generalement chaud pendant l ete ,...   \n",
       "2    votre fruit prefere est le citron , mais leu...   \n",
       "3    son fruit prefere est la fraise , mais votre...   \n",
       "4    les etats unis est generalement doux pendant...   \n",
       "\n",
       "                                       Sortie_modele  \n",
       "0  le pamplemousse est mon fruit prefere , mais l...  \n",
       "1  chine est generalement chaud pendant l ete , e...  \n",
       "2  votre fruit prefere est le citron , mais leur ...  \n",
       "3  son fruit prefere est la fraise , mais votre f...  \n",
       "4  les etats unis est generalement doux pendant l...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports nécessaires \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# récupération des résultats sous forme d'un dataframe \n",
    "df_results=pd.read_csv('results_mod3_vBA.csv')\n",
    "\n",
    "# retrait des balises <start> et <end> de la colonne \"Phrase_traduite\"\n",
    "phrase_purgee=[]\n",
    "for phrase in df_results['Phrase_traduite']:\n",
    "    phrase = phrase.replace('<start>','')\n",
    "    phrase = phrase.replace('<end>', '')\n",
    "    phrase_purgee.append(phrase)\n",
    "df_results['Phrase_traduite']=phrase_purgee    \n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ac21d",
   "metadata": {},
   "source": [
    "Evaluer les résultats entre la \"Phrase_traduite\" et la phrase \"Sortie_modele\" n'est pas immédiat car celles-ci peuvent être différentes mais correspondre toutes deux à une traduction possible. Il est également difficile de juger de la syntaxe et de l'orthographe des phrases. \n",
    "\n",
    "Nous avons utilisé deux indicateurs pour tenter de mieux comprendre les résultats: \n",
    "- vérification du nombre de mots dans chaque des phrases\n",
    "- comparaison des racinisation des mots de chaque des phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15945ef1",
   "metadata": {},
   "source": [
    "### Vérification du nombre de mots dans chaque des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d8d7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'Nb_words_cible' fournissant le nombre de mots dans la phrase cible \n",
    "nb_words_cible=[]\n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "    nb_words_cible.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_cible']=nb_words_cible\n",
    "\n",
    "# Création d'une colonne 'Nb_words_mod' fournissant le nombre de mots dans la phrase prédite par le modèle \n",
    "nb_words_mod=[]\n",
    "for sentence in df_results['Sortie_modele']:\n",
    "    nb_words_mod.append(len(word_tokenize(sentence, language='french')))\n",
    "df_results['nb_words_mod']=nb_words_mod\n",
    "\n",
    "# Création d'une colonne diff\n",
    "df_results['Différence']=df_results['nb_words_cible']-df_results['nb_words_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "412e817d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df_results.Différence)\n",
    "plt.title(\"Histogramme représentant la différence de nombre de mots entre la phrase traduite et la phrase sortie du modèle\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc63222",
   "metadata": {},
   "source": [
    "Cet histogramme montre qu'une très grande partie des phrases contiennent le même nombre de mots dans les deux cas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a39bd7",
   "metadata": {},
   "source": [
    "### Comparaison des racinisation des mots de chaque des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0850ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer stopwords de la classe nltk.corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Initialisation de la liste des mots vides en français puis ajout de certains mots \n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words.update(['moins', 'plus', 'a', 'entre', 'ete',',', '.'])\n",
    "\n",
    "# Intialisation de la racinisation en français \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e42cfeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>nb_words_cible</th>\n",
       "      <th>nb_words_mod</th>\n",
       "      <th>Différence</th>\n",
       "      <th>racine_cible</th>\n",
       "      <th>racine_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; the grapefruit is my favorite fruit ,...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais l...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...</td>\n",
       "      <td>[\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; china is usually hot during summer , ...</td>\n",
       "      <td>chine est generalement chaud pendant l ete ,...</td>\n",
       "      <td>chine est generalement chaud pendant l ete , e...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...</td>\n",
       "      <td>[\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; your favorite fruit is the lemon , bu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leur ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"citron\", \"prefer\", \"raisin\"]</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"citron\", \"favor\", \"raisin\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; her favorite fruit is the strawberry ...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre f...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; the united states is usually mild dur...</td>\n",
       "      <td>les etats unis est generalement doux pendant...</td>\n",
       "      <td>les etats unis est generalement doux pendant l...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...</td>\n",
       "      <td>[\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> the grapefruit is my favorite fruit ,...   \n",
       "1   <start> china is usually hot during summer , ...   \n",
       "2   <start> your favorite fruit is the lemon , bu...   \n",
       "3   <start> her favorite fruit is the strawberry ...   \n",
       "4   <start> the united states is usually mild dur...   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    le pamplemousse est mon fruit prefere , mais...   \n",
       "1    chine est generalement chaud pendant l ete ,...   \n",
       "2    votre fruit prefere est le citron , mais leu...   \n",
       "3    son fruit prefere est la fraise , mais votre...   \n",
       "4    les etats unis est generalement doux pendant...   \n",
       "\n",
       "                                       Sortie_modele  nb_words_cible  \\\n",
       "0  le pamplemousse est mon fruit prefere , mais l...              14   \n",
       "1  chine est generalement chaud pendant l ete , e...              16   \n",
       "2  votre fruit prefere est le citron , mais leur ...              14   \n",
       "3  son fruit prefere est la fraise , mais votre f...              14   \n",
       "4  les etats unis est generalement doux pendant l...              18   \n",
       "\n",
       "   nb_words_mod  Différence  \\\n",
       "0            14           0   \n",
       "1            16           0   \n",
       "2            14           0   \n",
       "3            14           0   \n",
       "4            18           0   \n",
       "\n",
       "                                        racine_cible  \\\n",
       "0  [\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...   \n",
       "1  [\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...   \n",
       "2  [\"fruit\", \"prefer\", \"citron\", \"prefer\", \"raisin\"]   \n",
       "3     [\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]   \n",
       "4  [\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...   \n",
       "\n",
       "                                          racine_mod  \n",
       "0  [\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...  \n",
       "1  [\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...  \n",
       "2   [\"fruit\", \"prefer\", \"citron\", \"favor\", \"raisin\"]  \n",
       "3     [\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]  \n",
       "4  [\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application à tout le dataframe \n",
    "\n",
    "# Instanciation des listes nécessaires \n",
    "racine_cible_tot=[]\n",
    "racine_cible=[]\n",
    "racine_mod_tot=[]\n",
    "racine_mod=[]\n",
    "\n",
    "# Création d'une colonne 'racine_cible' fournissant la liste des racines des mots des phrases de la colonne cible \n",
    "for sentence in df_results['Phrase_traduite']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "            racine_cible.append(stemmer.stem(mot))\n",
    "    racine_cible_tot.append(racine_cible)\n",
    "    racine_cible=[]\n",
    "    \n",
    "df_results['racine_cible']=racine_cible_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_cible'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_cible']]\n",
    "\n",
    "\n",
    "# Création d'une colonne 'racine_mod' fournissant la liste des racines des mots des phrases de la colonne prédite par le modèle  \n",
    "for sentence in df_results['Sortie_modele']:\n",
    "        # tokenisation des phrases en mots\n",
    "    mots = word_tokenize(sentence, language='french')\n",
    "    for mot in mots:\n",
    "        # suppression des mots présents dans la liste stop_words \n",
    "        if mot not in stop_words: \n",
    "        # racinisation des mots \n",
    "            racine_mod.append(stemmer.stem(mot))\n",
    "    racine_mod_tot.append(racine_mod)\n",
    "    racine_mod=[]\n",
    "\n",
    "df_results['racine_mod']=racine_mod_tot\n",
    "\n",
    "# rajout des guillemets \n",
    "df_results['racine_mod'] = [[f'\"{j}\"' for j in i] for i in df_results['racine_mod']]\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc9dbd",
   "metadata": {},
   "source": [
    "### Etablissement des scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a29232",
   "metadata": {},
   "source": [
    "Nous avons crée deux scores permettant de calculer un score total par la moyenne des deux premiers. \n",
    "\n",
    "Le premier score correspond au ratio du nombre de racines en commun sur le nombre de racines des phrases cibles. \n",
    "\n",
    "Le second score correspond à un calcul lié au nombre de mots d'écart entre les phrases traduites et les phrases sorties du modèle. Pour une phrase sans mot d'écart, le score sera de 1 et ce score diminue avec l'augmentation du nombre de mots d'écart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c03f985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase_a_traduire</th>\n",
       "      <th>Phrase_traduite</th>\n",
       "      <th>Sortie_modele</th>\n",
       "      <th>nb_words_cible</th>\n",
       "      <th>nb_words_mod</th>\n",
       "      <th>Différence</th>\n",
       "      <th>racine_cible</th>\n",
       "      <th>racine_mod</th>\n",
       "      <th>score_racine</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>score_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; the grapefruit is my favorite fruit ,...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais...</td>\n",
       "      <td>le pamplemousse est mon fruit prefere , mais l...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...</td>\n",
       "      <td>[\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; china is usually hot during summer , ...</td>\n",
       "      <td>chine est generalement chaud pendant l ete ,...</td>\n",
       "      <td>chine est generalement chaud pendant l ete , e...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...</td>\n",
       "      <td>[\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; your favorite fruit is the lemon , bu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leu...</td>\n",
       "      <td>votre fruit prefere est le citron , mais leur ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"citron\", \"prefer\", \"raisin\"]</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"citron\", \"favor\", \"raisin\"]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; her favorite fruit is the strawberry ...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre...</td>\n",
       "      <td>son fruit prefere est la fraise , mais votre f...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]</td>\n",
       "      <td>[\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; the united states is usually mild dur...</td>\n",
       "      <td>les etats unis est generalement doux pendant...</td>\n",
       "      <td>les etats unis est generalement doux pendant l...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...</td>\n",
       "      <td>[\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Phrase_a_traduire  \\\n",
       "0   <start> the grapefruit is my favorite fruit ,...   \n",
       "1   <start> china is usually hot during summer , ...   \n",
       "2   <start> your favorite fruit is the lemon , bu...   \n",
       "3   <start> her favorite fruit is the strawberry ...   \n",
       "4   <start> the united states is usually mild dur...   \n",
       "\n",
       "                                     Phrase_traduite  \\\n",
       "0    le pamplemousse est mon fruit prefere , mais...   \n",
       "1    chine est generalement chaud pendant l ete ,...   \n",
       "2    votre fruit prefere est le citron , mais leu...   \n",
       "3    son fruit prefere est la fraise , mais votre...   \n",
       "4    les etats unis est generalement doux pendant...   \n",
       "\n",
       "                                       Sortie_modele  nb_words_cible  \\\n",
       "0  le pamplemousse est mon fruit prefere , mais l...              14   \n",
       "1  chine est generalement chaud pendant l ete , e...              16   \n",
       "2  votre fruit prefere est le citron , mais leur ...              14   \n",
       "3  son fruit prefere est la fraise , mais votre f...              14   \n",
       "4  les etats unis est generalement doux pendant l...              18   \n",
       "\n",
       "   nb_words_mod  Différence  \\\n",
       "0            14           0   \n",
       "1            16           0   \n",
       "2            14           0   \n",
       "3            14           0   \n",
       "4            18           0   \n",
       "\n",
       "                                        racine_cible  \\\n",
       "0  [\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...   \n",
       "1  [\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...   \n",
       "2  [\"fruit\", \"prefer\", \"citron\", \"prefer\", \"raisin\"]   \n",
       "3     [\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]   \n",
       "4  [\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...   \n",
       "\n",
       "                                          racine_mod  score_racine  \\\n",
       "0  [\"pamplemouss\", \"fruit\", \"prefer\", \"orang\", \"f...      1.000000   \n",
       "1  [\"chin\", \"general\", \"chaud\", \"pend\", \"jam\", \"a...      1.000000   \n",
       "2   [\"fruit\", \"prefer\", \"citron\", \"favor\", \"raisin\"]      0.800000   \n",
       "3     [\"fruit\", \"prefer\", \"frais\", \"favor\", \"banan\"]      1.000000   \n",
       "4  [\"etat\", \"unis\", \"general\", \"doux\", \"pend\", \"h...      0.888889   \n",
       "\n",
       "   score_diff  score_tot  \n",
       "0         1.0   1.000000  \n",
       "1         1.0   1.000000  \n",
       "2         1.0   0.900000  \n",
       "3         1.0   1.000000  \n",
       "4         1.0   0.944444  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création d'une colonne \"comparaison racine\" fournissant le ratio nombre de racines en commun / nombre de racines des phrases cibles \n",
    "score=[]\n",
    "for i in range(df_results.shape[0]):\n",
    "    score.append(len(set(df_results.iloc[i,6]) & set(df_results.iloc[i,7]))/len(df_results.iloc[i,6]))\n",
    "    \n",
    "df_results['score_racine']=score\n",
    "\n",
    "df_results['score_diff']= 1-abs(df_results['Différence'])/18\n",
    "\n",
    "df_results['score_tot']=(df_results['score_diff']+ df_results['score_racine'])/2\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d59752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne du score_tot 0.9528791844459281\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne du score_tot\n",
    "\n",
    "print(\"moyenne du score_tot\", df_results['score_tot'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6e90d",
   "metadata": {},
   "source": [
    "Cette moyenne est très élevée car le modèle est sur entrainé (130 000 phrases pour très peu de mots). Les phrases du dataset sont également toujours similaires structurellement. \n",
    "\n",
    "Ainsi, le modèle fonctionne très bien sur les échantillons de test même si on peut constater certaines aberrations (correspond au moins bon score). \n",
    "\n",
    "Exemple de phrases mal traduites : \n",
    "\n",
    "1/ mauvaises associations \n",
    "1.1\n",
    "\"paris est pluvieux au printemps\" -> \"new jersey est pluvieux au printemps , les etats unis au chaud au printemps , les etats unis au chaud au printemps , les\"\n",
    "\n",
    "pourtant paris a été bien traduit ici : \n",
    "\"paris est agreable au mois d octobre , et il est jamais merveilleux en fevrier\" -> \"paris est agreable au mois d octobre , et il est jamais merveilleux en fevrier .\"\n",
    "\n",
    "il a dû associer \"est pluvieux au printemps\" à \"new jersey\" et à \"les etats unis au chaud au printemps , les etats unis au chaud au printemps , les\"\n",
    "\n",
    "1.2\n",
    "\"comment etait le temps dans le new jersey l hiver dernier ?\" -> \"vous alle a l epicerie ?\"\n",
    "\n",
    "1.3\n",
    "\"comment etait votre visite au new jersey en juin dernier ?\" -> \"comment aller en france ?\"\n",
    "\n",
    "\n",
    "2/ répétition \n",
    "\"il veut aller en inde\" -> \"il est alle en inde en inde en inde en inde en inde en inde en inde en inde en inde en inde en\"\n",
    "\n",
    "Nous pensons que les répétions sont dues à l'approche greedy. \n",
    "\n",
    "3/ mauvaises associations + répétition \n",
    "\"voudrait il aller la tour eiffel ? \" - > \"vous a conduit votre visite a conduit votre visite a conduit votre visite a conduit votre visite a conduit votre visite a conduit votre\"\n",
    "\n",
    "4/ mauvaises traductions incomprehensibles \n",
    "\n",
    "Ligne 11682: \" <start> china is mild during september , but it is sometimes snowy in march . <end>\",\" <start> chine est doux au mois de septembre , mais il est parfois enneigee en mars . <end>\",\"chine est doux au mois de septembre , mais il est parfois neigeux en mars .\"\n",
    "\n",
    "Ligne 18879: <start> china is mild during september . <end>, <start> chine est doux au mois de septembre . <end>,\"ils n aimons les etats unis est doux au mois de septembre , les mangues et en chine est doux au mois de septembre\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2b752",
   "metadata": {},
   "source": [
    "Enfin, lorsque l'on essaye d'autres phrases (mêmes très simples) hors du dataset, les résultats sont très mauvais. \n",
    "\n",
    "Exemples : \n",
    "\"united states is rainy\" -> \"il est pluvieux en espagnol\"\n",
    "\"the united states is rainy\" -> \"il n'aime pas les raisons\"\n",
    "\n",
    "Le modèle semble plus performant que dans les itérations précédentes. \n",
    "\n",
    "Il est toutefois difficile d'estimer si le modèle serait réellement performant avec un jeu d'entraînement plus diversifié. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
