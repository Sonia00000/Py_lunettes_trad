{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wel5g91jHpiX"
   },
   "source": [
    "# PROJET : \n",
    "# RECONNAISSANCE VOCALE\n",
    "# SYSTEME DE TRADUCTION ADAPTE AUX LUNETTES CONNECTEES  \n",
    "  \n",
    "    \n",
    "      \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ahQqnZPJ7W"
   },
   "source": [
    "Dans l'itération précédente, nous avons procédé à une traduction mot à mot en utilisant un ensemble de phrases traduites à la fois en anglais et en français et en se servant de la place des mots dans la phrase. Nous avions constaté que cette méthode n'était pas probante.\n",
    "\n",
    "Au cours de cette étape, nous avons procédé en entrainant un algorithme de deep learning qui se base sur la correspondance entre l'espace vectoriel de mots source vers les mots cibles. Grâce aux mots transparents entre les deux langues, l'algorithme peut générer une matrice de correspondance permettant la traduction de l'ensemble des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IPCDnCBHpRt"
   },
   "source": [
    "   \n",
    "      \n",
    "   \n",
    "## Partie II : Modélisation - Itération 2  \n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYblBncEQx7U"
   },
   "source": [
    "Cette partie reprend le dictionnaire de référence tel que travaillé précédemment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rBipRtr_qSux",
    "outputId": "469c3ea7-7eb4-4826-e1f5-1381f783b28c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anglais</th>\n",
       "      <th>Français</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>les</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>fut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Anglais Français\n",
       "0     the       le\n",
       "1     the      les\n",
       "2     the       la\n",
       "3     and       et\n",
       "4     was      fut"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset utilisé : DS2\n",
    "data = pd.read_csv(\"en-fr.txt\", sep = \" \", names = [\"Anglais\", \"Français\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WETZMQhOqsux",
    "outputId": "f0f1776f-6a02-4693-d54b-d97303721415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbre de mots anglais uniques dans le 2ème dataset : 94680 mots.\n",
      "\n",
      "Nbre de mots français uniques dans le 2ème dataset : 97034 mots.\n"
     ]
    }
   ],
   "source": [
    "uniq_eng = list(data.Anglais.unique())\n",
    "print(\"Nbre de mots anglais uniques dans le 2ème dataset :\", len(uniq_eng), \"mots.\\n\")\n",
    "\n",
    "uniq_fr = list(data.Français.unique())\n",
    "print(\"Nbre de mots français uniques dans le 2ème dataset :\", len(uniq_fr), \"mots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9I7hVVZIqs8u"
   },
   "outputs": [],
   "source": [
    "# Création du dictionnaire de référence (DS2) : association de toutes les traductions françaises possibles pour chaque mot\n",
    "#anglais en clé\n",
    "data_ord = data.groupby(['Anglais']).agg(lambda x : list(x)).reset_index()\n",
    "dico_ref = {data_ord['Anglais'][i] : data_ord['Français'][i] for i in range(data_ord.shape[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykfGxjjdHgv6"
   },
   "source": [
    "### 1. Création des dictionnaires de vectorisation des mots anglais et français  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YMzfNrkHXAK"
   },
   "source": [
    "Cette partie de code sert à créer les dictionnaires de vectorisation des mots français et anglais contenus dans notre dataset de référence (DS2) à partir de matrices d'embeddings pré-entrainées.\n",
    "\n",
    "En raison de la durée d'exécution, le code suivant ne sera exécuté qu'une seule fois : la suite des analyses se base sur les fichiers csv créés dans cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t1CACTkSH9eC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom pathlib import Path\\nfrom urllib.request import urlretrieve\\n\\nPATH_TO_DATA = Path()\\n\\nfr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\\nif not fr_embeddings_path.exists(): \\n    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)\\nen_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\\nif not en_embeddings_path.exists():\\n    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Téléchargement des matrices d'embedding pré-entrainées \n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "PATH_TO_DATA = Path()\n",
    "\n",
    "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
    "if not fr_embeddings_path.exists(): \n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)\n",
    "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
    "if not en_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY5nabYJILhY"
   },
   "source": [
    "La cellule suivante reprend le code de création de la classe Word2Vec() : \n",
    "\n",
    "+ Les méthodes d'initialisation de la classe et de loading permettent, à partir des matrices téléchargées précédemment, de générer un array de 2 millions de lignes (nombre de mots inclus dans les matrices pré-entrainées) et de 301 colonnes, représentant le mot et le vecteur de taille 300 correspondant, dont découlent les éléments suivants : \n",
    "\n",
    "    + Le mapping word/index et index/word (méthodes word2id et id2word)\n",
    "    \n",
    "    + La méthode words et la méthode embeddings : chaque ligne de la matrice est splittée après le 1er élément, constituant ainsi une liste des mots de la matrice (méthode words) et un array de 2 millions de lignes et de 300 colonnes représentant chaque vecteur (méthode embeddings)\n",
    "    \n",
    "+ La méthode encode retourne pour le mot inclus en attribut l'array du vecteur correspondant si ce mot appartient à la matrice pré-entrainée, sinon il retourne un array zéros de longeur 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dRhHlWuiICt_"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "class Word2Vec():\n",
    "    def __init__(self, filepath):\n",
    "        self.words, self.embeddings = self.load_wordvec(filepath)\n",
    "        # Mappings for O(1) retrieval:\n",
    "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
    "    \n",
    "    def load_wordvec(self, filepath):\n",
    "        assert str(filepath).endswith('.gz')\n",
    "        words = []\n",
    "        embeddings = []\n",
    "        with gzip.open(filepath, 'rt', encoding = 'utf-8') as f:\n",
    "            next(f)  # Skip header\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(',', 1)\n",
    "                words.append(word)                                 \n",
    "                embeddings.append(np.fromstring(vec, sep=','))     \n",
    "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
    "        return words, np.vstack(embeddings)\n",
    "    \n",
    "    def encode(self, word):\n",
    "        '''\n",
    "        Inputs:\n",
    "        -word : mot\n",
    "        \n",
    "        Output:\n",
    "        - l'embedding du mot correspondant\n",
    "        '''\n",
    "        if word in self.words : \n",
    "            row = self.embeddings[self.word2id[word],:]\n",
    "            return(row)\n",
    "        \n",
    "        else : \n",
    "            return(np.zeros((1, self.embeddings.shape[1])))  #si le mot n'est pas dans le dictionnaire\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw8v3Rg3ITcp"
   },
   "outputs": [],
   "source": [
    "# Chargement des données et création des mappings word/vector\n",
    "#fr_word2vec = Word2Vec(filepath=fr_embeddings_path)\n",
    "#en_word2vec = Word2Vec(filepath=en_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6jFjP_WIW7Y"
   },
   "outputs": [],
   "source": [
    "# Création des dictionnaires de vectorisation des mots du dataset 2\n",
    "#dico_eng = {}\n",
    "#for i in uniq_eng:\n",
    "    #if i in en_word2vec.words:\n",
    "        #dico_eng.update({i : en_word2vec.encode(i)})\n",
    "        \n",
    "#dico_fr = {}\n",
    "#for i in uniq_fr:\n",
    "    #if i in fr_word2vec.words:\n",
    "        #dico_fr.update({i : fr_word2vec.encode(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzB-dQJxIZqW"
   },
   "outputs": [],
   "source": [
    "# Création d'un fichier reprenant les dictionnaires incluant les vectorisations des mots anglais et français du DS2\n",
    "#pd.DataFrame.from_dict(data = dico_eng, orient = 'index').to_csv(\"dico_eng.csv\", header = False)\n",
    "#pd.DataFrame.from_dict(data = dico_fr, orient = 'index').to_csv(\"dico_fr.csv\", header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EquEw5NWIcsy"
   },
   "source": [
    "   \n",
    "### 2. Matrice de correspondance W  et traduction des mots anglais \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHeGnezBIhUk"
   },
   "source": [
    "##### Chargement des données des dictionnaires de vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GHYT80n3qtHn"
   },
   "outputs": [],
   "source": [
    "# Lecture des dictionnaires de vectorisations des mots anglais et français\n",
    "df_eng = pd.read_csv(\"dico_eng.csv\", header= None, index_col = 0)\n",
    "dico_eng = {df_eng.index[i] : np.array(df_eng.iloc[i, :]) for i in range(df_eng.shape[0])}\n",
    "\n",
    "df_fr = pd.read_csv(\"dico_fr.csv\", header= None, index_col = 0)\n",
    "dico_fr = {df_fr.index[i] : np.array(df_fr.iloc[i, :]) for i in range(df_fr.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_MCsHhzuqtWO"
   },
   "outputs": [],
   "source": [
    "# Création de la liste des mots anglais vectorisés\n",
    "dico_eng_words = list(dico_eng.keys())\n",
    "\n",
    "# Création de la liste des vecteurs correspondants\n",
    "dico_eng_embeds = []\n",
    "for i in dico_eng.keys():\n",
    "    dico_eng_embeds.append(dico_eng[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "glWIyHGDrFQ3"
   },
   "outputs": [],
   "source": [
    "# Création de la liste des mots français vectorisés\n",
    "dico_fr_words = list(dico_fr.keys())\n",
    "\n",
    "# Création de la liste des vecteurs correspondants\n",
    "dico_fr_embeds = []\n",
    "for i in dico_fr.keys():\n",
    "    dico_fr_embeds.append(dico_fr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvVP2kdjIrcT"
   },
   "source": [
    "##### Matrice de correspondance W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxHAE3opXF3s"
   },
   "source": [
    "Une fois que nous avons obtenu les vectorisations de tous les mots en communs des deux jeux de données, nous allons créer deux matrices d'embeddings X et Y contenant les vectorisations de tous les mots transparents apparaissant dans les deux langues. \n",
    "\n",
    "Le but sera ensuite de trouver la matrice W qui  projettera l'espace vectoriel des mots sources (Anglais dans notre cas) sur l'espace vectoriel des mots cibles (Français dans notre cas) afin que les mots ayant les mêmes significations dans les deux langues aient des coordonnées les plus proches possibles. \n",
    "\n",
    "Afin de trouver la matrice W, nous allons utiliser une formule reposant sur l'orthogonalité et les propriétés d'un matrice:\n",
    "<center> $W^* = UV^T$ avec $U$$\\Sigma$$V^T$ $=$ $SVD$ $(YX^T)$ </center>\n",
    "\n",
    "Ou SVD est la décomposition en valeur singulière d'une matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXmaM7B3rFdv",
    "outputId": "64b3ae09-d0e7-41dd-86e6-5cb35f9f8c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbre de mots transparents : 33072 mots.\n"
     ]
    }
   ],
   "source": [
    "# Création de la liste des mots transparents (identiques dans les dictionnaires français et anglais)\n",
    "mots_transparents = [word for word in dico_eng if word in dico_fr]\n",
    "print(\"Nbre de mots transparents :\", len(mots_transparents), \"mots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V4tCJXNwrFuE"
   },
   "outputs": [],
   "source": [
    "# Encodage des mots transparents\n",
    "X, Y = np.empty([300,len(mots_transparents)]),np.empty([300,len(mots_transparents)])\n",
    "for i, word in enumerate(mots_transparents): \n",
    "        X[:,i] = dico_eng[word]\n",
    "        Y[:,i] = dico_fr[word]\n",
    "        \n",
    "assert X.shape[0] == 300 and Y.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-S5AZ7TNrPn-"
   },
   "outputs": [],
   "source": [
    "# Calcul de W, la matrice de correspondance entre l'anglais et le français\n",
    "U, sigma, Vtranspose = np.linalg.svd(Y.dot(X.T))\n",
    "W = U.dot(Vtranspose)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVGVFA7PIx_w"
   },
   "source": [
    "##### Fonction de traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7dJ95iYS5ja"
   },
   "source": [
    "Nous allons créer une fonction qui permet d'associer à tout mot anglais les k mots français les plus similaires (k représentant le nombre de traductions proposées par le dictionnaire de référence). On calcule pour cela la métrique cosine similarity que l'on cherche à maximiser afin de proposer la meilleure traduction possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ycy9JeztrQc-"
   },
   "outputs": [],
   "source": [
    "# Création de la fonction de traduction de l'anglais au français\n",
    "def get_closest_french_words(eng_word):\n",
    "        '''\n",
    "        Inputs:\n",
    "        - eng_word : mot en anglais\n",
    "        \n",
    "        Output:\n",
    "        -renvoie les k mots les plus proches dans la traduction\n",
    "        '''\n",
    "        # k représente le nombre de mots proposés en traduction du mot anglais dans le dictionnaire de référence\n",
    "        k = len(dico_ref[eng_word])\n",
    "        \n",
    "        # On reprend le vecteur du mot anglais sélectionné\n",
    "        eng_obj = dico_eng[eng_word]\n",
    "        \n",
    "        # On projette le mot anglais dans l'espace vectoriel des mots français grâce la matrice de correspondance W\n",
    "        aligne_eng = W.dot(eng_obj.T)\n",
    "        \n",
    "        # On crée l'array reprenant tous les vecteurs des mots français\n",
    "        fr_embeds = np.array(dico_fr_embeds)\n",
    "        \n",
    "        # Calcul de la similitude du mot anglais avec les mots français avec la métrique cosine similarity\n",
    "        norm_prod = np.linalg.norm(aligne_eng)*np.linalg.norm(fr_embeds, axis=1) \n",
    "        scores = fr_embeds.dot(aligne_eng) / norm_prod \n",
    "        \n",
    "        # Récupération des k meilleurs scores de similitude\n",
    "        best_k = np.flip(np.argsort(scores))[:k]\n",
    "        \n",
    "        # Liste contenant une liste des k mots français les plus proches du mot anglais et la liste contenant leur degré de similitude\n",
    "        return [[dico_fr_words[idx] for idx in best_k], [scores[idx] for idx in best_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8HLovWLrREu",
    "outputId": "0be7ddcc-c909-41cf-d4c3-163d5ffcc5b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voiture', 'véhicule']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de traduction\n",
    "get_closest_french_words('car')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7ftiX2eWOX2"
   },
   "outputs": [],
   "source": [
    "# Création du dictionnaire de traduction qui affecte à chaque mot anglais, les k traductions possibles\n",
    "#from tqdm import tqdm\n",
    "#dico_trad = {}\n",
    "#for word in tqdm(dico_eng_words):\n",
    "    #dico_trad.update({word : get_closest_french_words(word)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uiJdGLlx3uD"
   },
   "outputs": [],
   "source": [
    "# Création d'un fichier csv pour recharger le dictionnaire obtenu\n",
    "#pd.DataFrame.from_dict(data = dico_trad, orient = 'index').to_csv(\"dico_trad.csv\", header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZimBZUUTUu2"
   },
   "source": [
    "### 3. Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Khv3vJ7TaxK"
   },
   "source": [
    "Une fois notre dictionnaire de traduction créé grâce au word embedding, nous allons ensuite le comparer au dictionnaire de traduction de référence.\n",
    "\n",
    "Le score représentera le pourcentage de similarité entre les traductions des deux dictionnaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E2c-XUpZ8gP6"
   },
   "outputs": [],
   "source": [
    "# Import du dataframe contenant le dictionnaire de traduction\n",
    "df_trad_imp = pd.read_csv(\"dico_trad.csv\", header = None, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nbBw3jqmyM6b"
   },
   "outputs": [],
   "source": [
    "# Retraitement du dataframe précédent\n",
    "carac = [\",\", \"'\", \"[\", \"]\"]\n",
    "for i in range(df_trad_imp.shape[0]):\n",
    "    for c in df_trad_imp.iloc[i, 0]:\n",
    "        if c in carac:\n",
    "            df_trad_imp.iloc[i, 0] = df_trad_imp.iloc[i, 0].replace(c, \" \")\n",
    "    df_trad_imp.iloc[i, 0] = df_trad_imp.iloc[i, 0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "BP5e8YxOrF33",
    "outputId": "2d9651c0-6be8-4bc4-867d-ebea9b74635d"
   },
   "outputs": [],
   "source": [
    "# Création du dataframe de scoring\n",
    "\n",
    "df_trad = df_trad_imp.reset_index().iloc[:, 0:2]\n",
    "df_trad.columns = ['Mot_anglais', 'Trad_func']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PRJDImLmtk0e",
    "outputId": "04477bb7-dbcc-470b-8ce5-44236373c7c0"
   },
   "outputs": [],
   "source": [
    "df_ref = pd.DataFrame(list(dico_ref.items()), columns = ['Mot_anglais', 'Trad_ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "onGy0c4r9aiq",
    "outputId": "d3b6e052-20f3-4be6-8f34-d4d601a4f8ce"
   },
   "outputs": [],
   "source": [
    "df_score = pd.merge(df_ref, df_trad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "vxlAViTlA-JA",
    "outputId": "915f2e0e-453c-45da-d5b3-f6f96bce9801"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mot_anglais</th>\n",
       "      <th>Trad_ref</th>\n",
       "      <th>Trad_func</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa</td>\n",
       "      <td>[aaa]</td>\n",
       "      <td>[lbl]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaa</td>\n",
       "      <td>[aaaa]</td>\n",
       "      <td>[aaaaaa]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaaa</td>\n",
       "      <td>[aaaaaa]</td>\n",
       "      <td>[aaaaaa]</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aab</td>\n",
       "      <td>[aab]</td>\n",
       "      <td>[jaan]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aac</td>\n",
       "      <td>[aac]</td>\n",
       "      <td>[xvid]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mot_anglais  Trad_ref Trad_func  Score\n",
       "0         aaa     [aaa]     [lbl]    0.0\n",
       "1        aaaa    [aaaa]  [aaaaaa]    0.0\n",
       "2      aaaaaa  [aaaaaa]  [aaaaaa]  100.0\n",
       "3         aab     [aab]    [jaan]    0.0\n",
       "4         aac     [aac]    [xvid]    0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul du score : rapport entre le nbre de traductions communes (dictionnaire de référence et \n",
    "#dictionnaire de traduction créé) et le nbre de traductions de référence\n",
    "score = []\n",
    "tot = 0\n",
    "res = 0\n",
    "for i in range(df_score.shape[0]):\n",
    "    for j in df_score.Trad_func[i]:\n",
    "        if j in df_score.Trad_ref[i]:\n",
    "            tot +=1\n",
    "        res = round(tot / len(df_score.Trad_ref[i]) * 100, 2)\n",
    "    tot = 0\n",
    "    score.append(res)\n",
    "\n",
    "df_score['Score'] = score\n",
    "df_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1T7D4-1JFZSP",
    "outputId": "820488f6-d492-4308-f141-045ea8e6684f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le sore est de : 46.90854424855996\n"
     ]
    }
   ],
   "source": [
    "score_model = df_score.Score.mean()\n",
    "print(\"Le sore est de :\", score_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgQGybJLT6dB"
   },
   "source": [
    "##### Utilisation d'une méthode utilisant un k fixe =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dictionnaire comportant les mots français et leurs vecteurs associés\n",
    "#for idx, i in enumerate(tqdm(uniq_fr)):\n",
    "#    if i in fr_word2vec.words:\n",
    "#        dico_fr.update({i : fr_word2vec.encode(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Création d'un dictionnaire comportant les mots anglais et leurs vecteurs associés\n",
    "#for idx, i in enumerate(tqdm(uniq_eng)):\n",
    "#    if i in en_word2vec.words:\n",
    "#        dico_eng.update({i : en_word2vec.encode(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On exporte les deux dictionnaires en fichier csv afin de pouvoir les réutiliser, leur création ayant pris pas mal de temps\n",
    "#pd.DataFrame.from_dict(data = dico_eng, orient = 'index').to_csv(\"dico_eng.csv\", header = False)\n",
    "#pd.DataFrame.from_dict(data = dico_fr, orient = 'index').to_csv(\"dico_fr.csv\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir des deux fichiers csv compressés en format gzip, nous créons deux objets de la classe word2vec\n",
    "fr_word2vec2 = Word2Vec(filepath='dico_fr.csv.gz', vocab_size=2000000)\n",
    "en_word2vec2 = Word2Vec(filepath='dico_eng.csv.gz', vocab_size=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les mots qui apparaissent dans les 2 vocabulaires (mots qui ont des chaines de caractères identiques)\n",
    "#mots_transparents = [word for word in fr_word2vec2.words if word in en_word2vec2.words]\n",
    "\n",
    "# On encode nos mots : obtention des embeddings de chaque mot\n",
    "#X, Y = np.empty([300,len(mots_transparents)]),np.empty([300,len(mots_transparents)])\n",
    "#for i, word in enumerate(mots_transparents) : \n",
    "#        X[:,i] = en_word2vec2.encode(word)\n",
    "#        Y[:,i] = fr_word2vec2.encode(word)\n",
    "#        \n",
    "#assert X.shape[0] == 300 and Y.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul de la matrice W\n",
    "U, sigma, Vtranspose = np.linalg.svd(Y.dot(X.T))\n",
    "W = U.dot(Vtranspose)        \n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#définition d'une fonction de traduction\n",
    "\"\"\"def get_closest_french_words(en_word, k):\n",
    "        en_obj = en_word2vec2.encode(en_word)\n",
    "        aligne_en = W.dot(en_obj.T)\n",
    "        fr_embeds = fr_word2vec2.embeddings\n",
    "        norm_prod = np.linalg.norm(aligne_en)*np.linalg.norm(fr_embeds, axis=1) \n",
    "        scores = fr_embeds.dot(aligne_en) / norm_prod \n",
    "        best_k = np.flip(np.argsort(scores))[:k]\n",
    "        return ([fr_word2vec2.words[idx] for idx in best_k], [scores[idx] for idx in best_k])\n",
    "    \n",
    "get_closest_french_words('cat', 5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dictionnaire de traduction Anglais: 5 mots français les plus probable\n",
    "#from tqdm import tqdm\n",
    "#dico_trad = {}\n",
    "#for word in tqdm(en_word2vec2.words):\n",
    "#    dico_trad.update({word : get_closest_french_words(word, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des mots d'origines et mots cibles en ignorant les probabilités \n",
    "\"\"\"dico_trad2 = {}\n",
    "for word in dico_trad.keys():\n",
    "    dico_trad2.update({word : dico_trad[word][0]})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dataframe à partie du dictionnaire dico_trad2\n",
    "#df_trad = pd.DataFrame(list(dico_trad2.items()), columns = ['Mot_anglais', 'Trad_func'])\n",
    "#df_trad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ord = data.groupby(['Anglais']).agg(lambda x : list(x)).reset_index()\n",
    "#dico_ref = {data_ord['Anglais'][i] : data_ord['Français'][i] for i in range(data_ord.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trad_csv = df_trad.to_csv('df_trad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trad2 = pd.read_csv('df_trad.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "carac = [\",\", \"'\", \"[\", \"]\"]\n",
    "for i in range(df_trad2.shape[0]):\n",
    "    for c in df_trad2.iloc[i, 1]:\n",
    "        if c in carac:\n",
    "            df_trad2.iloc[i, 1] = df_trad2.iloc[i, 1].replace(c, \" \")\n",
    "    df_trad2.iloc[i, 1] = df_trad2.iloc[i, 1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = pd.DataFrame(list(dico_ref.items()), columns = ['Mot_anglais', 'Trad_ref'])\n",
    "df_score = pd.merge(df_ref, df_trad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mot_anglais</th>\n",
       "      <th>Trad_ref</th>\n",
       "      <th>Trad_func</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa</td>\n",
       "      <td>[aaa]</td>\n",
       "      <td>[lbl, aaa, aem, cbg, aik]</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaa</td>\n",
       "      <td>[aaaa]</td>\n",
       "      <td>[aaaaaa, nnn, lbl, ksk, kah]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaaa</td>\n",
       "      <td>[aaaaaa]</td>\n",
       "      <td>[aaaaaa, eef, lbl, ksk, okai]</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aab</td>\n",
       "      <td>[aab]</td>\n",
       "      <td>[jaan, aln, aib, aab, mnc]</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aac</td>\n",
       "      <td>[aac]</td>\n",
       "      <td>[xvid, aac, wma, aif, sacd]</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mot_anglais  Trad_ref                      Trad_func  Score\n",
       "0         aaa     [aaa]      [lbl, aaa, aem, cbg, aik]  100.0\n",
       "1        aaaa    [aaaa]   [aaaaaa, nnn, lbl, ksk, kah]    0.0\n",
       "2      aaaaaa  [aaaaaa]  [aaaaaa, eef, lbl, ksk, okai]  100.0\n",
       "3         aab     [aab]     [jaan, aln, aib, aab, mnc]  100.0\n",
       "4         aac     [aac]    [xvid, aac, wma, aif, sacd]  100.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = []\n",
    "tot = 0\n",
    "res = 0\n",
    "for i in range(df_score.shape[0]):\n",
    "    for j in df_score.Trad_func[i]:\n",
    "        if j in df_score.Trad_ref[i]:\n",
    "            tot +=1\n",
    "    res = round(tot / len(df_score.Trad_ref[i]) * 100, 2)\n",
    "    tot = 0\n",
    "    score.append(res)\n",
    "\n",
    "df_score['Score'] = score\n",
    "df_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.221875972191356"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model = df_score.Score.mean()\n",
    "score_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les deux scoring, nous obtenons les scores de 46.90% et 59.22%. Ces scores ne sont pas très bons alors même que les matrices d'embedding sélectionnées ont été pré-entraînées pendant longtemps. \n",
    "\n",
    "Nous ne validons pas cette méthode de traduction mot à mot. \n",
    "\n",
    "Nous allons continuer par un système de traduction phrase à phrase (Seq2Seq)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
